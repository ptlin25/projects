{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from matplotlib import pyplot as plt\n",
        "from torch.nn import functional as F\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "tBY-ds2sInY8"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code I"
      ],
      "metadata": {
        "id": "GuNYsYvxTqU7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1\n",
        "First, t is embedded into a high-dimensional space using Guassian Fourier features before being passed through a linear layer and the swish activation function.\n",
        "\n",
        "Layers h1-h4 are the encoding layers. In each step of the encoding layers, the input is passed though a convolutional layer with progressively smaller dimensions and more channels. Each layer adds a dense projection of the embedded time. Finally, GroupNorm and the swish activation function are applied after each step.\n",
        "\n",
        "Next is the decoding layers. In each step of the decoding, the transposed convolutions (of the encoding layer) is applied. h = self.tconv-(torch.cat([h, h-], dim=1)) adds encoder features at each step.\n",
        "\n",
        "The final transposed convolution is a single-channel image."
      ],
      "metadata": {
        "id": "6W3LvIzMhVyh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YyQtV7155Nht"
      },
      "outputs": [],
      "source": [
        "#@title Defining a time-dependent score-based model (double click to expand or collapse)\n",
        "\n",
        "class GaussianFourierProjection(nn.Module):\n",
        "  \"\"\"Gaussian random features for encoding time steps.\"\"\"\n",
        "  def __init__(self, embed_dim, scale=30.):\n",
        "    super().__init__()\n",
        "    # Randomly sample weights during initialization. These weights are fixed\n",
        "    # during optimization and are not trainable.\n",
        "    self.W = nn.Parameter(torch.randn(embed_dim // 2) * scale, requires_grad=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    if x.dim() == 1:\n",
        "        x = x[:, None]\n",
        "\n",
        "    x_proj = x * self.W * 2 * np.pi\n",
        "    return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n",
        "\n",
        "\n",
        "class Dense(nn.Module):\n",
        "  \"\"\"A fully connected layer that reshapes outputs to feature maps.\"\"\"\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super().__init__()\n",
        "    self.dense = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.dense(x)[..., None, None]\n",
        "\n",
        "\n",
        "class ScoreNet(nn.Module):\n",
        "  \"\"\"A time-dependent score-based model built upon U-Net architecture.\"\"\"\n",
        "\n",
        "  def __init__(self, channels=[32, 64, 128, 256], embed_dim=256, group_num=4):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.embed = nn.Sequential(GaussianFourierProjection(embed_dim=embed_dim),\n",
        "         nn.Linear(embed_dim, embed_dim))\n",
        "\n",
        "    self.conv1 = nn.Conv2d(1, channels[0], 3, stride=1, bias=False)\n",
        "    self.dense1 = Dense(embed_dim, channels[0])\n",
        "    self.gnorm1 = nn.GroupNorm(group_num, num_channels=channels[0])\n",
        "\n",
        "    self.conv2 = nn.Conv2d(channels[0], channels[1], 3, stride=2, bias=False)\n",
        "    self.dense2 = Dense(embed_dim, channels[1])\n",
        "    self.gnorm2 = nn.GroupNorm(group_num, num_channels=channels[1])\n",
        "\n",
        "    self.conv3 = nn.Conv2d(channels[1], channels[2], 3, stride=2, bias=False)\n",
        "    self.dense3 = Dense(embed_dim, channels[2])\n",
        "    self.gnorm3 = nn.GroupNorm(group_num, num_channels=channels[2])\n",
        "\n",
        "    self.conv4 = nn.Conv2d(channels[2], channels[3], 3, stride=2, bias=False)\n",
        "    self.dense4 = Dense(embed_dim, channels[3])\n",
        "    self.gnorm4 = nn.GroupNorm(group_num, num_channels=channels[3])\n",
        "\n",
        "    self.tconv4 = nn.ConvTranspose2d(channels[3], channels[2], 3, stride=2, bias=False)\n",
        "    self.dense5 = Dense(embed_dim, channels[2])\n",
        "    self.tgnorm4 = nn.GroupNorm(group_num, num_channels=channels[2])\n",
        "\n",
        "    self.tconv3 = nn.ConvTranspose2d(channels[2] + channels[2], channels[1], 3, stride=2, bias=False, output_padding=1)\n",
        "    self.dense6 = Dense(embed_dim, channels[1])\n",
        "    self.tgnorm3 = nn.GroupNorm(group_num, num_channels=channels[1])\n",
        "\n",
        "    self.tconv2 = nn.ConvTranspose2d(channels[1] + channels[1], channels[0], 3, stride=2, bias=False, output_padding=1)\n",
        "    self.dense7 = Dense(embed_dim, channels[0])\n",
        "    self.tgnorm2 = nn.GroupNorm(group_num, num_channels=channels[0])\n",
        "\n",
        "    self.tconv1 = nn.ConvTranspose2d(channels[0] + channels[0], 1, 3, stride=1)\n",
        "\n",
        "    # The swish activation function\n",
        "    self.act = lambda x: x * torch.sigmoid(x)\n",
        "\n",
        "    self.rho0 = nn.Parameter(torch.tensor(1.0))\n",
        "    self.rho1 = nn.Parameter(torch.tensor(0.5))\n",
        "\n",
        "  def forward(self, x, t):\n",
        "    # Obtain the Gaussian random feature embedding for t\n",
        "    embed = self.act(self.embed(t))\n",
        "\n",
        "    h1 = self.conv1(x) # ...\n",
        "    h1 += self.dense1(embed) #...\n",
        "    h1 = self.gnorm1(h1) # ...\n",
        "    h1 = self.act(h1) # ...\n",
        "    h2 = self.conv2(h1) # ...\n",
        "    h2 += self.dense2(embed)\n",
        "    h2 = self.gnorm2(h2)\n",
        "    h2 = self.act(h2)\n",
        "    h3 = self.conv3(h2)\n",
        "    h3 += self.dense3(embed)\n",
        "    h3 = self.gnorm3(h3)\n",
        "    h3 = self.act(h3)\n",
        "    h4 = self.conv4(h3)\n",
        "    h4 += self.dense4(embed)\n",
        "    h4 = self.gnorm4(h4)\n",
        "    h4 = self.act(h4)\n",
        "\n",
        "    h = self.tconv4(h4) # ...\n",
        "    h += self.dense5(embed) # ...\n",
        "    h = self.tgnorm4(h)\n",
        "    h = self.act(h)\n",
        "    h = self.tconv3(torch.cat([h, h3], dim=1)) # ...\n",
        "    h += self.dense6(embed)\n",
        "    h = self.tgnorm3(h)\n",
        "    h = self.act(h)\n",
        "    h = self.tconv2(torch.cat([h, h2], dim=1))\n",
        "    h += self.dense7(embed)\n",
        "    h = self.tgnorm2(h)\n",
        "    h = self.act(h)\n",
        "    h = self.tconv1(torch.cat([h, h1], dim=1))\n",
        "\n",
        "    mu = self.rho0 * (x - self.rho1 * h)\n",
        "\n",
        "    return mu"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3"
      ],
      "metadata": {
        "id": "Mpm6LuZrjh0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_beta_schedule(timesteps, s=0.008):\n",
        "    steps = timesteps + 1\n",
        "    x = torch.linspace(0, timesteps, steps)\n",
        "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * np.pi * 0.5) ** 2\n",
        "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
        "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
        "    return betas.clamp(0.0001, 0.9999)\n",
        "\n",
        "class Diffusion(nn.Module):\n",
        "    def __init__(self, model, n_steps, device, min_beta=0.0001, max_beta=0.02):\n",
        "        # Store beta, alpha and \\bar alpha.\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.n_steps = n_steps\n",
        "        self.device = device\n",
        "\n",
        "        # defining beta and alpha\n",
        "        # self.beta = torch.linspace(min_beta, max_beta, n_steps).to(device)\n",
        "        self.beta = cosine_beta_schedule(n_steps).to(device)    # experimenting with cosine beta schedule\n",
        "        self.alpha = 1.0 - self.beta\n",
        "        self.alpha_bar = torch.cumprod(self.alpha, dim=0).to(device)\n",
        "\n",
        "    def forward_process(self, x0, t):\n",
        "        # Sample x_{t-1},x_t given x_0\n",
        "        alpha_bar_tm1 = self.alpha_bar[t - 1].view(-1, 1, 1, 1)\n",
        "        alpha_t = self.alpha[t].view(-1, 1, 1, 1)\n",
        "        alpha_bar_t = self.alpha_bar[t].view(-1, 1, 1, 1)\n",
        "\n",
        "        noise1 = torch.randn_like(x0)\n",
        "        x_tm1 = torch.sqrt(alpha_bar_tm1) * x0 + torch.sqrt(1 - alpha_bar_tm1) * noise1\n",
        "\n",
        "        noise2 = torch.randn_like(x0)\n",
        "        x_t = torch.sqrt(alpha_t) * x_tm1 + torch.sqrt(1 - alpha_t) * noise2\n",
        "        return x_tm1, x_t\n",
        "\n",
        "    def predict_next(self, x_t, t):\n",
        "        # Compute mu(xt,t)\n",
        "        return self.model(x_t, t)\n",
        "\n",
        "    def sample_xt_given_x0(self, x0, t):\n",
        "        alpha_bar_t = self.alpha_bar[t].view(-1, 1, 1, 1)\n",
        "        noise = torch.randn_like(x0)\n",
        "        x_t = torch.sqrt(alpha_bar_t) * x0 + torch.sqrt(1 - alpha_bar_t) * noise\n",
        "        return x_t\n"
      ],
      "metadata": {
        "id": "EdKAsTYRkqT4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4"
      ],
      "metadata": {
        "id": "mtNgPtd1jgPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(diffusion, x0):\n",
        "    B = x0.shape[0]\n",
        "    device = x0.device\n",
        "    T = diffusion.n_steps\n",
        "    t = torch.randint(1, T, (B,), device=device)\n",
        "\n",
        "    x_tm1, x_t = diffusion.forward_process(x0, t)\n",
        "    mu = diffusion.predict_next(x_t, t)\n",
        "    alpha_t = diffusion.alpha[t].view(-1, 1, 1, 1)\n",
        "    coeff = (1.0 / (2.0 * (1.0 - alpha_t))).view(B)\n",
        "    loss = (coeff * ((x_tm1 - mu) ** 2).mean(dim=(1, 2, 3))).mean()\n",
        "\n",
        "    return loss\n",
        "\n",
        "def compute_adjusted_loss(diffusion, x0, M=4):\n",
        "    B = x0.shape[0]\n",
        "    device = x0.device\n",
        "    T = diffusion.n_steps\n",
        "    t = torch.randint(1, T, (B,), device=device)\n",
        "\n",
        "    alpha_t = diffusion.alpha[t].view(B, 1, 1, 1)\n",
        "    alpha_bar_tm1 = diffusion.alpha_bar[t - 1].view(B, 1, 1, 1)\n",
        "    coeff = (1.0 / (2.0 * (1.0 - alpha_t))).view(B, 1, 1, 1)\n",
        "\n",
        "    noise1 = torch.randn_like(x0)\n",
        "    x_tm1 = torch.sqrt(alpha_bar_tm1) * x0 + torch.sqrt(1 - alpha_bar_tm1) * noise1\n",
        "\n",
        "    total_loss = torch.zeros(B, device=device)\n",
        "    for _ in range(M):\n",
        "        noise2 = torch.randn_like(x0)\n",
        "        x_t = torch.sqrt(alpha_t) * x_tm1 + torch.sqrt(1 - alpha_t) * noise2\n",
        "\n",
        "        mu = diffusion.predict_next(x_t, t)\n",
        "        loss = coeff * ((x_tm1 - mu) ** 2)\n",
        "        total_loss += loss.view(B, -1).mean(dim=1)\n",
        "\n",
        "    return (total_loss / M).mean()"
      ],
      "metadata": {
        "id": "InQM-0gjnKd_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5"
      ],
      "metadata": {
        "id": "O0hbT0ebjaot"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to reduce the variance of the estimate, we could sample multiple x_t for each x_0 and x_tm1. In other words, for each x_0, we first sample one x_tm1, then sample multiple x_t, and average the loss over the number of sampled x_t."
      ],
      "metadata": {
        "id": "krIO86SaCrkU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6"
      ],
      "metadata": {
        "id": "9NkPFn6njZUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, diffusion, train_data, optimizer, loss_fn, num_epochs=60, batch_size=100):\n",
        "    device = diffusion.device\n",
        "    train_dataset = torch.utils.data.TensorDataset(train_data)\n",
        "    loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    scheduler = LambdaLR(optimizer, lr_lambda=[\n",
        "        lambda epoch: 1.0,\n",
        "        lambda epoch: 0.1 ** (epoch // 20)\n",
        "    ])\n",
        "\n",
        "    for epoch in range(1, num_epochs+1):\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        # freeze rho1 for the first 10 epochs, to let the model learn mu before trying to learn rho1\n",
        "        # otherwise, the model tends to decay rho1 to 0 and ignore the predicted mu\n",
        "        model.rho1.requires_grad = False if epoch <= 10 else True\n",
        "\n",
        "        for batch in loader:\n",
        "            x0 = batch[0].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss = loss_fn(diffusion, x0)\n",
        "\n",
        "            # after rho1 is unfrozen, I added a regularization term to keep rho1 within a reasonable range\n",
        "            if epoch > 10:\n",
        "                loss += 0.01 * (0.5 - diffusion.model.rho1)**2\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "            model.rho0.data.clamp_(0, 1.0)\n",
        "            model.rho1.data.clamp_(0.2, 0.6)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        current_rho_lr = scheduler.get_last_lr()[1]\n",
        "        print(f\"Epoch {epoch}, Loss: {epoch_loss / len(loader):.4f}, rho_lr: {current_rho_lr:.6f}\")\n",
        "\n",
        "        avg_loss = epoch_loss / len(loader)\n",
        "        print(f\"rho0: {model.rho0.item():.4f}, rho1: {model.rho1.item():.4f}\")\n",
        "\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "def evaluate_model(model, diffusion, x_val, loss_fn, batch_size=100):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        total_loss = 0.0\n",
        "        for i in range(0, x_val.shape[0], batch_size):\n",
        "            x0 = x_val[i:i+batch_size].to(diffusion.device)\n",
        "            total_loss += loss_fn(diffusion, x0).item()\n",
        "        return total_loss / (x_val.shape[0] // batch_size)\n",
        "\n",
        "\n",
        "def get_mnist():\n",
        "    train_dataset = datasets.MNIST(root='./data', train=True, download=True)\n",
        "    test_dataset = datasets.MNIST(root='./data', train=False, download=True)\n",
        "\n",
        "    full_data = torch.cat([train_dataset.data, test_dataset.data], dim=0).float() / 255.0\n",
        "    full_data = full_data.unsqueeze(1)\n",
        "\n",
        "    train_data = full_data[0:50000].to(device)\n",
        "    val_data   = full_data[50000:60000].to(device)\n",
        "    test_data  = full_data[60000:70000].to(device)\n",
        "\n",
        "    return train_data, val_data, test_data"
      ],
      "metadata": {
        "id": "CcTYR1HWEy-h"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, val_data, test_data = get_mnist()"
      ],
      "metadata": {
        "id": "ZzaVwGCmFBPY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c610b725-bb85-4ac8-9f90-086891ad9955"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 40.5MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.12MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 10.2MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 7.69MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "default_losses = []\n",
        "adjusted_losses = []\n",
        "for _ in range(20):\n",
        "    indices = torch.randint(0, train_data.shape[0], (1000,))\n",
        "    subset = train_data[indices]\n",
        "\n",
        "    model_default = ScoreNet()\n",
        "    model_default.to(device)\n",
        "    diffusion_default = Diffusion(model_default, n_steps=200, device=device, min_beta=1e-4, max_beta=0.01)\n",
        "\n",
        "    rho_params = [model_default.rho0, model_default.rho1]\n",
        "    rho_ids = set(id(p) for p in rho_params)\n",
        "    base_params = [p for p in model_default.parameters() if id(p) not in rho_ids]\n",
        "    optimizer_default = torch.optim.Adam([\n",
        "        {\"params\": base_params, \"lr\": 0.001},\n",
        "        {\"params\": rho_params, \"lr\": 0.2}\n",
        "    ])\n",
        "\n",
        "    train(model_default, diffusion_default, subset, optimizer_default, compute_loss)\n",
        "    default_loss = evaluate_model(model_default, diffusion_default, val_data, compute_loss)\n",
        "    default_losses.append(default_loss)\n",
        "\n",
        "    model_adjusted = ScoreNet()\n",
        "    model_adjusted.to(device)\n",
        "    diffusion_adjusted = Diffusion(model_adjusted, n_steps=200, device=device, min_beta=1e-4, max_beta=0.01)\n",
        "\n",
        "    rho_params = [model_adjusted.rho0, model_adjusted.rho1]\n",
        "    rho_ids = set(id(p) for p in rho_params)\n",
        "    base_params = [p for p in model_adjusted.parameters() if id(p) not in rho_ids]\n",
        "    optimizer_adjusted = torch.optim.Adam([\n",
        "        {\"params\": base_params, \"lr\": 0.001},\n",
        "        {\"params\": rho_params, \"lr\": 0.2}\n",
        "    ])\n",
        "\n",
        "    train(model_adjusted, diffusion_adjusted, subset, optimizer_adjusted, compute_adjusted_loss)\n",
        "    adjusted_loss = evaluate_model(model_adjusted, diffusion_adjusted, val_data, compute_adjusted_loss)\n",
        "    adjusted_losses.append(adjusted_loss)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "A1dmXU5tNlgq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3429d3fb-ce75-42cb-a3be-3c6ea6745db0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 35.7769, rho_lr: 0.200000\n",
            "rho0: 0.0032, rho1: 0.5000\n",
            "Epoch 2, Loss: 14.0353, rho_lr: 0.200000\n",
            "rho0: 0.2185, rho1: 0.5000\n",
            "Epoch 3, Loss: 7.3162, rho_lr: 0.200000\n",
            "rho0: 0.3023, rho1: 0.5000\n",
            "Epoch 4, Loss: 4.5726, rho_lr: 0.200000\n",
            "rho0: 0.3639, rho1: 0.5000\n",
            "Epoch 5, Loss: 3.1346, rho_lr: 0.200000\n",
            "rho0: 0.3578, rho1: 0.5000\n",
            "Epoch 6, Loss: 2.6784, rho_lr: 0.200000\n",
            "rho0: 0.3654, rho1: 0.5000\n",
            "Epoch 7, Loss: 2.2105, rho_lr: 0.200000\n",
            "rho0: 0.3692, rho1: 0.5000\n",
            "Epoch 8, Loss: 2.0247, rho_lr: 0.200000\n",
            "rho0: 0.3652, rho1: 0.5000\n",
            "Epoch 9, Loss: 1.8523, rho_lr: 0.200000\n",
            "rho0: 0.3630, rho1: 0.5000\n",
            "Epoch 10, Loss: 1.7979, rho_lr: 0.200000\n",
            "rho0: 0.3674, rho1: 0.5000\n",
            "Epoch 11, Loss: 1.9509, rho_lr: 0.200000\n",
            "rho0: 0.4288, rho1: 0.3971\n",
            "Epoch 12, Loss: 1.4428, rho_lr: 0.200000\n",
            "rho0: 0.5282, rho1: 0.2710\n",
            "Epoch 13, Loss: 1.0103, rho_lr: 0.200000\n",
            "rho0: 0.6119, rho1: 0.2000\n",
            "Epoch 14, Loss: 0.9621, rho_lr: 0.200000\n",
            "rho0: 0.6026, rho1: 0.2000\n",
            "Epoch 15, Loss: 0.9164, rho_lr: 0.200000\n",
            "rho0: 0.6020, rho1: 0.2000\n",
            "Epoch 16, Loss: 0.9074, rho_lr: 0.200000\n",
            "rho0: 0.6154, rho1: 0.2000\n",
            "Epoch 17, Loss: 0.8875, rho_lr: 0.200000\n",
            "rho0: 0.6218, rho1: 0.2000\n",
            "Epoch 18, Loss: 0.8741, rho_lr: 0.200000\n",
            "rho0: 0.6199, rho1: 0.2000\n",
            "Epoch 19, Loss: 0.8645, rho_lr: 0.200000\n",
            "rho0: 0.6263, rho1: 0.2000\n",
            "Epoch 20, Loss: 0.8597, rho_lr: 0.020000\n",
            "rho0: 0.6315, rho1: 0.2000\n",
            "Epoch 21, Loss: 0.8416, rho_lr: 0.020000\n",
            "rho0: 0.6322, rho1: 0.2000\n",
            "Epoch 22, Loss: 0.8365, rho_lr: 0.020000\n",
            "rho0: 0.6337, rho1: 0.2000\n",
            "Epoch 23, Loss: 0.8243, rho_lr: 0.020000\n",
            "rho0: 0.6360, rho1: 0.2000\n",
            "Epoch 24, Loss: 0.8165, rho_lr: 0.020000\n",
            "rho0: 0.6386, rho1: 0.2000\n",
            "Epoch 25, Loss: 0.8107, rho_lr: 0.020000\n",
            "rho0: 0.6410, rho1: 0.2000\n",
            "Epoch 26, Loss: 0.8011, rho_lr: 0.020000\n",
            "rho0: 0.6434, rho1: 0.2000\n",
            "Epoch 27, Loss: 0.7937, rho_lr: 0.020000\n",
            "rho0: 0.6460, rho1: 0.2000\n",
            "Epoch 28, Loss: 0.7872, rho_lr: 0.020000\n",
            "rho0: 0.6483, rho1: 0.2000\n",
            "Epoch 29, Loss: 0.7805, rho_lr: 0.020000\n",
            "rho0: 0.6509, rho1: 0.2000\n",
            "Epoch 30, Loss: 0.7825, rho_lr: 0.020000\n",
            "rho0: 0.6530, rho1: 0.2000\n",
            "Epoch 31, Loss: 0.7685, rho_lr: 0.020000\n",
            "rho0: 0.6551, rho1: 0.2000\n",
            "Epoch 32, Loss: 0.7599, rho_lr: 0.020000\n",
            "rho0: 0.6577, rho1: 0.2000\n",
            "Epoch 33, Loss: 0.7515, rho_lr: 0.020000\n",
            "rho0: 0.6604, rho1: 0.2000\n",
            "Epoch 34, Loss: 0.7464, rho_lr: 0.020000\n",
            "rho0: 0.6627, rho1: 0.2000\n",
            "Epoch 35, Loss: 0.7375, rho_lr: 0.020000\n",
            "rho0: 0.6648, rho1: 0.2000\n",
            "Epoch 36, Loss: 0.7399, rho_lr: 0.020000\n",
            "rho0: 0.6672, rho1: 0.2000\n",
            "Epoch 37, Loss: 0.7311, rho_lr: 0.020000\n",
            "rho0: 0.6697, rho1: 0.2000\n",
            "Epoch 38, Loss: 0.7255, rho_lr: 0.020000\n",
            "rho0: 0.6714, rho1: 0.2000\n",
            "Epoch 39, Loss: 0.7307, rho_lr: 0.020000\n",
            "rho0: 0.6733, rho1: 0.2000\n",
            "Epoch 40, Loss: 0.7158, rho_lr: 0.002000\n",
            "rho0: 0.6756, rho1: 0.2000\n",
            "Epoch 41, Loss: 0.7169, rho_lr: 0.002000\n",
            "rho0: 0.6759, rho1: 0.2000\n",
            "Epoch 42, Loss: 0.7136, rho_lr: 0.002000\n",
            "rho0: 0.6762, rho1: 0.2000\n",
            "Epoch 43, Loss: 0.7061, rho_lr: 0.002000\n",
            "rho0: 0.6766, rho1: 0.2000\n",
            "Epoch 44, Loss: 0.6964, rho_lr: 0.002000\n",
            "rho0: 0.6770, rho1: 0.2000\n",
            "Epoch 45, Loss: 0.7019, rho_lr: 0.002000\n",
            "rho0: 0.6774, rho1: 0.2000\n",
            "Epoch 46, Loss: 0.6968, rho_lr: 0.002000\n",
            "rho0: 0.6778, rho1: 0.2000\n",
            "Epoch 47, Loss: 0.6936, rho_lr: 0.002000\n",
            "rho0: 0.6783, rho1: 0.2000\n",
            "Epoch 48, Loss: 0.6922, rho_lr: 0.002000\n",
            "rho0: 0.6787, rho1: 0.2000\n",
            "Epoch 49, Loss: 0.6880, rho_lr: 0.002000\n",
            "rho0: 0.6792, rho1: 0.2000\n",
            "Epoch 50, Loss: 0.6866, rho_lr: 0.002000\n",
            "rho0: 0.6796, rho1: 0.2000\n",
            "Epoch 51, Loss: 0.6770, rho_lr: 0.002000\n",
            "rho0: 0.6801, rho1: 0.2000\n",
            "Epoch 52, Loss: 0.6751, rho_lr: 0.002000\n",
            "rho0: 0.6806, rho1: 0.2000\n",
            "Epoch 53, Loss: 0.6751, rho_lr: 0.002000\n",
            "rho0: 0.6810, rho1: 0.2000\n",
            "Epoch 54, Loss: 0.6716, rho_lr: 0.002000\n",
            "rho0: 0.6815, rho1: 0.2000\n",
            "Epoch 55, Loss: 0.6691, rho_lr: 0.002000\n",
            "rho0: 0.6819, rho1: 0.2000\n",
            "Epoch 56, Loss: 0.6639, rho_lr: 0.002000\n",
            "rho0: 0.6825, rho1: 0.2000\n",
            "Epoch 57, Loss: 0.6676, rho_lr: 0.002000\n",
            "rho0: 0.6830, rho1: 0.2000\n",
            "Epoch 58, Loss: 0.6628, rho_lr: 0.002000\n",
            "rho0: 0.6835, rho1: 0.2000\n",
            "Epoch 59, Loss: 0.6609, rho_lr: 0.002000\n",
            "rho0: 0.6840, rho1: 0.2000\n",
            "Epoch 60, Loss: 0.6590, rho_lr: 0.000200\n",
            "rho0: 0.6845, rho1: 0.2000\n",
            "Epoch 1, Loss: 33.9354, rho_lr: 0.200000\n",
            "rho0: 0.0000, rho1: 0.5000\n",
            "Epoch 2, Loss: 12.6829, rho_lr: 0.200000\n",
            "rho0: 0.3498, rho1: 0.5000\n",
            "Epoch 3, Loss: 6.0944, rho_lr: 0.200000\n",
            "rho0: 0.3612, rho1: 0.5000\n",
            "Epoch 4, Loss: 3.9298, rho_lr: 0.200000\n",
            "rho0: 0.3577, rho1: 0.5000\n",
            "Epoch 5, Loss: 2.8480, rho_lr: 0.200000\n",
            "rho0: 0.3948, rho1: 0.5000\n",
            "Epoch 6, Loss: 2.4352, rho_lr: 0.200000\n",
            "rho0: 0.3648, rho1: 0.5000\n",
            "Epoch 7, Loss: 2.1761, rho_lr: 0.200000\n",
            "rho0: 0.3777, rho1: 0.5000\n",
            "Epoch 8, Loss: 1.9890, rho_lr: 0.200000\n",
            "rho0: 0.3737, rho1: 0.5000\n",
            "Epoch 9, Loss: 1.8532, rho_lr: 0.200000\n",
            "rho0: 0.3699, rho1: 0.5000\n",
            "Epoch 10, Loss: 1.7438, rho_lr: 0.200000\n",
            "rho0: 0.3752, rho1: 0.5000\n",
            "Epoch 11, Loss: 1.7791, rho_lr: 0.200000\n",
            "rho0: 0.4290, rho1: 0.3933\n",
            "Epoch 12, Loss: 1.2826, rho_lr: 0.200000\n",
            "rho0: 0.5431, rho1: 0.2443\n",
            "Epoch 13, Loss: 0.9688, rho_lr: 0.200000\n",
            "rho0: 0.6322, rho1: 0.2000\n",
            "Epoch 14, Loss: 0.9138, rho_lr: 0.200000\n",
            "rho0: 0.6023, rho1: 0.2000\n",
            "Epoch 15, Loss: 0.8944, rho_lr: 0.200000\n",
            "rho0: 0.6107, rho1: 0.2000\n",
            "Epoch 16, Loss: 0.8750, rho_lr: 0.200000\n",
            "rho0: 0.6232, rho1: 0.2000\n",
            "Epoch 17, Loss: 0.8622, rho_lr: 0.200000\n",
            "rho0: 0.6282, rho1: 0.2000\n",
            "Epoch 18, Loss: 0.8437, rho_lr: 0.200000\n",
            "rho0: 0.6295, rho1: 0.2000\n",
            "Epoch 19, Loss: 0.8369, rho_lr: 0.200000\n",
            "rho0: 0.6358, rho1: 0.2000\n",
            "Epoch 20, Loss: 0.8183, rho_lr: 0.020000\n",
            "rho0: 0.6422, rho1: 0.2000\n",
            "Epoch 21, Loss: 0.8105, rho_lr: 0.020000\n",
            "rho0: 0.6426, rho1: 0.2000\n",
            "Epoch 22, Loss: 0.7975, rho_lr: 0.020000\n",
            "rho0: 0.6441, rho1: 0.2000\n",
            "Epoch 23, Loss: 0.7887, rho_lr: 0.020000\n",
            "rho0: 0.6463, rho1: 0.2000\n",
            "Epoch 24, Loss: 0.7783, rho_lr: 0.020000\n",
            "rho0: 0.6495, rho1: 0.2000\n",
            "Epoch 25, Loss: 0.7680, rho_lr: 0.020000\n",
            "rho0: 0.6523, rho1: 0.2000\n",
            "Epoch 26, Loss: 0.7636, rho_lr: 0.020000\n",
            "rho0: 0.6553, rho1: 0.2000\n",
            "Epoch 27, Loss: 0.7541, rho_lr: 0.020000\n",
            "rho0: 0.6582, rho1: 0.2000\n",
            "Epoch 28, Loss: 0.7476, rho_lr: 0.020000\n",
            "rho0: 0.6608, rho1: 0.2000\n",
            "Epoch 29, Loss: 0.7443, rho_lr: 0.020000\n",
            "rho0: 0.6638, rho1: 0.2000\n",
            "Epoch 30, Loss: 0.7335, rho_lr: 0.020000\n",
            "rho0: 0.6667, rho1: 0.2000\n",
            "Epoch 31, Loss: 0.7242, rho_lr: 0.020000\n",
            "rho0: 0.6694, rho1: 0.2000\n",
            "Epoch 32, Loss: 0.7174, rho_lr: 0.020000\n",
            "rho0: 0.6719, rho1: 0.2000\n",
            "Epoch 33, Loss: 0.7111, rho_lr: 0.020000\n",
            "rho0: 0.6742, rho1: 0.2000\n",
            "Epoch 34, Loss: 0.7060, rho_lr: 0.020000\n",
            "rho0: 0.6770, rho1: 0.2000\n",
            "Epoch 35, Loss: 0.7022, rho_lr: 0.020000\n",
            "rho0: 0.6801, rho1: 0.2000\n",
            "Epoch 36, Loss: 0.6896, rho_lr: 0.020000\n",
            "rho0: 0.6829, rho1: 0.2000\n",
            "Epoch 37, Loss: 0.6882, rho_lr: 0.020000\n",
            "rho0: 0.6856, rho1: 0.2000\n",
            "Epoch 38, Loss: 0.6834, rho_lr: 0.020000\n",
            "rho0: 0.6878, rho1: 0.2000\n",
            "Epoch 39, Loss: 0.6766, rho_lr: 0.020000\n",
            "rho0: 0.6903, rho1: 0.2000\n",
            "Epoch 40, Loss: 0.6695, rho_lr: 0.002000\n",
            "rho0: 0.6932, rho1: 0.2000\n",
            "Epoch 41, Loss: 0.6671, rho_lr: 0.002000\n",
            "rho0: 0.6935, rho1: 0.2000\n",
            "Epoch 42, Loss: 0.6624, rho_lr: 0.002000\n",
            "rho0: 0.6939, rho1: 0.2000\n",
            "Epoch 43, Loss: 0.6608, rho_lr: 0.002000\n",
            "rho0: 0.6944, rho1: 0.2000\n",
            "Epoch 44, Loss: 0.6577, rho_lr: 0.002000\n",
            "rho0: 0.6949, rho1: 0.2000\n",
            "Epoch 45, Loss: 0.6535, rho_lr: 0.002000\n",
            "rho0: 0.6954, rho1: 0.2000\n",
            "Epoch 46, Loss: 0.6473, rho_lr: 0.002000\n",
            "rho0: 0.6959, rho1: 0.2000\n",
            "Epoch 47, Loss: 0.6479, rho_lr: 0.002000\n",
            "rho0: 0.6964, rho1: 0.2000\n",
            "Epoch 48, Loss: 0.6433, rho_lr: 0.002000\n",
            "rho0: 0.6969, rho1: 0.2000\n",
            "Epoch 49, Loss: 0.6382, rho_lr: 0.002000\n",
            "rho0: 0.6974, rho1: 0.2000\n",
            "Epoch 50, Loss: 0.6374, rho_lr: 0.002000\n",
            "rho0: 0.6980, rho1: 0.2000\n",
            "Epoch 51, Loss: 0.6352, rho_lr: 0.002000\n",
            "rho0: 0.6985, rho1: 0.2000\n",
            "Epoch 52, Loss: 0.6300, rho_lr: 0.002000\n",
            "rho0: 0.6991, rho1: 0.2000\n",
            "Epoch 53, Loss: 0.6270, rho_lr: 0.002000\n",
            "rho0: 0.6997, rho1: 0.2000\n",
            "Epoch 54, Loss: 0.6253, rho_lr: 0.002000\n",
            "rho0: 0.7003, rho1: 0.2000\n",
            "Epoch 55, Loss: 0.6222, rho_lr: 0.002000\n",
            "rho0: 0.7009, rho1: 0.2000\n",
            "Epoch 56, Loss: 0.6196, rho_lr: 0.002000\n",
            "rho0: 0.7015, rho1: 0.2000\n",
            "Epoch 57, Loss: 0.6164, rho_lr: 0.002000\n",
            "rho0: 0.7021, rho1: 0.2000\n",
            "Epoch 58, Loss: 0.6169, rho_lr: 0.002000\n",
            "rho0: 0.7027, rho1: 0.2000\n",
            "Epoch 59, Loss: 0.6143, rho_lr: 0.002000\n",
            "rho0: 0.7033, rho1: 0.2000\n",
            "Epoch 60, Loss: 0.6122, rho_lr: 0.000200\n",
            "rho0: 0.7039, rho1: 0.2000\n",
            "Epoch 1, Loss: 23.7504, rho_lr: 0.200000\n",
            "rho0: 0.1175, rho1: 0.5000\n",
            "Epoch 2, Loss: 9.8271, rho_lr: 0.200000\n",
            "rho0: 0.2585, rho1: 0.5000\n",
            "Epoch 3, Loss: 5.0458, rho_lr: 0.200000\n",
            "rho0: 0.3545, rho1: 0.5000\n",
            "Epoch 4, Loss: 3.2684, rho_lr: 0.200000\n",
            "rho0: 0.3508, rho1: 0.5000\n",
            "Epoch 5, Loss: 2.5653, rho_lr: 0.200000\n",
            "rho0: 0.3514, rho1: 0.5000\n",
            "Epoch 6, Loss: 2.2676, rho_lr: 0.200000\n",
            "rho0: 0.3320, rho1: 0.5000\n",
            "Epoch 7, Loss: 1.9662, rho_lr: 0.200000\n",
            "rho0: 0.3357, rho1: 0.5000\n",
            "Epoch 8, Loss: 1.8205, rho_lr: 0.200000\n",
            "rho0: 0.3466, rho1: 0.5000\n",
            "Epoch 9, Loss: 1.6626, rho_lr: 0.200000\n",
            "rho0: 0.3611, rho1: 0.5000\n",
            "Epoch 10, Loss: 1.6139, rho_lr: 0.200000\n",
            "rho0: 0.3596, rho1: 0.5000\n",
            "Epoch 11, Loss: 1.6873, rho_lr: 0.200000\n",
            "rho0: 0.4164, rho1: 0.3977\n",
            "Epoch 12, Loss: 1.2002, rho_lr: 0.200000\n",
            "rho0: 0.5446, rho1: 0.2368\n",
            "Epoch 13, Loss: 0.9062, rho_lr: 0.200000\n",
            "rho0: 0.6252, rho1: 0.2000\n",
            "Epoch 14, Loss: 0.8625, rho_lr: 0.200000\n",
            "rho0: 0.6068, rho1: 0.2000\n",
            "Epoch 15, Loss: 0.8238, rho_lr: 0.200000\n",
            "rho0: 0.6235, rho1: 0.2000\n",
            "Epoch 16, Loss: 0.8080, rho_lr: 0.200000\n",
            "rho0: 0.6389, rho1: 0.2000\n",
            "Epoch 17, Loss: 0.7902, rho_lr: 0.200000\n",
            "rho0: 0.6387, rho1: 0.2000\n",
            "Epoch 18, Loss: 0.7840, rho_lr: 0.200000\n",
            "rho0: 0.6509, rho1: 0.2000\n",
            "Epoch 19, Loss: 0.7716, rho_lr: 0.200000\n",
            "rho0: 0.6558, rho1: 0.2000\n",
            "Epoch 20, Loss: 0.7530, rho_lr: 0.020000\n",
            "rho0: 0.6668, rho1: 0.2000\n",
            "Epoch 21, Loss: 0.7483, rho_lr: 0.020000\n",
            "rho0: 0.6681, rho1: 0.2000\n",
            "Epoch 22, Loss: 0.7269, rho_lr: 0.020000\n",
            "rho0: 0.6707, rho1: 0.2000\n",
            "Epoch 23, Loss: 0.7274, rho_lr: 0.020000\n",
            "rho0: 0.6747, rho1: 0.2000\n",
            "Epoch 24, Loss: 0.7115, rho_lr: 0.020000\n",
            "rho0: 0.6791, rho1: 0.2000\n",
            "Epoch 25, Loss: 0.7058, rho_lr: 0.020000\n",
            "rho0: 0.6829, rho1: 0.2000\n",
            "Epoch 26, Loss: 0.6967, rho_lr: 0.020000\n",
            "rho0: 0.6868, rho1: 0.2000\n",
            "Epoch 27, Loss: 0.6877, rho_lr: 0.020000\n",
            "rho0: 0.6907, rho1: 0.2000\n",
            "Epoch 28, Loss: 0.6818, rho_lr: 0.020000\n",
            "rho0: 0.6945, rho1: 0.2000\n",
            "Epoch 29, Loss: 0.6744, rho_lr: 0.020000\n",
            "rho0: 0.6985, rho1: 0.2000\n",
            "Epoch 30, Loss: 0.6672, rho_lr: 0.020000\n",
            "rho0: 0.7025, rho1: 0.2000\n",
            "Epoch 31, Loss: 0.6661, rho_lr: 0.020000\n",
            "rho0: 0.7066, rho1: 0.2000\n",
            "Epoch 32, Loss: 0.6607, rho_lr: 0.020000\n",
            "rho0: 0.7103, rho1: 0.2000\n",
            "Epoch 33, Loss: 0.6555, rho_lr: 0.020000\n",
            "rho0: 0.7141, rho1: 0.2000\n",
            "Epoch 34, Loss: 0.6461, rho_lr: 0.020000\n",
            "rho0: 0.7182, rho1: 0.2000\n",
            "Epoch 35, Loss: 0.6412, rho_lr: 0.020000\n",
            "rho0: 0.7218, rho1: 0.2000\n",
            "Epoch 36, Loss: 0.6347, rho_lr: 0.020000\n",
            "rho0: 0.7252, rho1: 0.2000\n",
            "Epoch 37, Loss: 0.6326, rho_lr: 0.020000\n",
            "rho0: 0.7290, rho1: 0.2000\n",
            "Epoch 38, Loss: 0.6263, rho_lr: 0.020000\n",
            "rho0: 0.7325, rho1: 0.2000\n",
            "Epoch 39, Loss: 0.6201, rho_lr: 0.020000\n",
            "rho0: 0.7365, rho1: 0.2000\n",
            "Epoch 40, Loss: 0.6165, rho_lr: 0.002000\n",
            "rho0: 0.7400, rho1: 0.2000\n",
            "Epoch 41, Loss: 0.6108, rho_lr: 0.002000\n",
            "rho0: 0.7404, rho1: 0.2000\n",
            "Epoch 42, Loss: 0.6049, rho_lr: 0.002000\n",
            "rho0: 0.7409, rho1: 0.2000\n",
            "Epoch 43, Loss: 0.6101, rho_lr: 0.002000\n",
            "rho0: 0.7415, rho1: 0.2000\n",
            "Epoch 44, Loss: 0.6080, rho_lr: 0.002000\n",
            "rho0: 0.7421, rho1: 0.2000\n",
            "Epoch 45, Loss: 0.6014, rho_lr: 0.002000\n",
            "rho0: 0.7427, rho1: 0.2000\n",
            "Epoch 46, Loss: 0.5982, rho_lr: 0.002000\n",
            "rho0: 0.7433, rho1: 0.2000\n",
            "Epoch 47, Loss: 0.5995, rho_lr: 0.002000\n",
            "rho0: 0.7440, rho1: 0.2000\n",
            "Epoch 48, Loss: 0.5906, rho_lr: 0.002000\n",
            "rho0: 0.7446, rho1: 0.2000\n",
            "Epoch 49, Loss: 0.5918, rho_lr: 0.002000\n",
            "rho0: 0.7452, rho1: 0.2000\n",
            "Epoch 50, Loss: 0.5894, rho_lr: 0.002000\n",
            "rho0: 0.7459, rho1: 0.2000\n",
            "Epoch 51, Loss: 0.5903, rho_lr: 0.002000\n",
            "rho0: 0.7465, rho1: 0.2000\n",
            "Epoch 52, Loss: 0.5863, rho_lr: 0.002000\n",
            "rho0: 0.7472, rho1: 0.2000\n",
            "Epoch 53, Loss: 0.5841, rho_lr: 0.002000\n",
            "rho0: 0.7479, rho1: 0.2000\n",
            "Epoch 54, Loss: 0.5830, rho_lr: 0.002000\n",
            "rho0: 0.7486, rho1: 0.2000\n",
            "Epoch 55, Loss: 0.5797, rho_lr: 0.002000\n",
            "rho0: 0.7492, rho1: 0.2000\n",
            "Epoch 56, Loss: 0.5780, rho_lr: 0.002000\n",
            "rho0: 0.7499, rho1: 0.2000\n",
            "Epoch 57, Loss: 0.5747, rho_lr: 0.002000\n",
            "rho0: 0.7506, rho1: 0.2000\n",
            "Epoch 58, Loss: 0.5737, rho_lr: 0.002000\n",
            "rho0: 0.7513, rho1: 0.2000\n",
            "Epoch 59, Loss: 0.5719, rho_lr: 0.002000\n",
            "rho0: 0.7520, rho1: 0.2000\n",
            "Epoch 60, Loss: 0.5737, rho_lr: 0.000200\n",
            "rho0: 0.7527, rho1: 0.2000\n",
            "Epoch 1, Loss: 36.3888, rho_lr: 0.200000\n",
            "rho0: 0.0000, rho1: 0.5000\n",
            "Epoch 2, Loss: 14.5992, rho_lr: 0.200000\n",
            "rho0: 0.2616, rho1: 0.5000\n",
            "Epoch 3, Loss: 9.0476, rho_lr: 0.200000\n",
            "rho0: 0.2465, rho1: 0.5000\n",
            "Epoch 4, Loss: 5.2606, rho_lr: 0.200000\n",
            "rho0: 0.3396, rho1: 0.5000\n",
            "Epoch 5, Loss: 3.7020, rho_lr: 0.200000\n",
            "rho0: 0.3418, rho1: 0.5000\n",
            "Epoch 6, Loss: 2.9087, rho_lr: 0.200000\n",
            "rho0: 0.3440, rho1: 0.5000\n",
            "Epoch 7, Loss: 2.4889, rho_lr: 0.200000\n",
            "rho0: 0.3364, rho1: 0.5000\n",
            "Epoch 8, Loss: 2.1747, rho_lr: 0.200000\n",
            "rho0: 0.3449, rho1: 0.5000\n",
            "Epoch 9, Loss: 1.9999, rho_lr: 0.200000\n",
            "rho0: 0.3392, rho1: 0.5000\n",
            "Epoch 10, Loss: 1.8259, rho_lr: 0.200000\n",
            "rho0: 0.3359, rho1: 0.5000\n",
            "Epoch 11, Loss: 1.9961, rho_lr: 0.200000\n",
            "rho0: 0.3794, rho1: 0.3954\n",
            "Epoch 12, Loss: 1.5004, rho_lr: 0.200000\n",
            "rho0: 0.4751, rho1: 0.3111\n",
            "Epoch 13, Loss: 1.1216, rho_lr: 0.200000\n",
            "rho0: 0.5729, rho1: 0.2000\n",
            "Epoch 14, Loss: 0.9982, rho_lr: 0.200000\n",
            "rho0: 0.5784, rho1: 0.2000\n",
            "Epoch 15, Loss: 0.9561, rho_lr: 0.200000\n",
            "rho0: 0.5734, rho1: 0.2000\n",
            "Epoch 16, Loss: 0.9421, rho_lr: 0.200000\n",
            "rho0: 0.5805, rho1: 0.2000\n",
            "Epoch 17, Loss: 0.9213, rho_lr: 0.200000\n",
            "rho0: 0.5920, rho1: 0.2000\n",
            "Epoch 18, Loss: 0.8963, rho_lr: 0.200000\n",
            "rho0: 0.5888, rho1: 0.2000\n",
            "Epoch 19, Loss: 0.8883, rho_lr: 0.200000\n",
            "rho0: 0.5960, rho1: 0.2000\n",
            "Epoch 20, Loss: 0.8825, rho_lr: 0.020000\n",
            "rho0: 0.6024, rho1: 0.2000\n",
            "Epoch 21, Loss: 0.8717, rho_lr: 0.020000\n",
            "rho0: 0.6021, rho1: 0.2000\n",
            "Epoch 22, Loss: 0.8605, rho_lr: 0.020000\n",
            "rho0: 0.6032, rho1: 0.2000\n",
            "Epoch 23, Loss: 0.8473, rho_lr: 0.020000\n",
            "rho0: 0.6059, rho1: 0.2000\n",
            "Epoch 24, Loss: 0.8343, rho_lr: 0.020000\n",
            "rho0: 0.6089, rho1: 0.2000\n",
            "Epoch 25, Loss: 0.8263, rho_lr: 0.020000\n",
            "rho0: 0.6119, rho1: 0.2000\n",
            "Epoch 26, Loss: 0.8154, rho_lr: 0.020000\n",
            "rho0: 0.6145, rho1: 0.2000\n",
            "Epoch 27, Loss: 0.8085, rho_lr: 0.020000\n",
            "rho0: 0.6170, rho1: 0.2000\n",
            "Epoch 28, Loss: 0.7964, rho_lr: 0.020000\n",
            "rho0: 0.6205, rho1: 0.2000\n",
            "Epoch 29, Loss: 0.7822, rho_lr: 0.020000\n",
            "rho0: 0.6238, rho1: 0.2000\n",
            "Epoch 30, Loss: 0.7812, rho_lr: 0.020000\n",
            "rho0: 0.6260, rho1: 0.2000\n",
            "Epoch 31, Loss: 0.7813, rho_lr: 0.020000\n",
            "rho0: 0.6283, rho1: 0.2000\n",
            "Epoch 32, Loss: 0.7624, rho_lr: 0.020000\n",
            "rho0: 0.6319, rho1: 0.2000\n",
            "Epoch 33, Loss: 0.7556, rho_lr: 0.020000\n",
            "rho0: 0.6353, rho1: 0.2000\n",
            "Epoch 34, Loss: 0.7515, rho_lr: 0.020000\n",
            "rho0: 0.6377, rho1: 0.2000\n",
            "Epoch 35, Loss: 0.7410, rho_lr: 0.020000\n",
            "rho0: 0.6407, rho1: 0.2000\n",
            "Epoch 36, Loss: 0.7384, rho_lr: 0.020000\n",
            "rho0: 0.6439, rho1: 0.2000\n",
            "Epoch 37, Loss: 0.7289, rho_lr: 0.020000\n",
            "rho0: 0.6470, rho1: 0.2000\n",
            "Epoch 38, Loss: 0.7217, rho_lr: 0.020000\n",
            "rho0: 0.6503, rho1: 0.2000\n",
            "Epoch 39, Loss: 0.7152, rho_lr: 0.020000\n",
            "rho0: 0.6534, rho1: 0.2000\n",
            "Epoch 40, Loss: 0.7100, rho_lr: 0.002000\n",
            "rho0: 0.6566, rho1: 0.2000\n",
            "Epoch 41, Loss: 0.6994, rho_lr: 0.002000\n",
            "rho0: 0.6569, rho1: 0.2000\n",
            "Epoch 42, Loss: 0.7018, rho_lr: 0.002000\n",
            "rho0: 0.6573, rho1: 0.2000\n",
            "Epoch 43, Loss: 0.6930, rho_lr: 0.002000\n",
            "rho0: 0.6578, rho1: 0.2000\n",
            "Epoch 44, Loss: 0.6880, rho_lr: 0.002000\n",
            "rho0: 0.6583, rho1: 0.2000\n",
            "Epoch 45, Loss: 0.6847, rho_lr: 0.002000\n",
            "rho0: 0.6589, rho1: 0.2000\n",
            "Epoch 46, Loss: 0.6844, rho_lr: 0.002000\n",
            "rho0: 0.6595, rho1: 0.2000\n",
            "Epoch 47, Loss: 0.6785, rho_lr: 0.002000\n",
            "rho0: 0.6602, rho1: 0.2000\n",
            "Epoch 48, Loss: 0.6784, rho_lr: 0.002000\n",
            "rho0: 0.6608, rho1: 0.2000\n",
            "Epoch 49, Loss: 0.6772, rho_lr: 0.002000\n",
            "rho0: 0.6614, rho1: 0.2000\n",
            "Epoch 50, Loss: 0.6672, rho_lr: 0.002000\n",
            "rho0: 0.6620, rho1: 0.2000\n",
            "Epoch 51, Loss: 0.6679, rho_lr: 0.002000\n",
            "rho0: 0.6627, rho1: 0.2000\n",
            "Epoch 52, Loss: 0.6577, rho_lr: 0.002000\n",
            "rho0: 0.6633, rho1: 0.2000\n",
            "Epoch 53, Loss: 0.6567, rho_lr: 0.002000\n",
            "rho0: 0.6640, rho1: 0.2000\n",
            "Epoch 54, Loss: 0.6590, rho_lr: 0.002000\n",
            "rho0: 0.6646, rho1: 0.2000\n",
            "Epoch 55, Loss: 0.6529, rho_lr: 0.002000\n",
            "rho0: 0.6654, rho1: 0.2000\n",
            "Epoch 56, Loss: 0.6498, rho_lr: 0.002000\n",
            "rho0: 0.6661, rho1: 0.2000\n",
            "Epoch 57, Loss: 0.6516, rho_lr: 0.002000\n",
            "rho0: 0.6669, rho1: 0.2000\n",
            "Epoch 58, Loss: 0.6433, rho_lr: 0.002000\n",
            "rho0: 0.6676, rho1: 0.2000\n",
            "Epoch 59, Loss: 0.6397, rho_lr: 0.002000\n",
            "rho0: 0.6683, rho1: 0.2000\n",
            "Epoch 60, Loss: 0.6397, rho_lr: 0.000200\n",
            "rho0: 0.6690, rho1: 0.2000\n",
            "Epoch 1, Loss: 30.9503, rho_lr: 0.200000\n",
            "rho0: 0.0000, rho1: 0.5000\n",
            "Epoch 2, Loss: 16.0070, rho_lr: 0.200000\n",
            "rho0: 0.1066, rho1: 0.5000\n",
            "Epoch 3, Loss: 10.4148, rho_lr: 0.200000\n",
            "rho0: 0.2400, rho1: 0.5000\n",
            "Epoch 4, Loss: 6.1872, rho_lr: 0.200000\n",
            "rho0: 0.2473, rho1: 0.5000\n",
            "Epoch 5, Loss: 4.3068, rho_lr: 0.200000\n",
            "rho0: 0.3208, rho1: 0.5000\n",
            "Epoch 6, Loss: 3.2013, rho_lr: 0.200000\n",
            "rho0: 0.3270, rho1: 0.5000\n",
            "Epoch 7, Loss: 2.8064, rho_lr: 0.200000\n",
            "rho0: 0.3398, rho1: 0.5000\n",
            "Epoch 8, Loss: 2.5846, rho_lr: 0.200000\n",
            "rho0: 0.3354, rho1: 0.5000\n",
            "Epoch 9, Loss: 2.2587, rho_lr: 0.200000\n",
            "rho0: 0.3413, rho1: 0.5000\n",
            "Epoch 10, Loss: 2.0995, rho_lr: 0.200000\n",
            "rho0: 0.3395, rho1: 0.5000\n",
            "Epoch 11, Loss: 2.0104, rho_lr: 0.200000\n",
            "rho0: 0.4255, rho1: 0.3657\n",
            "Epoch 12, Loss: 1.4488, rho_lr: 0.200000\n",
            "rho0: 0.5531, rho1: 0.2000\n",
            "Epoch 13, Loss: 1.1127, rho_lr: 0.200000\n",
            "rho0: 0.5880, rho1: 0.2000\n",
            "Epoch 14, Loss: 1.0562, rho_lr: 0.200000\n",
            "rho0: 0.5803, rho1: 0.2000\n",
            "Epoch 15, Loss: 1.0278, rho_lr: 0.200000\n",
            "rho0: 0.5887, rho1: 0.2000\n",
            "Epoch 16, Loss: 1.0060, rho_lr: 0.200000\n",
            "rho0: 0.5924, rho1: 0.2000\n",
            "Epoch 17, Loss: 0.9867, rho_lr: 0.200000\n",
            "rho0: 0.5985, rho1: 0.2000\n",
            "Epoch 18, Loss: 0.9656, rho_lr: 0.200000\n",
            "rho0: 0.6074, rho1: 0.2000\n",
            "Epoch 19, Loss: 0.9495, rho_lr: 0.200000\n",
            "rho0: 0.6105, rho1: 0.2000\n",
            "Epoch 20, Loss: 0.9227, rho_lr: 0.020000\n",
            "rho0: 0.6180, rho1: 0.2000\n",
            "Epoch 21, Loss: 0.9240, rho_lr: 0.020000\n",
            "rho0: 0.6181, rho1: 0.2000\n",
            "Epoch 22, Loss: 0.9061, rho_lr: 0.020000\n",
            "rho0: 0.6194, rho1: 0.2000\n",
            "Epoch 23, Loss: 0.8972, rho_lr: 0.020000\n",
            "rho0: 0.6218, rho1: 0.2000\n",
            "Epoch 24, Loss: 0.8839, rho_lr: 0.020000\n",
            "rho0: 0.6252, rho1: 0.2000\n",
            "Epoch 25, Loss: 0.8682, rho_lr: 0.020000\n",
            "rho0: 0.6281, rho1: 0.2000\n",
            "Epoch 26, Loss: 0.8658, rho_lr: 0.020000\n",
            "rho0: 0.6310, rho1: 0.2000\n",
            "Epoch 27, Loss: 0.8612, rho_lr: 0.020000\n",
            "rho0: 0.6336, rho1: 0.2000\n",
            "Epoch 28, Loss: 0.8411, rho_lr: 0.020000\n",
            "rho0: 0.6370, rho1: 0.2000\n",
            "Epoch 29, Loss: 0.8360, rho_lr: 0.020000\n",
            "rho0: 0.6397, rho1: 0.2000\n",
            "Epoch 30, Loss: 0.8225, rho_lr: 0.020000\n",
            "rho0: 0.6423, rho1: 0.2000\n",
            "Epoch 31, Loss: 0.8218, rho_lr: 0.020000\n",
            "rho0: 0.6450, rho1: 0.2000\n",
            "Epoch 32, Loss: 0.8151, rho_lr: 0.020000\n",
            "rho0: 0.6474, rho1: 0.2000\n",
            "Epoch 33, Loss: 0.7975, rho_lr: 0.020000\n",
            "rho0: 0.6502, rho1: 0.2000\n",
            "Epoch 34, Loss: 0.7911, rho_lr: 0.020000\n",
            "rho0: 0.6532, rho1: 0.2000\n",
            "Epoch 35, Loss: 0.7864, rho_lr: 0.020000\n",
            "rho0: 0.6564, rho1: 0.2000\n",
            "Epoch 36, Loss: 0.7738, rho_lr: 0.020000\n",
            "rho0: 0.6592, rho1: 0.2000\n",
            "Epoch 37, Loss: 0.7709, rho_lr: 0.020000\n",
            "rho0: 0.6619, rho1: 0.2000\n",
            "Epoch 38, Loss: 0.7581, rho_lr: 0.020000\n",
            "rho0: 0.6654, rho1: 0.2000\n",
            "Epoch 39, Loss: 0.7517, rho_lr: 0.020000\n",
            "rho0: 0.6683, rho1: 0.2000\n",
            "Epoch 40, Loss: 0.7499, rho_lr: 0.002000\n",
            "rho0: 0.6710, rho1: 0.2000\n",
            "Epoch 41, Loss: 0.7464, rho_lr: 0.002000\n",
            "rho0: 0.6712, rho1: 0.2000\n",
            "Epoch 42, Loss: 0.7358, rho_lr: 0.002000\n",
            "rho0: 0.6716, rho1: 0.2000\n",
            "Epoch 43, Loss: 0.7225, rho_lr: 0.002000\n",
            "rho0: 0.6720, rho1: 0.2000\n",
            "Epoch 44, Loss: 0.7278, rho_lr: 0.002000\n",
            "rho0: 0.6726, rho1: 0.2000\n",
            "Epoch 45, Loss: 0.7217, rho_lr: 0.002000\n",
            "rho0: 0.6731, rho1: 0.2000\n",
            "Epoch 46, Loss: 0.7160, rho_lr: 0.002000\n",
            "rho0: 0.6737, rho1: 0.2000\n",
            "Epoch 47, Loss: 0.7065, rho_lr: 0.002000\n",
            "rho0: 0.6742, rho1: 0.2000\n",
            "Epoch 48, Loss: 0.7032, rho_lr: 0.002000\n",
            "rho0: 0.6748, rho1: 0.2000\n",
            "Epoch 49, Loss: 0.6998, rho_lr: 0.002000\n",
            "rho0: 0.6753, rho1: 0.2000\n",
            "Epoch 50, Loss: 0.6990, rho_lr: 0.002000\n",
            "rho0: 0.6760, rho1: 0.2000\n",
            "Epoch 51, Loss: 0.6915, rho_lr: 0.002000\n",
            "rho0: 0.6767, rho1: 0.2000\n",
            "Epoch 52, Loss: 0.6880, rho_lr: 0.002000\n",
            "rho0: 0.6773, rho1: 0.2000\n",
            "Epoch 53, Loss: 0.6853, rho_lr: 0.002000\n",
            "rho0: 0.6780, rho1: 0.2000\n",
            "Epoch 54, Loss: 0.6779, rho_lr: 0.002000\n",
            "rho0: 0.6786, rho1: 0.2000\n",
            "Epoch 55, Loss: 0.6760, rho_lr: 0.002000\n",
            "rho0: 0.6792, rho1: 0.2000\n",
            "Epoch 56, Loss: 0.6755, rho_lr: 0.002000\n",
            "rho0: 0.6799, rho1: 0.2000\n",
            "Epoch 57, Loss: 0.6732, rho_lr: 0.002000\n",
            "rho0: 0.6807, rho1: 0.2000\n",
            "Epoch 58, Loss: 0.6638, rho_lr: 0.002000\n",
            "rho0: 0.6813, rho1: 0.2000\n",
            "Epoch 59, Loss: 0.6636, rho_lr: 0.002000\n",
            "rho0: 0.6820, rho1: 0.2000\n",
            "Epoch 60, Loss: 0.6590, rho_lr: 0.000200\n",
            "rho0: 0.6827, rho1: 0.2000\n",
            "Epoch 1, Loss: 30.8767, rho_lr: 0.200000\n",
            "rho0: 0.0000, rho1: 0.5000\n",
            "Epoch 2, Loss: 14.4576, rho_lr: 0.200000\n",
            "rho0: 0.3536, rho1: 0.5000\n",
            "Epoch 3, Loss: 7.9361, rho_lr: 0.200000\n",
            "rho0: 0.2665, rho1: 0.5000\n",
            "Epoch 4, Loss: 4.7518, rho_lr: 0.200000\n",
            "rho0: 0.3444, rho1: 0.5000\n",
            "Epoch 5, Loss: 3.4872, rho_lr: 0.200000\n",
            "rho0: 0.3480, rho1: 0.5000\n",
            "Epoch 6, Loss: 2.8585, rho_lr: 0.200000\n",
            "rho0: 0.3287, rho1: 0.5000\n",
            "Epoch 7, Loss: 2.5387, rho_lr: 0.200000\n",
            "rho0: 0.3414, rho1: 0.5000\n",
            "Epoch 8, Loss: 2.2789, rho_lr: 0.200000\n",
            "rho0: 0.3444, rho1: 0.5000\n",
            "Epoch 9, Loss: 2.0990, rho_lr: 0.200000\n",
            "rho0: 0.3515, rho1: 0.5000\n",
            "Epoch 10, Loss: 1.9784, rho_lr: 0.200000\n",
            "rho0: 0.3541, rho1: 0.5000\n",
            "Epoch 11, Loss: 1.9537, rho_lr: 0.200000\n",
            "rho0: 0.4449, rho1: 0.3650\n",
            "Epoch 12, Loss: 1.3420, rho_lr: 0.200000\n",
            "rho0: 0.5853, rho1: 0.2000\n",
            "Epoch 13, Loss: 1.0404, rho_lr: 0.200000\n",
            "rho0: 0.5939, rho1: 0.2000\n",
            "Epoch 14, Loss: 1.0112, rho_lr: 0.200000\n",
            "rho0: 0.6090, rho1: 0.2000\n",
            "Epoch 15, Loss: 0.9726, rho_lr: 0.200000\n",
            "rho0: 0.6136, rho1: 0.2000\n",
            "Epoch 16, Loss: 0.9515, rho_lr: 0.200000\n",
            "rho0: 0.6239, rho1: 0.2000\n",
            "Epoch 17, Loss: 0.9285, rho_lr: 0.200000\n",
            "rho0: 0.6281, rho1: 0.2000\n",
            "Epoch 18, Loss: 0.9047, rho_lr: 0.200000\n",
            "rho0: 0.6384, rho1: 0.2000\n",
            "Epoch 19, Loss: 0.8993, rho_lr: 0.200000\n",
            "rho0: 0.6422, rho1: 0.2000\n",
            "Epoch 20, Loss: 0.8716, rho_lr: 0.020000\n",
            "rho0: 0.6516, rho1: 0.2000\n",
            "Epoch 21, Loss: 0.8703, rho_lr: 0.020000\n",
            "rho0: 0.6525, rho1: 0.2000\n",
            "Epoch 22, Loss: 0.8521, rho_lr: 0.020000\n",
            "rho0: 0.6551, rho1: 0.2000\n",
            "Epoch 23, Loss: 0.8349, rho_lr: 0.020000\n",
            "rho0: 0.6584, rho1: 0.2000\n",
            "Epoch 24, Loss: 0.8399, rho_lr: 0.020000\n",
            "rho0: 0.6617, rho1: 0.2000\n",
            "Epoch 25, Loss: 0.8143, rho_lr: 0.020000\n",
            "rho0: 0.6654, rho1: 0.2000\n",
            "Epoch 26, Loss: 0.8079, rho_lr: 0.020000\n",
            "rho0: 0.6692, rho1: 0.2000\n",
            "Epoch 27, Loss: 0.7933, rho_lr: 0.020000\n",
            "rho0: 0.6728, rho1: 0.2000\n",
            "Epoch 28, Loss: 0.7846, rho_lr: 0.020000\n",
            "rho0: 0.6765, rho1: 0.2000\n",
            "Epoch 29, Loss: 0.7788, rho_lr: 0.020000\n",
            "rho0: 0.6802, rho1: 0.2000\n",
            "Epoch 30, Loss: 0.7757, rho_lr: 0.020000\n",
            "rho0: 0.6838, rho1: 0.2000\n",
            "Epoch 31, Loss: 0.7684, rho_lr: 0.020000\n",
            "rho0: 0.6875, rho1: 0.2000\n",
            "Epoch 32, Loss: 0.7542, rho_lr: 0.020000\n",
            "rho0: 0.6913, rho1: 0.2000\n",
            "Epoch 33, Loss: 0.7449, rho_lr: 0.020000\n",
            "rho0: 0.6951, rho1: 0.2000\n",
            "Epoch 34, Loss: 0.7352, rho_lr: 0.020000\n",
            "rho0: 0.6982, rho1: 0.2000\n",
            "Epoch 35, Loss: 0.7318, rho_lr: 0.020000\n",
            "rho0: 0.7014, rho1: 0.2000\n",
            "Epoch 36, Loss: 0.7283, rho_lr: 0.020000\n",
            "rho0: 0.7049, rho1: 0.2000\n",
            "Epoch 37, Loss: 0.7155, rho_lr: 0.020000\n",
            "rho0: 0.7083, rho1: 0.2000\n",
            "Epoch 38, Loss: 0.7132, rho_lr: 0.020000\n",
            "rho0: 0.7118, rho1: 0.2000\n",
            "Epoch 39, Loss: 0.7019, rho_lr: 0.020000\n",
            "rho0: 0.7152, rho1: 0.2000\n",
            "Epoch 40, Loss: 0.6934, rho_lr: 0.002000\n",
            "rho0: 0.7183, rho1: 0.2000\n",
            "Epoch 41, Loss: 0.6923, rho_lr: 0.002000\n",
            "rho0: 0.7186, rho1: 0.2000\n",
            "Epoch 42, Loss: 0.6922, rho_lr: 0.002000\n",
            "rho0: 0.7191, rho1: 0.2000\n",
            "Epoch 43, Loss: 0.6816, rho_lr: 0.002000\n",
            "rho0: 0.7196, rho1: 0.2000\n",
            "Epoch 44, Loss: 0.6786, rho_lr: 0.002000\n",
            "rho0: 0.7202, rho1: 0.2000\n",
            "Epoch 45, Loss: 0.6709, rho_lr: 0.002000\n",
            "rho0: 0.7208, rho1: 0.2000\n",
            "Epoch 46, Loss: 0.6667, rho_lr: 0.002000\n",
            "rho0: 0.7213, rho1: 0.2000\n",
            "Epoch 47, Loss: 0.6630, rho_lr: 0.002000\n",
            "rho0: 0.7219, rho1: 0.2000\n",
            "Epoch 48, Loss: 0.6625, rho_lr: 0.002000\n",
            "rho0: 0.7226, rho1: 0.2000\n",
            "Epoch 49, Loss: 0.6569, rho_lr: 0.002000\n",
            "rho0: 0.7232, rho1: 0.2000\n",
            "Epoch 50, Loss: 0.6527, rho_lr: 0.002000\n",
            "rho0: 0.7239, rho1: 0.2000\n",
            "Epoch 51, Loss: 0.6523, rho_lr: 0.002000\n",
            "rho0: 0.7245, rho1: 0.2000\n",
            "Epoch 52, Loss: 0.6498, rho_lr: 0.002000\n",
            "rho0: 0.7251, rho1: 0.2000\n",
            "Epoch 53, Loss: 0.6523, rho_lr: 0.002000\n",
            "rho0: 0.7258, rho1: 0.2000\n",
            "Epoch 54, Loss: 0.6447, rho_lr: 0.002000\n",
            "rho0: 0.7265, rho1: 0.2000\n",
            "Epoch 55, Loss: 0.6410, rho_lr: 0.002000\n",
            "rho0: 0.7272, rho1: 0.2000\n",
            "Epoch 56, Loss: 0.6388, rho_lr: 0.002000\n",
            "rho0: 0.7278, rho1: 0.2000\n",
            "Epoch 57, Loss: 0.6338, rho_lr: 0.002000\n",
            "rho0: 0.7286, rho1: 0.2000\n",
            "Epoch 58, Loss: 0.6330, rho_lr: 0.002000\n",
            "rho0: 0.7293, rho1: 0.2000\n",
            "Epoch 59, Loss: 0.6291, rho_lr: 0.002000\n",
            "rho0: 0.7300, rho1: 0.2000\n",
            "Epoch 60, Loss: 0.6277, rho_lr: 0.000200\n",
            "rho0: 0.7307, rho1: 0.2000\n",
            "Epoch 1, Loss: 42.6252, rho_lr: 0.200000\n",
            "rho0: 0.0000, rho1: 0.5000\n",
            "Epoch 2, Loss: 15.9406, rho_lr: 0.200000\n",
            "rho0: 0.1791, rho1: 0.5000\n",
            "Epoch 3, Loss: 9.6539, rho_lr: 0.200000\n",
            "rho0: 0.2632, rho1: 0.5000\n",
            "Epoch 4, Loss: 6.1899, rho_lr: 0.200000\n",
            "rho0: 0.2783, rho1: 0.5000\n",
            "Epoch 5, Loss: 4.9390, rho_lr: 0.200000\n",
            "rho0: 0.2823, rho1: 0.5000\n",
            "Epoch 6, Loss: 3.5793, rho_lr: 0.200000\n",
            "rho0: 0.3056, rho1: 0.5000\n",
            "Epoch 7, Loss: 2.9175, rho_lr: 0.200000\n",
            "rho0: 0.2896, rho1: 0.5000\n",
            "Epoch 8, Loss: 2.6369, rho_lr: 0.200000\n",
            "rho0: 0.2839, rho1: 0.5000\n",
            "Epoch 9, Loss: 2.3583, rho_lr: 0.200000\n",
            "rho0: 0.2911, rho1: 0.5000\n",
            "Epoch 10, Loss: 2.2447, rho_lr: 0.200000\n",
            "rho0: 0.3019, rho1: 0.5000\n",
            "Epoch 11, Loss: 2.2027, rho_lr: 0.200000\n",
            "rho0: 0.3613, rho1: 0.3752\n",
            "Epoch 12, Loss: 1.6424, rho_lr: 0.200000\n",
            "rho0: 0.4680, rho1: 0.2104\n",
            "Epoch 13, Loss: 1.2203, rho_lr: 0.200000\n",
            "rho0: 0.5382, rho1: 0.2000\n",
            "Epoch 14, Loss: 1.1942, rho_lr: 0.200000\n",
            "rho0: 0.5250, rho1: 0.2000\n",
            "Epoch 15, Loss: 1.1625, rho_lr: 0.200000\n",
            "rho0: 0.5399, rho1: 0.2000\n",
            "Epoch 16, Loss: 1.1091, rho_lr: 0.200000\n",
            "rho0: 0.5439, rho1: 0.2000\n",
            "Epoch 17, Loss: 1.0969, rho_lr: 0.200000\n",
            "rho0: 0.5488, rho1: 0.2000\n",
            "Epoch 18, Loss: 1.0611, rho_lr: 0.200000\n",
            "rho0: 0.5573, rho1: 0.2000\n",
            "Epoch 19, Loss: 1.0623, rho_lr: 0.200000\n",
            "rho0: 0.5603, rho1: 0.2000\n",
            "Epoch 20, Loss: 1.0457, rho_lr: 0.020000\n",
            "rho0: 0.5671, rho1: 0.2000\n",
            "Epoch 21, Loss: 1.0202, rho_lr: 0.020000\n",
            "rho0: 0.5683, rho1: 0.2000\n",
            "Epoch 22, Loss: 0.9866, rho_lr: 0.020000\n",
            "rho0: 0.5699, rho1: 0.2000\n",
            "Epoch 23, Loss: 0.9865, rho_lr: 0.020000\n",
            "rho0: 0.5721, rho1: 0.2000\n",
            "Epoch 24, Loss: 0.9671, rho_lr: 0.020000\n",
            "rho0: 0.5749, rho1: 0.2000\n",
            "Epoch 25, Loss: 0.9514, rho_lr: 0.020000\n",
            "rho0: 0.5779, rho1: 0.2000\n",
            "Epoch 26, Loss: 0.9367, rho_lr: 0.020000\n",
            "rho0: 0.5807, rho1: 0.2000\n",
            "Epoch 27, Loss: 0.9338, rho_lr: 0.020000\n",
            "rho0: 0.5838, rho1: 0.2000\n",
            "Epoch 28, Loss: 0.9204, rho_lr: 0.020000\n",
            "rho0: 0.5871, rho1: 0.2000\n",
            "Epoch 29, Loss: 0.9016, rho_lr: 0.020000\n",
            "rho0: 0.5898, rho1: 0.2000\n",
            "Epoch 30, Loss: 0.8906, rho_lr: 0.020000\n",
            "rho0: 0.5929, rho1: 0.2000\n",
            "Epoch 31, Loss: 0.8787, rho_lr: 0.020000\n",
            "rho0: 0.5962, rho1: 0.2000\n",
            "Epoch 32, Loss: 0.8745, rho_lr: 0.020000\n",
            "rho0: 0.5992, rho1: 0.2000\n",
            "Epoch 33, Loss: 0.8647, rho_lr: 0.020000\n",
            "rho0: 0.6027, rho1: 0.2000\n",
            "Epoch 34, Loss: 0.8538, rho_lr: 0.020000\n",
            "rho0: 0.6059, rho1: 0.2000\n",
            "Epoch 35, Loss: 0.8463, rho_lr: 0.020000\n",
            "rho0: 0.6091, rho1: 0.2000\n",
            "Epoch 36, Loss: 0.8338, rho_lr: 0.020000\n",
            "rho0: 0.6126, rho1: 0.2000\n",
            "Epoch 37, Loss: 0.8221, rho_lr: 0.020000\n",
            "rho0: 0.6162, rho1: 0.2000\n",
            "Epoch 38, Loss: 0.8111, rho_lr: 0.020000\n",
            "rho0: 0.6197, rho1: 0.2000\n",
            "Epoch 39, Loss: 0.8095, rho_lr: 0.020000\n",
            "rho0: 0.6231, rho1: 0.2000\n",
            "Epoch 40, Loss: 0.7979, rho_lr: 0.002000\n",
            "rho0: 0.6263, rho1: 0.2000\n",
            "Epoch 41, Loss: 0.7934, rho_lr: 0.002000\n",
            "rho0: 0.6267, rho1: 0.2000\n",
            "Epoch 42, Loss: 0.7781, rho_lr: 0.002000\n",
            "rho0: 0.6271, rho1: 0.2000\n",
            "Epoch 43, Loss: 0.7765, rho_lr: 0.002000\n",
            "rho0: 0.6275, rho1: 0.2000\n",
            "Epoch 44, Loss: 0.7649, rho_lr: 0.002000\n",
            "rho0: 0.6281, rho1: 0.2000\n",
            "Epoch 45, Loss: 0.7701, rho_lr: 0.002000\n",
            "rho0: 0.6287, rho1: 0.2000\n",
            "Epoch 46, Loss: 0.7525, rho_lr: 0.002000\n",
            "rho0: 0.6293, rho1: 0.2000\n",
            "Epoch 47, Loss: 0.7566, rho_lr: 0.002000\n",
            "rho0: 0.6299, rho1: 0.2000\n",
            "Epoch 48, Loss: 0.7480, rho_lr: 0.002000\n",
            "rho0: 0.6305, rho1: 0.2000\n",
            "Epoch 49, Loss: 0.7388, rho_lr: 0.002000\n",
            "rho0: 0.6311, rho1: 0.2000\n",
            "Epoch 50, Loss: 0.7415, rho_lr: 0.002000\n",
            "rho0: 0.6317, rho1: 0.2000\n",
            "Epoch 51, Loss: 0.7311, rho_lr: 0.002000\n",
            "rho0: 0.6323, rho1: 0.2000\n",
            "Epoch 52, Loss: 0.7272, rho_lr: 0.002000\n",
            "rho0: 0.6330, rho1: 0.2000\n",
            "Epoch 53, Loss: 0.7237, rho_lr: 0.002000\n",
            "rho0: 0.6336, rho1: 0.2000\n",
            "Epoch 54, Loss: 0.7276, rho_lr: 0.002000\n",
            "rho0: 0.6343, rho1: 0.2000\n",
            "Epoch 55, Loss: 0.7110, rho_lr: 0.002000\n",
            "rho0: 0.6350, rho1: 0.2000\n",
            "Epoch 56, Loss: 0.7100, rho_lr: 0.002000\n",
            "rho0: 0.6357, rho1: 0.2000\n",
            "Epoch 57, Loss: 0.7087, rho_lr: 0.002000\n",
            "rho0: 0.6364, rho1: 0.2000\n",
            "Epoch 58, Loss: 0.7060, rho_lr: 0.002000\n",
            "rho0: 0.6370, rho1: 0.2000\n",
            "Epoch 59, Loss: 0.6969, rho_lr: 0.002000\n",
            "rho0: 0.6377, rho1: 0.2000\n",
            "Epoch 60, Loss: 0.6986, rho_lr: 0.000200\n",
            "rho0: 0.6384, rho1: 0.2000\n",
            "Epoch 1, Loss: 34.5825, rho_lr: 0.200000\n",
            "rho0: 0.0000, rho1: 0.5000\n",
            "Epoch 2, Loss: 13.3458, rho_lr: 0.200000\n",
            "rho0: 0.3447, rho1: 0.5000\n",
            "Epoch 3, Loss: 7.2393, rho_lr: 0.200000\n",
            "rho0: 0.3111, rho1: 0.5000\n",
            "Epoch 4, Loss: 4.8480, rho_lr: 0.200000\n",
            "rho0: 0.3544, rho1: 0.5000\n",
            "Epoch 5, Loss: 3.5011, rho_lr: 0.200000\n",
            "rho0: 0.3318, rho1: 0.5000\n",
            "Epoch 6, Loss: 2.8472, rho_lr: 0.200000\n",
            "rho0: 0.3364, rho1: 0.5000\n",
            "Epoch 7, Loss: 2.4472, rho_lr: 0.200000\n",
            "rho0: 0.3460, rho1: 0.5000\n",
            "Epoch 8, Loss: 2.3022, rho_lr: 0.200000\n",
            "rho0: 0.3434, rho1: 0.5000\n",
            "Epoch 9, Loss: 2.2485, rho_lr: 0.200000\n",
            "rho0: 0.3370, rho1: 0.5000\n",
            "Epoch 10, Loss: 1.9862, rho_lr: 0.200000\n",
            "rho0: 0.3399, rho1: 0.5000\n",
            "Epoch 11, Loss: 2.0126, rho_lr: 0.200000\n",
            "rho0: 0.3976, rho1: 0.4011\n",
            "Epoch 12, Loss: 1.4873, rho_lr: 0.200000\n",
            "rho0: 0.5146, rho1: 0.2451\n",
            "Epoch 13, Loss: 1.0958, rho_lr: 0.200000\n",
            "rho0: 0.5880, rho1: 0.2000\n",
            "Epoch 14, Loss: 1.0229, rho_lr: 0.200000\n",
            "rho0: 0.5788, rho1: 0.2000\n",
            "Epoch 15, Loss: 0.9866, rho_lr: 0.200000\n",
            "rho0: 0.5952, rho1: 0.2000\n",
            "Epoch 16, Loss: 0.9690, rho_lr: 0.200000\n",
            "rho0: 0.6004, rho1: 0.2000\n",
            "Epoch 17, Loss: 0.9661, rho_lr: 0.200000\n",
            "rho0: 0.6073, rho1: 0.2000\n",
            "Epoch 18, Loss: 0.9415, rho_lr: 0.200000\n",
            "rho0: 0.6099, rho1: 0.2000\n",
            "Epoch 19, Loss: 0.9192, rho_lr: 0.200000\n",
            "rho0: 0.6164, rho1: 0.2000\n",
            "Epoch 20, Loss: 0.9105, rho_lr: 0.020000\n",
            "rho0: 0.6199, rho1: 0.2000\n",
            "Epoch 21, Loss: 0.8927, rho_lr: 0.020000\n",
            "rho0: 0.6214, rho1: 0.2000\n",
            "Epoch 22, Loss: 0.8844, rho_lr: 0.020000\n",
            "rho0: 0.6236, rho1: 0.2000\n",
            "Epoch 23, Loss: 0.8793, rho_lr: 0.020000\n",
            "rho0: 0.6260, rho1: 0.2000\n",
            "Epoch 24, Loss: 0.8642, rho_lr: 0.020000\n",
            "rho0: 0.6288, rho1: 0.2000\n",
            "Epoch 25, Loss: 0.8499, rho_lr: 0.020000\n",
            "rho0: 0.6313, rho1: 0.2000\n",
            "Epoch 26, Loss: 0.8416, rho_lr: 0.020000\n",
            "rho0: 0.6343, rho1: 0.2000\n",
            "Epoch 27, Loss: 0.8400, rho_lr: 0.020000\n",
            "rho0: 0.6374, rho1: 0.2000\n",
            "Epoch 28, Loss: 0.8305, rho_lr: 0.020000\n",
            "rho0: 0.6402, rho1: 0.2000\n",
            "Epoch 29, Loss: 0.8148, rho_lr: 0.020000\n",
            "rho0: 0.6428, rho1: 0.2000\n",
            "Epoch 30, Loss: 0.8057, rho_lr: 0.020000\n",
            "rho0: 0.6457, rho1: 0.2000\n",
            "Epoch 31, Loss: 0.8052, rho_lr: 0.020000\n",
            "rho0: 0.6491, rho1: 0.2000\n",
            "Epoch 32, Loss: 0.7864, rho_lr: 0.020000\n",
            "rho0: 0.6520, rho1: 0.2000\n",
            "Epoch 33, Loss: 0.7798, rho_lr: 0.020000\n",
            "rho0: 0.6552, rho1: 0.2000\n",
            "Epoch 34, Loss: 0.7729, rho_lr: 0.020000\n",
            "rho0: 0.6581, rho1: 0.2000\n",
            "Epoch 35, Loss: 0.7683, rho_lr: 0.020000\n",
            "rho0: 0.6611, rho1: 0.2000\n",
            "Epoch 36, Loss: 0.7616, rho_lr: 0.020000\n",
            "rho0: 0.6639, rho1: 0.2000\n",
            "Epoch 37, Loss: 0.7592, rho_lr: 0.020000\n",
            "rho0: 0.6670, rho1: 0.2000\n",
            "Epoch 38, Loss: 0.7460, rho_lr: 0.020000\n",
            "rho0: 0.6698, rho1: 0.2000\n",
            "Epoch 39, Loss: 0.7381, rho_lr: 0.020000\n",
            "rho0: 0.6729, rho1: 0.2000\n",
            "Epoch 40, Loss: 0.7271, rho_lr: 0.002000\n",
            "rho0: 0.6761, rho1: 0.2000\n",
            "Epoch 41, Loss: 0.7236, rho_lr: 0.002000\n",
            "rho0: 0.6764, rho1: 0.2000\n",
            "Epoch 42, Loss: 0.7185, rho_lr: 0.002000\n",
            "rho0: 0.6768, rho1: 0.2000\n",
            "Epoch 43, Loss: 0.7140, rho_lr: 0.002000\n",
            "rho0: 0.6773, rho1: 0.2000\n",
            "Epoch 44, Loss: 0.7101, rho_lr: 0.002000\n",
            "rho0: 0.6778, rho1: 0.2000\n",
            "Epoch 45, Loss: 0.7029, rho_lr: 0.002000\n",
            "rho0: 0.6783, rho1: 0.2000\n",
            "Epoch 46, Loss: 0.6979, rho_lr: 0.002000\n",
            "rho0: 0.6788, rho1: 0.2000\n",
            "Epoch 47, Loss: 0.6897, rho_lr: 0.002000\n",
            "rho0: 0.6793, rho1: 0.2000\n",
            "Epoch 48, Loss: 0.6922, rho_lr: 0.002000\n",
            "rho0: 0.6798, rho1: 0.2000\n",
            "Epoch 49, Loss: 0.6954, rho_lr: 0.002000\n",
            "rho0: 0.6804, rho1: 0.2000\n",
            "Epoch 50, Loss: 0.6880, rho_lr: 0.002000\n",
            "rho0: 0.6809, rho1: 0.2000\n",
            "Epoch 51, Loss: 0.6824, rho_lr: 0.002000\n",
            "rho0: 0.6815, rho1: 0.2000\n",
            "Epoch 52, Loss: 0.6742, rho_lr: 0.002000\n",
            "rho0: 0.6821, rho1: 0.2000\n",
            "Epoch 53, Loss: 0.6765, rho_lr: 0.002000\n",
            "rho0: 0.6827, rho1: 0.2000\n",
            "Epoch 54, Loss: 0.6717, rho_lr: 0.002000\n",
            "rho0: 0.6833, rho1: 0.2000\n",
            "Epoch 55, Loss: 0.6642, rho_lr: 0.002000\n",
            "rho0: 0.6839, rho1: 0.2000\n",
            "Epoch 56, Loss: 0.6645, rho_lr: 0.002000\n",
            "rho0: 0.6846, rho1: 0.2000\n",
            "Epoch 57, Loss: 0.6580, rho_lr: 0.002000\n",
            "rho0: 0.6852, rho1: 0.2000\n",
            "Epoch 58, Loss: 0.6549, rho_lr: 0.002000\n",
            "rho0: 0.6858, rho1: 0.2000\n",
            "Epoch 59, Loss: 0.6529, rho_lr: 0.002000\n",
            "rho0: 0.6864, rho1: 0.2000\n",
            "Epoch 60, Loss: 0.6525, rho_lr: 0.000200\n",
            "rho0: 0.6870, rho1: 0.2000\n",
            "Epoch 1, Loss: 45.0106, rho_lr: 0.200000\n",
            "rho0: 0.0000, rho1: 0.5000\n",
            "Epoch 2, Loss: 15.3897, rho_lr: 0.200000\n",
            "rho0: 0.2134, rho1: 0.5000\n",
            "Epoch 3, Loss: 11.5158, rho_lr: 0.200000\n",
            "rho0: 0.1718, rho1: 0.5000\n",
            "Epoch 4, Loss: 7.6576, rho_lr: 0.200000\n",
            "rho0: 0.2705, rho1: 0.5000\n",
            "Epoch 5, Loss: 5.4650, rho_lr: 0.200000\n",
            "rho0: 0.2852, rho1: 0.5000\n",
            "Epoch 6, Loss: 4.0883, rho_lr: 0.200000\n",
            "rho0: 0.3018, rho1: 0.5000\n",
            "Epoch 7, Loss: 3.3176, rho_lr: 0.200000\n",
            "rho0: 0.3153, rho1: 0.5000\n",
            "Epoch 8, Loss: 2.8967, rho_lr: 0.200000\n",
            "rho0: 0.3105, rho1: 0.5000\n",
            "Epoch 9, Loss: 2.5530, rho_lr: 0.200000\n",
            "rho0: 0.3113, rho1: 0.5000\n",
            "Epoch 10, Loss: 2.3902, rho_lr: 0.200000\n",
            "rho0: 0.3125, rho1: 0.5000\n",
            "Epoch 11, Loss: 2.3589, rho_lr: 0.200000\n",
            "rho0: 0.3641, rho1: 0.4130\n",
            "Epoch 12, Loss: 1.8202, rho_lr: 0.200000\n",
            "rho0: 0.4646, rho1: 0.2579\n",
            "Epoch 13, Loss: 1.3013, rho_lr: 0.200000\n",
            "rho0: 0.5615, rho1: 0.2000\n",
            "Epoch 14, Loss: 1.2503, rho_lr: 0.200000\n",
            "rho0: 0.5446, rho1: 0.2000\n",
            "Epoch 15, Loss: 1.1750, rho_lr: 0.200000\n",
            "rho0: 0.5601, rho1: 0.2000\n",
            "Epoch 16, Loss: 1.1350, rho_lr: 0.200000\n",
            "rho0: 0.5679, rho1: 0.2000\n",
            "Epoch 17, Loss: 1.0918, rho_lr: 0.200000\n",
            "rho0: 0.5767, rho1: 0.2000\n",
            "Epoch 18, Loss: 1.0815, rho_lr: 0.200000\n",
            "rho0: 0.5807, rho1: 0.2000\n",
            "Epoch 19, Loss: 1.0568, rho_lr: 0.200000\n",
            "rho0: 0.5868, rho1: 0.2000\n",
            "Epoch 20, Loss: 1.0418, rho_lr: 0.020000\n",
            "rho0: 0.5933, rho1: 0.2000\n",
            "Epoch 21, Loss: 1.0294, rho_lr: 0.020000\n",
            "rho0: 0.5942, rho1: 0.2000\n",
            "Epoch 22, Loss: 1.0027, rho_lr: 0.020000\n",
            "rho0: 0.5961, rho1: 0.2000\n",
            "Epoch 23, Loss: 1.0015, rho_lr: 0.020000\n",
            "rho0: 0.5980, rho1: 0.2000\n",
            "Epoch 24, Loss: 0.9856, rho_lr: 0.020000\n",
            "rho0: 0.6004, rho1: 0.2000\n",
            "Epoch 25, Loss: 0.9773, rho_lr: 0.020000\n",
            "rho0: 0.6026, rho1: 0.2000\n",
            "Epoch 26, Loss: 0.9576, rho_lr: 0.020000\n",
            "rho0: 0.6051, rho1: 0.2000\n",
            "Epoch 27, Loss: 0.9601, rho_lr: 0.020000\n",
            "rho0: 0.6076, rho1: 0.2000\n",
            "Epoch 28, Loss: 0.9430, rho_lr: 0.020000\n",
            "rho0: 0.6103, rho1: 0.2000\n",
            "Epoch 29, Loss: 0.9324, rho_lr: 0.020000\n",
            "rho0: 0.6132, rho1: 0.2000\n",
            "Epoch 30, Loss: 0.9228, rho_lr: 0.020000\n",
            "rho0: 0.6160, rho1: 0.2000\n",
            "Epoch 31, Loss: 0.9063, rho_lr: 0.020000\n",
            "rho0: 0.6188, rho1: 0.2000\n",
            "Epoch 32, Loss: 0.8959, rho_lr: 0.020000\n",
            "rho0: 0.6210, rho1: 0.2000\n",
            "Epoch 33, Loss: 0.9011, rho_lr: 0.020000\n",
            "rho0: 0.6235, rho1: 0.2000\n",
            "Epoch 34, Loss: 0.8888, rho_lr: 0.020000\n",
            "rho0: 0.6263, rho1: 0.2000\n",
            "Epoch 35, Loss: 0.8706, rho_lr: 0.020000\n",
            "rho0: 0.6287, rho1: 0.2000\n",
            "Epoch 36, Loss: 0.8587, rho_lr: 0.020000\n",
            "rho0: 0.6314, rho1: 0.2000\n",
            "Epoch 37, Loss: 0.8492, rho_lr: 0.020000\n",
            "rho0: 0.6340, rho1: 0.2000\n",
            "Epoch 38, Loss: 0.8434, rho_lr: 0.020000\n",
            "rho0: 0.6365, rho1: 0.2000\n",
            "Epoch 39, Loss: 0.8339, rho_lr: 0.020000\n",
            "rho0: 0.6388, rho1: 0.2000\n",
            "Epoch 40, Loss: 0.8334, rho_lr: 0.002000\n",
            "rho0: 0.6413, rho1: 0.2000\n",
            "Epoch 41, Loss: 0.8325, rho_lr: 0.002000\n",
            "rho0: 0.6416, rho1: 0.2000\n",
            "Epoch 42, Loss: 0.8219, rho_lr: 0.002000\n",
            "rho0: 0.6420, rho1: 0.2000\n",
            "Epoch 43, Loss: 0.8143, rho_lr: 0.002000\n",
            "rho0: 0.6423, rho1: 0.2000\n",
            "Epoch 44, Loss: 0.8158, rho_lr: 0.002000\n",
            "rho0: 0.6428, rho1: 0.2000\n",
            "Epoch 45, Loss: 0.7935, rho_lr: 0.002000\n",
            "rho0: 0.6432, rho1: 0.2000\n",
            "Epoch 46, Loss: 0.7977, rho_lr: 0.002000\n",
            "rho0: 0.6437, rho1: 0.2000\n",
            "Epoch 47, Loss: 0.7914, rho_lr: 0.002000\n",
            "rho0: 0.6442, rho1: 0.2000\n",
            "Epoch 48, Loss: 0.7853, rho_lr: 0.002000\n",
            "rho0: 0.6446, rho1: 0.2000\n",
            "Epoch 49, Loss: 0.7872, rho_lr: 0.002000\n",
            "rho0: 0.6451, rho1: 0.2000\n",
            "Epoch 50, Loss: 0.7753, rho_lr: 0.002000\n",
            "rho0: 0.6456, rho1: 0.2000\n",
            "Epoch 51, Loss: 0.7708, rho_lr: 0.002000\n",
            "rho0: 0.6461, rho1: 0.2000\n",
            "Epoch 52, Loss: 0.7646, rho_lr: 0.002000\n",
            "rho0: 0.6466, rho1: 0.2000\n",
            "Epoch 53, Loss: 0.7611, rho_lr: 0.002000\n",
            "rho0: 0.6472, rho1: 0.2000\n",
            "Epoch 54, Loss: 0.7496, rho_lr: 0.002000\n",
            "rho0: 0.6478, rho1: 0.2000\n",
            "Epoch 55, Loss: 0.7539, rho_lr: 0.002000\n",
            "rho0: 0.6483, rho1: 0.2000\n",
            "Epoch 56, Loss: 0.7506, rho_lr: 0.002000\n",
            "rho0: 0.6488, rho1: 0.2000\n",
            "Epoch 57, Loss: 0.7484, rho_lr: 0.002000\n",
            "rho0: 0.6494, rho1: 0.2000\n",
            "Epoch 58, Loss: 0.7370, rho_lr: 0.002000\n",
            "rho0: 0.6500, rho1: 0.2000\n",
            "Epoch 59, Loss: 0.7402, rho_lr: 0.002000\n",
            "rho0: 0.6506, rho1: 0.2000\n",
            "Epoch 60, Loss: 0.7348, rho_lr: 0.000200\n",
            "rho0: 0.6511, rho1: 0.2000\n",
            "Epoch 1, Loss: 42.9868, rho_lr: 0.200000\n",
            "rho0: 0.0000, rho1: 0.5000\n",
            "Epoch 2, Loss: 14.7930, rho_lr: 0.200000\n",
            "rho0: 0.2218, rho1: 0.5000\n",
            "Epoch 3, Loss: 8.0731, rho_lr: 0.200000\n",
            "rho0: 0.2519, rho1: 0.5000\n",
            "Epoch 4, Loss: 6.1088, rho_lr: 0.200000\n",
            "rho0: 0.2716, rho1: 0.5000\n",
            "Epoch 5, Loss: 4.3441, rho_lr: 0.200000\n",
            "rho0: 0.2627, rho1: 0.5000\n",
            "Epoch 6, Loss: 3.7815, rho_lr: 0.200000\n",
            "rho0: 0.2706, rho1: 0.5000\n",
            "Epoch 7, Loss: 3.0809, rho_lr: 0.200000\n",
            "rho0: 0.2818, rho1: 0.5000\n",
            "Epoch 8, Loss: 2.8043, rho_lr: 0.200000\n",
            "rho0: 0.2865, rho1: 0.5000\n",
            "Epoch 9, Loss: 2.5811, rho_lr: 0.200000\n",
            "rho0: 0.2933, rho1: 0.5000\n",
            "Epoch 10, Loss: 2.3939, rho_lr: 0.200000\n",
            "rho0: 0.2986, rho1: 0.5000\n",
            "Epoch 11, Loss: 2.3060, rho_lr: 0.200000\n",
            "rho0: 0.3678, rho1: 0.3819\n",
            "Epoch 12, Loss: 1.6858, rho_lr: 0.200000\n",
            "rho0: 0.4776, rho1: 0.2000\n",
            "Epoch 13, Loss: 1.3132, rho_lr: 0.200000\n",
            "rho0: 0.5400, rho1: 0.2000\n",
            "Epoch 14, Loss: 1.2430, rho_lr: 0.200000\n",
            "rho0: 0.5280, rho1: 0.2000\n",
            "Epoch 15, Loss: 1.1963, rho_lr: 0.200000\n",
            "rho0: 0.5507, rho1: 0.2000\n",
            "Epoch 16, Loss: 1.1803, rho_lr: 0.200000\n",
            "rho0: 0.5534, rho1: 0.2000\n",
            "Epoch 17, Loss: 1.1449, rho_lr: 0.200000\n",
            "rho0: 0.5672, rho1: 0.2000\n",
            "Epoch 18, Loss: 1.1333, rho_lr: 0.200000\n",
            "rho0: 0.5692, rho1: 0.2000\n",
            "Epoch 19, Loss: 1.0911, rho_lr: 0.200000\n",
            "rho0: 0.5803, rho1: 0.2000\n",
            "Epoch 20, Loss: 1.0754, rho_lr: 0.020000\n",
            "rho0: 0.5844, rho1: 0.2000\n",
            "Epoch 21, Loss: 1.0602, rho_lr: 0.020000\n",
            "rho0: 0.5856, rho1: 0.2000\n",
            "Epoch 22, Loss: 1.0439, rho_lr: 0.020000\n",
            "rho0: 0.5878, rho1: 0.2000\n",
            "Epoch 23, Loss: 1.0286, rho_lr: 0.020000\n",
            "rho0: 0.5902, rho1: 0.2000\n",
            "Epoch 24, Loss: 1.0015, rho_lr: 0.020000\n",
            "rho0: 0.5931, rho1: 0.2000\n",
            "Epoch 25, Loss: 0.9907, rho_lr: 0.020000\n",
            "rho0: 0.5968, rho1: 0.2000\n",
            "Epoch 26, Loss: 0.9867, rho_lr: 0.020000\n",
            "rho0: 0.6001, rho1: 0.2000\n",
            "Epoch 27, Loss: 0.9749, rho_lr: 0.020000\n",
            "rho0: 0.6029, rho1: 0.2000\n",
            "Epoch 28, Loss: 0.9613, rho_lr: 0.020000\n",
            "rho0: 0.6055, rho1: 0.2000\n",
            "Epoch 29, Loss: 0.9537, rho_lr: 0.020000\n",
            "rho0: 0.6081, rho1: 0.2000\n",
            "Epoch 30, Loss: 0.9269, rho_lr: 0.020000\n",
            "rho0: 0.6109, rho1: 0.2000\n",
            "Epoch 31, Loss: 0.9224, rho_lr: 0.020000\n",
            "rho0: 0.6136, rho1: 0.2000\n",
            "Epoch 32, Loss: 0.9092, rho_lr: 0.020000\n",
            "rho0: 0.6167, rho1: 0.2000\n",
            "Epoch 33, Loss: 0.8963, rho_lr: 0.020000\n",
            "rho0: 0.6196, rho1: 0.2000\n",
            "Epoch 34, Loss: 0.8861, rho_lr: 0.020000\n",
            "rho0: 0.6227, rho1: 0.2000\n",
            "Epoch 35, Loss: 0.8765, rho_lr: 0.020000\n",
            "rho0: 0.6260, rho1: 0.2000\n",
            "Epoch 36, Loss: 0.8645, rho_lr: 0.020000\n",
            "rho0: 0.6289, rho1: 0.2000\n",
            "Epoch 37, Loss: 0.8579, rho_lr: 0.020000\n",
            "rho0: 0.6320, rho1: 0.2000\n",
            "Epoch 38, Loss: 0.8460, rho_lr: 0.020000\n",
            "rho0: 0.6351, rho1: 0.2000\n",
            "Epoch 39, Loss: 0.8314, rho_lr: 0.020000\n",
            "rho0: 0.6386, rho1: 0.2000\n",
            "Epoch 40, Loss: 0.8325, rho_lr: 0.002000\n",
            "rho0: 0.6420, rho1: 0.2000\n",
            "Epoch 41, Loss: 0.8201, rho_lr: 0.002000\n",
            "rho0: 0.6423, rho1: 0.2000\n",
            "Epoch 42, Loss: 0.8089, rho_lr: 0.002000\n",
            "rho0: 0.6427, rho1: 0.2000\n",
            "Epoch 43, Loss: 0.8037, rho_lr: 0.002000\n",
            "rho0: 0.6431, rho1: 0.2000\n",
            "Epoch 44, Loss: 0.7975, rho_lr: 0.002000\n",
            "rho0: 0.6436, rho1: 0.2000\n",
            "Epoch 45, Loss: 0.7852, rho_lr: 0.002000\n",
            "rho0: 0.6441, rho1: 0.2000\n",
            "Epoch 46, Loss: 0.7876, rho_lr: 0.002000\n",
            "rho0: 0.6447, rho1: 0.2000\n",
            "Epoch 47, Loss: 0.7732, rho_lr: 0.002000\n",
            "rho0: 0.6452, rho1: 0.2000\n",
            "Epoch 48, Loss: 0.7736, rho_lr: 0.002000\n",
            "rho0: 0.6458, rho1: 0.2000\n",
            "Epoch 49, Loss: 0.7671, rho_lr: 0.002000\n",
            "rho0: 0.6464, rho1: 0.2000\n",
            "Epoch 50, Loss: 0.7629, rho_lr: 0.002000\n",
            "rho0: 0.6470, rho1: 0.2000\n",
            "Epoch 51, Loss: 0.7521, rho_lr: 0.002000\n",
            "rho0: 0.6476, rho1: 0.2000\n",
            "Epoch 52, Loss: 0.7486, rho_lr: 0.002000\n",
            "rho0: 0.6481, rho1: 0.2000\n",
            "Epoch 53, Loss: 0.7409, rho_lr: 0.002000\n",
            "rho0: 0.6487, rho1: 0.2000\n",
            "Epoch 54, Loss: 0.7409, rho_lr: 0.002000\n",
            "rho0: 0.6494, rho1: 0.2000\n",
            "Epoch 55, Loss: 0.7375, rho_lr: 0.002000\n",
            "rho0: 0.6500, rho1: 0.2000\n",
            "Epoch 56, Loss: 0.7278, rho_lr: 0.002000\n",
            "rho0: 0.6507, rho1: 0.2000\n",
            "Epoch 57, Loss: 0.7294, rho_lr: 0.002000\n",
            "rho0: 0.6513, rho1: 0.2000\n",
            "Epoch 58, Loss: 0.7256, rho_lr: 0.002000\n",
            "rho0: 0.6520, rho1: 0.2000\n",
            "Epoch 59, Loss: 0.7166, rho_lr: 0.002000\n",
            "rho0: 0.6527, rho1: 0.2000\n",
            "Epoch 60, Loss: 0.7080, rho_lr: 0.000200\n",
            "rho0: 0.6533, rho1: 0.2000\n",
            "Epoch 1, Loss: 37.6052, rho_lr: 0.200000\n",
            "rho0: 0.0000, rho1: 0.5000\n",
            "Epoch 2, Loss: 16.5279, rho_lr: 0.200000\n",
            "rho0: 0.1997, rho1: 0.5000\n",
            "Epoch 3, Loss: 9.8220, rho_lr: 0.200000\n",
            "rho0: 0.3245, rho1: 0.5000\n",
            "Epoch 4, Loss: 5.3659, rho_lr: 0.200000\n",
            "rho0: 0.3183, rho1: 0.5000\n",
            "Epoch 5, Loss: 3.9461, rho_lr: 0.200000\n",
            "rho0: 0.3170, rho1: 0.5000\n",
            "Epoch 6, Loss: 2.8781, rho_lr: 0.200000\n",
            "rho0: 0.3315, rho1: 0.5000\n",
            "Epoch 7, Loss: 2.4554, rho_lr: 0.200000\n",
            "rho0: 0.3252, rho1: 0.5000\n",
            "Epoch 8, Loss: 2.2670, rho_lr: 0.200000\n",
            "rho0: 0.3326, rho1: 0.5000\n",
            "Epoch 9, Loss: 2.1572, rho_lr: 0.200000\n",
            "rho0: 0.3339, rho1: 0.5000\n",
            "Epoch 10, Loss: 2.0835, rho_lr: 0.200000\n",
            "rho0: 0.3371, rho1: 0.5000\n",
            "Epoch 11, Loss: 2.0660, rho_lr: 0.200000\n",
            "rho0: 0.3979, rho1: 0.3686\n",
            "Epoch 12, Loss: 1.4590, rho_lr: 0.200000\n",
            "rho0: 0.5234, rho1: 0.2265\n",
            "Epoch 13, Loss: 1.1127, rho_lr: 0.200000\n",
            "rho0: 0.5861, rho1: 0.2000\n",
            "Epoch 14, Loss: 1.0519, rho_lr: 0.200000\n",
            "rho0: 0.5755, rho1: 0.2000\n",
            "Epoch 15, Loss: 1.0337, rho_lr: 0.200000\n",
            "rho0: 0.5815, rho1: 0.2000\n",
            "Epoch 16, Loss: 0.9935, rho_lr: 0.200000\n",
            "rho0: 0.5994, rho1: 0.2000\n",
            "Epoch 17, Loss: 1.0019, rho_lr: 0.200000\n",
            "rho0: 0.5925, rho1: 0.2000\n",
            "Epoch 18, Loss: 0.9736, rho_lr: 0.200000\n",
            "rho0: 0.6066, rho1: 0.2000\n",
            "Epoch 19, Loss: 0.9476, rho_lr: 0.200000\n",
            "rho0: 0.6130, rho1: 0.2000\n",
            "Epoch 20, Loss: 0.9423, rho_lr: 0.020000\n",
            "rho0: 0.6156, rho1: 0.2000\n",
            "Epoch 21, Loss: 0.9221, rho_lr: 0.020000\n",
            "rho0: 0.6164, rho1: 0.2000\n",
            "Epoch 22, Loss: 0.9232, rho_lr: 0.020000\n",
            "rho0: 0.6191, rho1: 0.2000\n",
            "Epoch 23, Loss: 0.9072, rho_lr: 0.020000\n",
            "rho0: 0.6221, rho1: 0.2000\n",
            "Epoch 24, Loss: 0.8976, rho_lr: 0.020000\n",
            "rho0: 0.6258, rho1: 0.2000\n",
            "Epoch 25, Loss: 0.8742, rho_lr: 0.020000\n",
            "rho0: 0.6292, rho1: 0.2000\n",
            "Epoch 26, Loss: 0.8662, rho_lr: 0.020000\n",
            "rho0: 0.6323, rho1: 0.2000\n",
            "Epoch 27, Loss: 0.8555, rho_lr: 0.020000\n",
            "rho0: 0.6355, rho1: 0.2000\n",
            "Epoch 28, Loss: 0.8505, rho_lr: 0.020000\n",
            "rho0: 0.6387, rho1: 0.2000\n",
            "Epoch 29, Loss: 0.8448, rho_lr: 0.020000\n",
            "rho0: 0.6414, rho1: 0.2000\n",
            "Epoch 30, Loss: 0.8302, rho_lr: 0.020000\n",
            "rho0: 0.6449, rho1: 0.2000\n",
            "Epoch 31, Loss: 0.8274, rho_lr: 0.020000\n",
            "rho0: 0.6478, rho1: 0.2000\n",
            "Epoch 32, Loss: 0.8122, rho_lr: 0.020000\n",
            "rho0: 0.6510, rho1: 0.2000\n",
            "Epoch 33, Loss: 0.8130, rho_lr: 0.020000\n",
            "rho0: 0.6542, rho1: 0.2000\n",
            "Epoch 34, Loss: 0.8023, rho_lr: 0.020000\n",
            "rho0: 0.6569, rho1: 0.2000\n",
            "Epoch 35, Loss: 0.7938, rho_lr: 0.020000\n",
            "rho0: 0.6605, rho1: 0.2000\n",
            "Epoch 36, Loss: 0.7774, rho_lr: 0.020000\n",
            "rho0: 0.6642, rho1: 0.2000\n",
            "Epoch 37, Loss: 0.7772, rho_lr: 0.020000\n",
            "rho0: 0.6672, rho1: 0.2000\n",
            "Epoch 38, Loss: 0.7651, rho_lr: 0.020000\n",
            "rho0: 0.6704, rho1: 0.2000\n",
            "Epoch 39, Loss: 0.7608, rho_lr: 0.020000\n",
            "rho0: 0.6738, rho1: 0.2000\n",
            "Epoch 40, Loss: 0.7525, rho_lr: 0.002000\n",
            "rho0: 0.6772, rho1: 0.2000\n",
            "Epoch 41, Loss: 0.7508, rho_lr: 0.002000\n",
            "rho0: 0.6775, rho1: 0.2000\n",
            "Epoch 42, Loss: 0.7379, rho_lr: 0.002000\n",
            "rho0: 0.6779, rho1: 0.2000\n",
            "Epoch 43, Loss: 0.7361, rho_lr: 0.002000\n",
            "rho0: 0.6784, rho1: 0.2000\n",
            "Epoch 44, Loss: 0.7343, rho_lr: 0.002000\n",
            "rho0: 0.6789, rho1: 0.2000\n",
            "Epoch 45, Loss: 0.7251, rho_lr: 0.002000\n",
            "rho0: 0.6794, rho1: 0.2000\n",
            "Epoch 46, Loss: 0.7207, rho_lr: 0.002000\n",
            "rho0: 0.6799, rho1: 0.2000\n",
            "Epoch 47, Loss: 0.7190, rho_lr: 0.002000\n",
            "rho0: 0.6805, rho1: 0.2000\n",
            "Epoch 48, Loss: 0.7102, rho_lr: 0.002000\n",
            "rho0: 0.6811, rho1: 0.2000\n",
            "Epoch 49, Loss: 0.7079, rho_lr: 0.002000\n",
            "rho0: 0.6817, rho1: 0.2000\n",
            "Epoch 50, Loss: 0.6969, rho_lr: 0.002000\n",
            "rho0: 0.6823, rho1: 0.2000\n",
            "Epoch 51, Loss: 0.6975, rho_lr: 0.002000\n",
            "rho0: 0.6829, rho1: 0.2000\n",
            "Epoch 52, Loss: 0.6938, rho_lr: 0.002000\n",
            "rho0: 0.6835, rho1: 0.2000\n",
            "Epoch 53, Loss: 0.6963, rho_lr: 0.002000\n",
            "rho0: 0.6841, rho1: 0.2000\n",
            "Epoch 54, Loss: 0.6877, rho_lr: 0.002000\n",
            "rho0: 0.6848, rho1: 0.2000\n",
            "Epoch 55, Loss: 0.6823, rho_lr: 0.002000\n",
            "rho0: 0.6855, rho1: 0.2000\n",
            "Epoch 56, Loss: 0.6794, rho_lr: 0.002000\n",
            "rho0: 0.6861, rho1: 0.2000\n",
            "Epoch 57, Loss: 0.6766, rho_lr: 0.002000\n",
            "rho0: 0.6867, rho1: 0.2000\n",
            "Epoch 58, Loss: 0.6727, rho_lr: 0.002000\n",
            "rho0: 0.6873, rho1: 0.2000\n",
            "Epoch 59, Loss: 0.6676, rho_lr: 0.002000\n",
            "rho0: 0.6880, rho1: 0.2000\n",
            "Epoch 60, Loss: 0.6704, rho_lr: 0.000200\n",
            "rho0: 0.6886, rho1: 0.2000\n",
            "Epoch 1, Loss: 40.0272, rho_lr: 0.200000\n",
            "rho0: 0.0000, rho1: 0.5000\n",
            "Epoch 2, Loss: 12.6643, rho_lr: 0.200000\n",
            "rho0: 0.2573, rho1: 0.5000\n",
            "Epoch 3, Loss: 7.5278, rho_lr: 0.200000\n",
            "rho0: 0.2602, rho1: 0.5000\n",
            "Epoch 4, Loss: 5.3992, rho_lr: 0.200000\n",
            "rho0: 0.2900, rho1: 0.5000\n",
            "Epoch 5, Loss: 3.9016, rho_lr: 0.200000\n",
            "rho0: 0.3195, rho1: 0.5000\n",
            "Epoch 6, Loss: 3.1431, rho_lr: 0.200000\n",
            "rho0: 0.3138, rho1: 0.5000\n",
            "Epoch 7, Loss: 2.7299, rho_lr: 0.200000\n",
            "rho0: 0.3144, rho1: 0.5000\n",
            "Epoch 8, Loss: 2.3532, rho_lr: 0.200000\n",
            "rho0: 0.3247, rho1: 0.5000\n",
            "Epoch 9, Loss: 2.1974, rho_lr: 0.200000\n",
            "rho0: 0.3269, rho1: 0.5000\n",
            "Epoch 10, Loss: 2.0956, rho_lr: 0.200000\n",
            "rho0: 0.3305, rho1: 0.5000\n",
            "Epoch 11, Loss: 2.1397, rho_lr: 0.200000\n",
            "rho0: 0.3851, rho1: 0.3754\n",
            "Epoch 12, Loss: 1.5320, rho_lr: 0.200000\n",
            "rho0: 0.5014, rho1: 0.2631\n",
            "Epoch 13, Loss: 1.1326, rho_lr: 0.200000\n",
            "rho0: 0.5869, rho1: 0.2000\n",
            "Epoch 14, Loss: 1.0514, rho_lr: 0.200000\n",
            "rho0: 0.5778, rho1: 0.2000\n",
            "Epoch 15, Loss: 1.0148, rho_lr: 0.200000\n",
            "rho0: 0.5895, rho1: 0.2000\n",
            "Epoch 16, Loss: 0.9991, rho_lr: 0.200000\n",
            "rho0: 0.6016, rho1: 0.2000\n",
            "Epoch 17, Loss: 0.9628, rho_lr: 0.200000\n",
            "rho0: 0.6064, rho1: 0.2000\n",
            "Epoch 18, Loss: 0.9578, rho_lr: 0.200000\n",
            "rho0: 0.6143, rho1: 0.2000\n",
            "Epoch 19, Loss: 0.9288, rho_lr: 0.200000\n",
            "rho0: 0.6210, rho1: 0.2000\n",
            "Epoch 20, Loss: 0.9142, rho_lr: 0.020000\n",
            "rho0: 0.6274, rho1: 0.2000\n",
            "Epoch 21, Loss: 0.9018, rho_lr: 0.020000\n",
            "rho0: 0.6284, rho1: 0.2000\n",
            "Epoch 22, Loss: 0.8752, rho_lr: 0.020000\n",
            "rho0: 0.6305, rho1: 0.2000\n",
            "Epoch 23, Loss: 0.8767, rho_lr: 0.020000\n",
            "rho0: 0.6331, rho1: 0.2000\n",
            "Epoch 24, Loss: 0.8682, rho_lr: 0.020000\n",
            "rho0: 0.6361, rho1: 0.2000\n",
            "Epoch 25, Loss: 0.8573, rho_lr: 0.020000\n",
            "rho0: 0.6390, rho1: 0.2000\n",
            "Epoch 26, Loss: 0.8454, rho_lr: 0.020000\n",
            "rho0: 0.6422, rho1: 0.2000\n",
            "Epoch 27, Loss: 0.8381, rho_lr: 0.020000\n",
            "rho0: 0.6457, rho1: 0.2000\n",
            "Epoch 28, Loss: 0.8133, rho_lr: 0.020000\n",
            "rho0: 0.6490, rho1: 0.2000\n",
            "Epoch 29, Loss: 0.8063, rho_lr: 0.020000\n",
            "rho0: 0.6527, rho1: 0.2000\n",
            "Epoch 30, Loss: 0.7960, rho_lr: 0.020000\n",
            "rho0: 0.6564, rho1: 0.2000\n",
            "Epoch 31, Loss: 0.7866, rho_lr: 0.020000\n",
            "rho0: 0.6597, rho1: 0.2000\n",
            "Epoch 32, Loss: 0.7803, rho_lr: 0.020000\n",
            "rho0: 0.6628, rho1: 0.2000\n",
            "Epoch 33, Loss: 0.7757, rho_lr: 0.020000\n",
            "rho0: 0.6654, rho1: 0.2000\n",
            "Epoch 34, Loss: 0.7613, rho_lr: 0.020000\n",
            "rho0: 0.6687, rho1: 0.2000\n",
            "Epoch 35, Loss: 0.7494, rho_lr: 0.020000\n",
            "rho0: 0.6721, rho1: 0.2000\n",
            "Epoch 36, Loss: 0.7479, rho_lr: 0.020000\n",
            "rho0: 0.6752, rho1: 0.2000\n",
            "Epoch 37, Loss: 0.7390, rho_lr: 0.020000\n",
            "rho0: 0.6783, rho1: 0.2000\n",
            "Epoch 38, Loss: 0.7374, rho_lr: 0.020000\n",
            "rho0: 0.6812, rho1: 0.2000\n",
            "Epoch 39, Loss: 0.7269, rho_lr: 0.020000\n",
            "rho0: 0.6843, rho1: 0.2000\n",
            "Epoch 40, Loss: 0.7192, rho_lr: 0.002000\n",
            "rho0: 0.6873, rho1: 0.2000\n",
            "Epoch 41, Loss: 0.7135, rho_lr: 0.002000\n",
            "rho0: 0.6876, rho1: 0.2000\n",
            "Epoch 42, Loss: 0.7104, rho_lr: 0.002000\n",
            "rho0: 0.6881, rho1: 0.2000\n",
            "Epoch 43, Loss: 0.7037, rho_lr: 0.002000\n",
            "rho0: 0.6886, rho1: 0.2000\n",
            "Epoch 44, Loss: 0.6935, rho_lr: 0.002000\n",
            "rho0: 0.6891, rho1: 0.2000\n",
            "Epoch 45, Loss: 0.6972, rho_lr: 0.002000\n",
            "rho0: 0.6896, rho1: 0.2000\n",
            "Epoch 46, Loss: 0.6944, rho_lr: 0.002000\n",
            "rho0: 0.6902, rho1: 0.2000\n",
            "Epoch 47, Loss: 0.6853, rho_lr: 0.002000\n",
            "rho0: 0.6907, rho1: 0.2000\n",
            "Epoch 48, Loss: 0.6769, rho_lr: 0.002000\n",
            "rho0: 0.6913, rho1: 0.2000\n",
            "Epoch 49, Loss: 0.6810, rho_lr: 0.002000\n",
            "rho0: 0.6919, rho1: 0.2000\n",
            "Epoch 50, Loss: 0.6771, rho_lr: 0.002000\n",
            "rho0: 0.6926, rho1: 0.2000\n",
            "Epoch 51, Loss: 0.6698, rho_lr: 0.002000\n",
            "rho0: 0.6932, rho1: 0.2000\n",
            "Epoch 52, Loss: 0.6671, rho_lr: 0.002000\n",
            "rho0: 0.6938, rho1: 0.2000\n",
            "Epoch 53, Loss: 0.6680, rho_lr: 0.002000\n",
            "rho0: 0.6944, rho1: 0.2000\n",
            "Epoch 54, Loss: 0.6611, rho_lr: 0.002000\n",
            "rho0: 0.6951, rho1: 0.2000\n",
            "Epoch 55, Loss: 0.6530, rho_lr: 0.002000\n",
            "rho0: 0.6958, rho1: 0.2000\n",
            "Epoch 56, Loss: 0.6533, rho_lr: 0.002000\n",
            "rho0: 0.6965, rho1: 0.2000\n",
            "Epoch 57, Loss: 0.6501, rho_lr: 0.002000\n",
            "rho0: 0.6971, rho1: 0.2000\n",
            "Epoch 58, Loss: 0.6475, rho_lr: 0.002000\n",
            "rho0: 0.6978, rho1: 0.2000\n",
            "Epoch 59, Loss: 0.6441, rho_lr: 0.002000\n",
            "rho0: 0.6985, rho1: 0.2000\n",
            "Epoch 60, Loss: 0.6431, rho_lr: 0.000200\n",
            "rho0: 0.6992, rho1: 0.2000\n",
            "Epoch 1, Loss: 53.7795, rho_lr: 0.200000\n",
            "rho0: 0.0000, rho1: 0.5000\n",
            "Epoch 2, Loss: 16.1016, rho_lr: 0.200000\n",
            "rho0: 0.1573, rho1: 0.5000\n",
            "Epoch 3, Loss: 10.5335, rho_lr: 0.200000\n",
            "rho0: 0.2605, rho1: 0.5000\n",
            "Epoch 4, Loss: 7.1044, rho_lr: 0.200000\n",
            "rho0: 0.2842, rho1: 0.5000\n",
            "Epoch 5, Loss: 5.0010, rho_lr: 0.200000\n",
            "rho0: 0.2862, rho1: 0.5000\n",
            "Epoch 6, Loss: 3.8786, rho_lr: 0.200000\n",
            "rho0: 0.3238, rho1: 0.5000\n",
            "Epoch 7, Loss: 3.2389, rho_lr: 0.200000\n",
            "rho0: 0.3278, rho1: 0.5000\n",
            "Epoch 8, Loss: 2.8310, rho_lr: 0.200000\n",
            "rho0: 0.3333, rho1: 0.5000\n",
            "Epoch 9, Loss: 2.5830, rho_lr: 0.200000\n",
            "rho0: 0.3378, rho1: 0.5000\n",
            "Epoch 10, Loss: 2.4405, rho_lr: 0.200000\n",
            "rho0: 0.3437, rho1: 0.5000\n",
            "Epoch 11, Loss: 2.2888, rho_lr: 0.200000\n",
            "rho0: 0.4014, rho1: 0.3590\n",
            "Epoch 12, Loss: 1.6665, rho_lr: 0.200000\n",
            "rho0: 0.5083, rho1: 0.2255\n",
            "Epoch 13, Loss: 1.2462, rho_lr: 0.200000\n",
            "rho0: 0.5820, rho1: 0.2000\n",
            "Epoch 14, Loss: 1.1816, rho_lr: 0.200000\n",
            "rho0: 0.5736, rho1: 0.2000\n",
            "Epoch 15, Loss: 1.1487, rho_lr: 0.200000\n",
            "rho0: 0.5850, rho1: 0.2000\n",
            "Epoch 16, Loss: 1.1222, rho_lr: 0.200000\n",
            "rho0: 0.5953, rho1: 0.2000\n",
            "Epoch 17, Loss: 1.0878, rho_lr: 0.200000\n",
            "rho0: 0.6016, rho1: 0.2000\n",
            "Epoch 18, Loss: 1.0685, rho_lr: 0.200000\n",
            "rho0: 0.6036, rho1: 0.2000\n",
            "Epoch 19, Loss: 1.0528, rho_lr: 0.200000\n",
            "rho0: 0.6111, rho1: 0.2000\n",
            "Epoch 20, Loss: 1.0360, rho_lr: 0.020000\n",
            "rho0: 0.6149, rho1: 0.2000\n",
            "Epoch 21, Loss: 1.0055, rho_lr: 0.020000\n",
            "rho0: 0.6161, rho1: 0.2000\n",
            "Epoch 22, Loss: 1.0056, rho_lr: 0.020000\n",
            "rho0: 0.6175, rho1: 0.2000\n",
            "Epoch 23, Loss: 0.9970, rho_lr: 0.020000\n",
            "rho0: 0.6193, rho1: 0.2000\n",
            "Epoch 24, Loss: 0.9951, rho_lr: 0.020000\n",
            "rho0: 0.6216, rho1: 0.2000\n",
            "Epoch 25, Loss: 0.9713, rho_lr: 0.020000\n",
            "rho0: 0.6236, rho1: 0.2000\n",
            "Epoch 26, Loss: 0.9497, rho_lr: 0.020000\n",
            "rho0: 0.6264, rho1: 0.2000\n",
            "Epoch 27, Loss: 0.9482, rho_lr: 0.020000\n",
            "rho0: 0.6289, rho1: 0.2000\n",
            "Epoch 28, Loss: 0.9255, rho_lr: 0.020000\n",
            "rho0: 0.6318, rho1: 0.2000\n",
            "Epoch 29, Loss: 0.9207, rho_lr: 0.020000\n",
            "rho0: 0.6343, rho1: 0.2000\n",
            "Epoch 30, Loss: 0.9071, rho_lr: 0.020000\n",
            "rho0: 0.6370, rho1: 0.2000\n",
            "Epoch 31, Loss: 0.9017, rho_lr: 0.020000\n",
            "rho0: 0.6399, rho1: 0.2000\n",
            "Epoch 32, Loss: 0.8851, rho_lr: 0.020000\n",
            "rho0: 0.6423, rho1: 0.2000\n",
            "Epoch 33, Loss: 0.8838, rho_lr: 0.020000\n",
            "rho0: 0.6447, rho1: 0.2000\n",
            "Epoch 34, Loss: 0.8715, rho_lr: 0.020000\n",
            "rho0: 0.6468, rho1: 0.2000\n",
            "Epoch 35, Loss: 0.8641, rho_lr: 0.020000\n",
            "rho0: 0.6495, rho1: 0.2000\n",
            "Epoch 36, Loss: 0.8507, rho_lr: 0.020000\n",
            "rho0: 0.6520, rho1: 0.2000\n",
            "Epoch 37, Loss: 0.8390, rho_lr: 0.020000\n",
            "rho0: 0.6545, rho1: 0.2000\n",
            "Epoch 38, Loss: 0.8352, rho_lr: 0.020000\n",
            "rho0: 0.6573, rho1: 0.2000\n",
            "Epoch 39, Loss: 0.8279, rho_lr: 0.020000\n",
            "rho0: 0.6599, rho1: 0.2000\n",
            "Epoch 40, Loss: 0.8107, rho_lr: 0.002000\n",
            "rho0: 0.6623, rho1: 0.2000\n",
            "Epoch 41, Loss: 0.8062, rho_lr: 0.002000\n",
            "rho0: 0.6626, rho1: 0.2000\n",
            "Epoch 42, Loss: 0.8016, rho_lr: 0.002000\n",
            "rho0: 0.6629, rho1: 0.2000\n",
            "Epoch 43, Loss: 0.7977, rho_lr: 0.002000\n",
            "rho0: 0.6632, rho1: 0.2000\n",
            "Epoch 44, Loss: 0.7900, rho_lr: 0.002000\n",
            "rho0: 0.6636, rho1: 0.2000\n",
            "Epoch 45, Loss: 0.7870, rho_lr: 0.002000\n",
            "rho0: 0.6640, rho1: 0.2000\n",
            "Epoch 46, Loss: 0.7770, rho_lr: 0.002000\n",
            "rho0: 0.6645, rho1: 0.2000\n",
            "Epoch 47, Loss: 0.7670, rho_lr: 0.002000\n",
            "rho0: 0.6650, rho1: 0.2000\n",
            "Epoch 48, Loss: 0.7716, rho_lr: 0.002000\n",
            "rho0: 0.6654, rho1: 0.2000\n",
            "Epoch 49, Loss: 0.7585, rho_lr: 0.002000\n",
            "rho0: 0.6659, rho1: 0.2000\n",
            "Epoch 50, Loss: 0.7486, rho_lr: 0.002000\n",
            "rho0: 0.6665, rho1: 0.2000\n",
            "Epoch 51, Loss: 0.7461, rho_lr: 0.002000\n",
            "rho0: 0.6670, rho1: 0.2000\n",
            "Epoch 52, Loss: 0.7439, rho_lr: 0.002000\n",
            "rho0: 0.6675, rho1: 0.2000\n",
            "Epoch 53, Loss: 0.7360, rho_lr: 0.002000\n",
            "rho0: 0.6681, rho1: 0.2000\n",
            "Epoch 54, Loss: 0.7357, rho_lr: 0.002000\n",
            "rho0: 0.6686, rho1: 0.2000\n",
            "Epoch 55, Loss: 0.7285, rho_lr: 0.002000\n",
            "rho0: 0.6692, rho1: 0.2000\n",
            "Epoch 56, Loss: 0.7271, rho_lr: 0.002000\n",
            "rho0: 0.6697, rho1: 0.2000\n",
            "Epoch 57, Loss: 0.7199, rho_lr: 0.002000\n",
            "rho0: 0.6703, rho1: 0.2000\n",
            "Epoch 58, Loss: 0.7202, rho_lr: 0.002000\n",
            "rho0: 0.6709, rho1: 0.2000\n",
            "Epoch 59, Loss: 0.7065, rho_lr: 0.002000\n",
            "rho0: 0.6714, rho1: 0.2000\n",
            "Epoch 60, Loss: 0.7112, rho_lr: 0.000200\n",
            "rho0: 0.6721, rho1: 0.2000\n",
            "Epoch 1, Loss: 31.7940, rho_lr: 0.200000\n",
            "rho0: 0.0000, rho1: 0.5000\n",
            "Epoch 2, Loss: 14.3701, rho_lr: 0.200000\n",
            "rho0: 0.2930, rho1: 0.5000\n",
            "Epoch 3, Loss: 7.0806, rho_lr: 0.200000\n",
            "rho0: 0.2756, rho1: 0.5000\n",
            "Epoch 4, Loss: 4.4029, rho_lr: 0.200000\n",
            "rho0: 0.3037, rho1: 0.5000\n",
            "Epoch 5, Loss: 3.1730, rho_lr: 0.200000\n",
            "rho0: 0.3309, rho1: 0.5000\n",
            "Epoch 6, Loss: 2.6542, rho_lr: 0.200000\n",
            "rho0: 0.3200, rho1: 0.5000\n",
            "Epoch 7, Loss: 2.3245, rho_lr: 0.200000\n",
            "rho0: 0.3092, rho1: 0.5000\n",
            "Epoch 8, Loss: 2.1320, rho_lr: 0.200000\n",
            "rho0: 0.3259, rho1: 0.5000\n",
            "Epoch 9, Loss: 1.9854, rho_lr: 0.200000\n",
            "rho0: 0.3224, rho1: 0.5000\n",
            "Epoch 10, Loss: 1.8525, rho_lr: 0.200000\n",
            "rho0: 0.3255, rho1: 0.5000\n",
            "Epoch 11, Loss: 1.9277, rho_lr: 0.200000\n",
            "rho0: 0.3731, rho1: 0.3882\n",
            "Epoch 12, Loss: 1.4665, rho_lr: 0.200000\n",
            "rho0: 0.4852, rho1: 0.2657\n",
            "Epoch 13, Loss: 1.0888, rho_lr: 0.200000\n",
            "rho0: 0.5781, rho1: 0.2000\n",
            "Epoch 14, Loss: 1.0084, rho_lr: 0.200000\n",
            "rho0: 0.5527, rho1: 0.2000\n",
            "Epoch 15, Loss: 0.9824, rho_lr: 0.200000\n",
            "rho0: 0.5694, rho1: 0.2000\n",
            "Epoch 16, Loss: 0.9557, rho_lr: 0.200000\n",
            "rho0: 0.5805, rho1: 0.2000\n",
            "Epoch 17, Loss: 0.9366, rho_lr: 0.200000\n",
            "rho0: 0.5775, rho1: 0.2000\n",
            "Epoch 18, Loss: 0.9239, rho_lr: 0.200000\n",
            "rho0: 0.5871, rho1: 0.2000\n",
            "Epoch 19, Loss: 0.9032, rho_lr: 0.200000\n",
            "rho0: 0.5943, rho1: 0.2000\n",
            "Epoch 20, Loss: 0.8964, rho_lr: 0.020000\n",
            "rho0: 0.5964, rho1: 0.2000\n",
            "Epoch 21, Loss: 0.8807, rho_lr: 0.020000\n",
            "rho0: 0.5980, rho1: 0.2000\n",
            "Epoch 22, Loss: 0.8715, rho_lr: 0.020000\n",
            "rho0: 0.6008, rho1: 0.2000\n",
            "Epoch 23, Loss: 0.8590, rho_lr: 0.020000\n",
            "rho0: 0.6034, rho1: 0.2000\n",
            "Epoch 24, Loss: 0.8394, rho_lr: 0.020000\n",
            "rho0: 0.6064, rho1: 0.2000\n",
            "Epoch 25, Loss: 0.8332, rho_lr: 0.020000\n",
            "rho0: 0.6099, rho1: 0.2000\n",
            "Epoch 26, Loss: 0.8177, rho_lr: 0.020000\n",
            "rho0: 0.6135, rho1: 0.2000\n",
            "Epoch 27, Loss: 0.8070, rho_lr: 0.020000\n",
            "rho0: 0.6171, rho1: 0.2000\n",
            "Epoch 28, Loss: 0.7995, rho_lr: 0.020000\n",
            "rho0: 0.6210, rho1: 0.2000\n",
            "Epoch 29, Loss: 0.7882, rho_lr: 0.020000\n",
            "rho0: 0.6236, rho1: 0.2000\n",
            "Epoch 30, Loss: 0.7854, rho_lr: 0.020000\n",
            "rho0: 0.6263, rho1: 0.2000\n",
            "Epoch 31, Loss: 0.7748, rho_lr: 0.020000\n",
            "rho0: 0.6304, rho1: 0.2000\n",
            "Epoch 32, Loss: 0.7641, rho_lr: 0.020000\n",
            "rho0: 0.6339, rho1: 0.2000\n",
            "Epoch 33, Loss: 0.7558, rho_lr: 0.020000\n",
            "rho0: 0.6368, rho1: 0.2000\n",
            "Epoch 34, Loss: 0.7434, rho_lr: 0.020000\n",
            "rho0: 0.6405, rho1: 0.2000\n",
            "Epoch 35, Loss: 0.7373, rho_lr: 0.020000\n",
            "rho0: 0.6440, rho1: 0.2000\n",
            "Epoch 36, Loss: 0.7260, rho_lr: 0.020000\n",
            "rho0: 0.6472, rho1: 0.2000\n",
            "Epoch 37, Loss: 0.7275, rho_lr: 0.020000\n",
            "rho0: 0.6510, rho1: 0.2000\n",
            "Epoch 38, Loss: 0.7192, rho_lr: 0.020000\n",
            "rho0: 0.6545, rho1: 0.2000\n",
            "Epoch 39, Loss: 0.7087, rho_lr: 0.020000\n",
            "rho0: 0.6579, rho1: 0.2000\n",
            "Epoch 40, Loss: 0.7008, rho_lr: 0.002000\n",
            "rho0: 0.6615, rho1: 0.2000\n",
            "Epoch 41, Loss: 0.6953, rho_lr: 0.002000\n",
            "rho0: 0.6618, rho1: 0.2000\n",
            "Epoch 42, Loss: 0.6916, rho_lr: 0.002000\n",
            "rho0: 0.6623, rho1: 0.2000\n",
            "Epoch 43, Loss: 0.6887, rho_lr: 0.002000\n",
            "rho0: 0.6629, rho1: 0.2000\n",
            "Epoch 44, Loss: 0.6789, rho_lr: 0.002000\n",
            "rho0: 0.6635, rho1: 0.2000\n",
            "Epoch 45, Loss: 0.6786, rho_lr: 0.002000\n",
            "rho0: 0.6643, rho1: 0.2000\n",
            "Epoch 46, Loss: 0.6719, rho_lr: 0.002000\n",
            "rho0: 0.6650, rho1: 0.2000\n",
            "Epoch 47, Loss: 0.6651, rho_lr: 0.002000\n",
            "rho0: 0.6656, rho1: 0.2000\n",
            "Epoch 48, Loss: 0.6656, rho_lr: 0.002000\n",
            "rho0: 0.6663, rho1: 0.2000\n",
            "Epoch 49, Loss: 0.6620, rho_lr: 0.002000\n",
            "rho0: 0.6671, rho1: 0.2000\n",
            "Epoch 50, Loss: 0.6578, rho_lr: 0.002000\n",
            "rho0: 0.6679, rho1: 0.2000\n",
            "Epoch 51, Loss: 0.6496, rho_lr: 0.002000\n",
            "rho0: 0.6686, rho1: 0.2000\n",
            "Epoch 52, Loss: 0.6493, rho_lr: 0.002000\n",
            "rho0: 0.6694, rho1: 0.2000\n",
            "Epoch 53, Loss: 0.6465, rho_lr: 0.002000\n",
            "rho0: 0.6701, rho1: 0.2000\n",
            "Epoch 54, Loss: 0.6459, rho_lr: 0.002000\n",
            "rho0: 0.6709, rho1: 0.2000\n",
            "Epoch 55, Loss: 0.6410, rho_lr: 0.002000\n",
            "rho0: 0.6717, rho1: 0.2000\n",
            "Epoch 56, Loss: 0.6347, rho_lr: 0.002000\n",
            "rho0: 0.6725, rho1: 0.2000\n",
            "Epoch 57, Loss: 0.6334, rho_lr: 0.002000\n",
            "rho0: 0.6733, rho1: 0.2000\n",
            "Epoch 58, Loss: 0.6305, rho_lr: 0.002000\n",
            "rho0: 0.6741, rho1: 0.2000\n",
            "Epoch 59, Loss: 0.6320, rho_lr: 0.002000\n",
            "rho0: 0.6749, rho1: 0.2000\n",
            "Epoch 60, Loss: 0.6258, rho_lr: 0.000200\n",
            "rho0: 0.6757, rho1: 0.2000\n",
            "Epoch 1, Loss: 35.9093, rho_lr: 0.200000\n",
            "rho0: 0.0000, rho1: 0.5000\n",
            "Epoch 2, Loss: 13.4391, rho_lr: 0.200000\n",
            "rho0: 0.2412, rho1: 0.5000\n",
            "Epoch 3, Loss: 6.9333, rho_lr: 0.200000\n",
            "rho0: 0.2981, rho1: 0.5000\n",
            "Epoch 4, Loss: 4.7343, rho_lr: 0.200000\n",
            "rho0: 0.3072, rho1: 0.5000\n",
            "Epoch 5, Loss: 3.5950, rho_lr: 0.200000\n",
            "rho0: 0.3485, rho1: 0.5000\n",
            "Epoch 6, Loss: 2.9394, rho_lr: 0.200000\n",
            "rho0: 0.3413, rho1: 0.5000\n",
            "Epoch 7, Loss: 2.5612, rho_lr: 0.200000\n",
            "rho0: 0.3444, rho1: 0.5000\n",
            "Epoch 8, Loss: 2.3973, rho_lr: 0.200000\n",
            "rho0: 0.3406, rho1: 0.5000\n",
            "Epoch 9, Loss: 2.3234, rho_lr: 0.200000\n",
            "rho0: 0.3465, rho1: 0.5000\n",
            "Epoch 10, Loss: 2.1659, rho_lr: 0.200000\n",
            "rho0: 0.3491, rho1: 0.5000\n",
            "Epoch 11, Loss: 2.1790, rho_lr: 0.200000\n",
            "rho0: 0.4009, rho1: 0.3849\n",
            "Epoch 12, Loss: 1.6401, rho_lr: 0.200000\n",
            "rho0: 0.5188, rho1: 0.2622\n",
            "Epoch 13, Loss: 1.1326, rho_lr: 0.200000\n",
            "rho0: 0.5975, rho1: 0.2000\n",
            "Epoch 14, Loss: 1.0785, rho_lr: 0.200000\n",
            "rho0: 0.5846, rho1: 0.2000\n",
            "Epoch 15, Loss: 1.0186, rho_lr: 0.200000\n",
            "rho0: 0.5932, rho1: 0.2000\n",
            "Epoch 16, Loss: 0.9921, rho_lr: 0.200000\n",
            "rho0: 0.6062, rho1: 0.2000\n",
            "Epoch 17, Loss: 0.9847, rho_lr: 0.200000\n",
            "rho0: 0.6046, rho1: 0.2000\n",
            "Epoch 18, Loss: 0.9734, rho_lr: 0.200000\n",
            "rho0: 0.6155, rho1: 0.2000\n",
            "Epoch 19, Loss: 0.9490, rho_lr: 0.200000\n",
            "rho0: 0.6199, rho1: 0.2000\n",
            "Epoch 20, Loss: 0.9244, rho_lr: 0.020000\n",
            "rho0: 0.6239, rho1: 0.2000\n",
            "Epoch 21, Loss: 0.9225, rho_lr: 0.020000\n",
            "rho0: 0.6254, rho1: 0.2000\n",
            "Epoch 22, Loss: 0.9157, rho_lr: 0.020000\n",
            "rho0: 0.6274, rho1: 0.2000\n",
            "Epoch 23, Loss: 0.9003, rho_lr: 0.020000\n",
            "rho0: 0.6302, rho1: 0.2000\n",
            "Epoch 24, Loss: 0.8844, rho_lr: 0.020000\n",
            "rho0: 0.6329, rho1: 0.2000\n",
            "Epoch 25, Loss: 0.8632, rho_lr: 0.020000\n",
            "rho0: 0.6356, rho1: 0.2000\n",
            "Epoch 26, Loss: 0.8686, rho_lr: 0.020000\n",
            "rho0: 0.6385, rho1: 0.2000\n",
            "Epoch 27, Loss: 0.8634, rho_lr: 0.020000\n",
            "rho0: 0.6413, rho1: 0.2000\n",
            "Epoch 28, Loss: 0.8469, rho_lr: 0.020000\n",
            "rho0: 0.6443, rho1: 0.2000\n",
            "Epoch 29, Loss: 0.8429, rho_lr: 0.020000\n",
            "rho0: 0.6474, rho1: 0.2000\n",
            "Epoch 30, Loss: 0.8317, rho_lr: 0.020000\n",
            "rho0: 0.6504, rho1: 0.2000\n",
            "Epoch 31, Loss: 0.8184, rho_lr: 0.020000\n",
            "rho0: 0.6535, rho1: 0.2000\n",
            "Epoch 32, Loss: 0.8099, rho_lr: 0.020000\n",
            "rho0: 0.6559, rho1: 0.2000\n",
            "Epoch 33, Loss: 0.8064, rho_lr: 0.020000\n",
            "rho0: 0.6596, rho1: 0.2000\n",
            "Epoch 34, Loss: 0.7934, rho_lr: 0.020000\n",
            "rho0: 0.6623, rho1: 0.2000\n",
            "Epoch 35, Loss: 0.7847, rho_lr: 0.020000\n",
            "rho0: 0.6651, rho1: 0.2000\n",
            "Epoch 36, Loss: 0.7772, rho_lr: 0.020000\n",
            "rho0: 0.6681, rho1: 0.2000\n",
            "Epoch 37, Loss: 0.7696, rho_lr: 0.020000\n",
            "rho0: 0.6710, rho1: 0.2000\n",
            "Epoch 38, Loss: 0.7640, rho_lr: 0.020000\n",
            "rho0: 0.6738, rho1: 0.2000\n",
            "Epoch 39, Loss: 0.7593, rho_lr: 0.020000\n",
            "rho0: 0.6769, rho1: 0.2000\n",
            "Epoch 40, Loss: 0.7487, rho_lr: 0.002000\n",
            "rho0: 0.6797, rho1: 0.2000\n",
            "Epoch 41, Loss: 0.7477, rho_lr: 0.002000\n",
            "rho0: 0.6800, rho1: 0.2000\n",
            "Epoch 42, Loss: 0.7345, rho_lr: 0.002000\n",
            "rho0: 0.6804, rho1: 0.2000\n",
            "Epoch 43, Loss: 0.7299, rho_lr: 0.002000\n",
            "rho0: 0.6809, rho1: 0.2000\n",
            "Epoch 44, Loss: 0.7306, rho_lr: 0.002000\n",
            "rho0: 0.6814, rho1: 0.2000\n",
            "Epoch 45, Loss: 0.7211, rho_lr: 0.002000\n",
            "rho0: 0.6820, rho1: 0.2000\n",
            "Epoch 46, Loss: 0.7173, rho_lr: 0.002000\n",
            "rho0: 0.6825, rho1: 0.2000\n",
            "Epoch 47, Loss: 0.7092, rho_lr: 0.002000\n",
            "rho0: 0.6830, rho1: 0.2000\n",
            "Epoch 48, Loss: 0.7056, rho_lr: 0.002000\n",
            "rho0: 0.6837, rho1: 0.2000\n",
            "Epoch 49, Loss: 0.7029, rho_lr: 0.002000\n",
            "rho0: 0.6843, rho1: 0.2000\n",
            "Epoch 50, Loss: 0.6970, rho_lr: 0.002000\n",
            "rho0: 0.6849, rho1: 0.2000\n",
            "Epoch 51, Loss: 0.6961, rho_lr: 0.002000\n",
            "rho0: 0.6856, rho1: 0.2000\n",
            "Epoch 52, Loss: 0.6861, rho_lr: 0.002000\n",
            "rho0: 0.6861, rho1: 0.2000\n",
            "Epoch 53, Loss: 0.6849, rho_lr: 0.002000\n",
            "rho0: 0.6867, rho1: 0.2000\n",
            "Epoch 54, Loss: 0.6826, rho_lr: 0.002000\n",
            "rho0: 0.6874, rho1: 0.2000\n",
            "Epoch 55, Loss: 0.6776, rho_lr: 0.002000\n",
            "rho0: 0.6880, rho1: 0.2000\n",
            "Epoch 56, Loss: 0.6808, rho_lr: 0.002000\n",
            "rho0: 0.6887, rho1: 0.2000\n",
            "Epoch 57, Loss: 0.6693, rho_lr: 0.002000\n",
            "rho0: 0.6894, rho1: 0.2000\n",
            "Epoch 58, Loss: 0.6668, rho_lr: 0.002000\n",
            "rho0: 0.6901, rho1: 0.2000\n",
            "Epoch 59, Loss: 0.6630, rho_lr: 0.002000\n",
            "rho0: 0.6908, rho1: 0.2000\n",
            "Epoch 60, Loss: 0.6603, rho_lr: 0.000200\n",
            "rho0: 0.6915, rho1: 0.2000\n",
            "Epoch 1, Loss: 35.2576, rho_lr: 0.200000\n",
            "rho0: 0.0000, rho1: 0.5000\n",
            "Epoch 2, Loss: 16.2802, rho_lr: 0.200000\n",
            "rho0: 0.2074, rho1: 0.5000\n",
            "Epoch 3, Loss: 10.2485, rho_lr: 0.200000\n",
            "rho0: 0.2591, rho1: 0.5000\n",
            "Epoch 4, Loss: 6.2532, rho_lr: 0.200000\n",
            "rho0: 0.2592, rho1: 0.5000\n",
            "Epoch 5, Loss: 4.2230, rho_lr: 0.200000\n",
            "rho0: 0.2833, rho1: 0.5000\n",
            "Epoch 6, Loss: 3.4927, rho_lr: 0.200000\n",
            "rho0: 0.3213, rho1: 0.5000\n",
            "Epoch 7, Loss: 2.8176, rho_lr: 0.200000\n",
            "rho0: 0.3092, rho1: 0.5000\n",
            "Epoch 8, Loss: 2.5606, rho_lr: 0.200000\n",
            "rho0: 0.3130, rho1: 0.5000\n",
            "Epoch 9, Loss: 2.2543, rho_lr: 0.200000\n",
            "rho0: 0.3140, rho1: 0.5000\n",
            "Epoch 10, Loss: 2.1287, rho_lr: 0.200000\n",
            "rho0: 0.3245, rho1: 0.5000\n",
            "Epoch 11, Loss: 2.1531, rho_lr: 0.200000\n",
            "rho0: 0.3996, rho1: 0.3719\n",
            "Epoch 12, Loss: 1.6678, rho_lr: 0.200000\n",
            "rho0: 0.5241, rho1: 0.2268\n",
            "Epoch 13, Loss: 1.1351, rho_lr: 0.200000\n",
            "rho0: 0.5796, rho1: 0.2000\n",
            "Epoch 14, Loss: 1.1057, rho_lr: 0.200000\n",
            "rho0: 0.5727, rho1: 0.2000\n",
            "Epoch 15, Loss: 1.0638, rho_lr: 0.200000\n",
            "rho0: 0.5812, rho1: 0.2000\n",
            "Epoch 16, Loss: 1.0346, rho_lr: 0.200000\n",
            "rho0: 0.5947, rho1: 0.2000\n",
            "Epoch 17, Loss: 1.0276, rho_lr: 0.200000\n",
            "rho0: 0.5955, rho1: 0.2000\n",
            "Epoch 18, Loss: 0.9925, rho_lr: 0.200000\n",
            "rho0: 0.6069, rho1: 0.2000\n",
            "Epoch 19, Loss: 0.9849, rho_lr: 0.200000\n",
            "rho0: 0.6100, rho1: 0.2000\n",
            "Epoch 20, Loss: 0.9708, rho_lr: 0.020000\n",
            "rho0: 0.6161, rho1: 0.2000\n",
            "Epoch 21, Loss: 0.9366, rho_lr: 0.020000\n",
            "rho0: 0.6176, rho1: 0.2000\n",
            "Epoch 22, Loss: 0.9447, rho_lr: 0.020000\n",
            "rho0: 0.6195, rho1: 0.2000\n",
            "Epoch 23, Loss: 0.9186, rho_lr: 0.020000\n",
            "rho0: 0.6220, rho1: 0.2000\n",
            "Epoch 24, Loss: 0.9090, rho_lr: 0.020000\n",
            "rho0: 0.6252, rho1: 0.2000\n",
            "Epoch 25, Loss: 0.9021, rho_lr: 0.020000\n",
            "rho0: 0.6285, rho1: 0.2000\n",
            "Epoch 26, Loss: 0.8948, rho_lr: 0.020000\n",
            "rho0: 0.6320, rho1: 0.2000\n",
            "Epoch 27, Loss: 0.8791, rho_lr: 0.020000\n",
            "rho0: 0.6352, rho1: 0.2000\n",
            "Epoch 28, Loss: 0.8669, rho_lr: 0.020000\n",
            "rho0: 0.6383, rho1: 0.2000\n",
            "Epoch 29, Loss: 0.8601, rho_lr: 0.020000\n",
            "rho0: 0.6415, rho1: 0.2000\n",
            "Epoch 30, Loss: 0.8447, rho_lr: 0.020000\n",
            "rho0: 0.6446, rho1: 0.2000\n",
            "Epoch 31, Loss: 0.8426, rho_lr: 0.020000\n",
            "rho0: 0.6486, rho1: 0.2000\n",
            "Epoch 32, Loss: 0.8321, rho_lr: 0.020000\n",
            "rho0: 0.6526, rho1: 0.2000\n",
            "Epoch 33, Loss: 0.8227, rho_lr: 0.020000\n",
            "rho0: 0.6554, rho1: 0.2000\n",
            "Epoch 34, Loss: 0.8169, rho_lr: 0.020000\n",
            "rho0: 0.6583, rho1: 0.2000\n",
            "Epoch 35, Loss: 0.8020, rho_lr: 0.020000\n",
            "rho0: 0.6617, rho1: 0.2000\n",
            "Epoch 36, Loss: 0.7948, rho_lr: 0.020000\n",
            "rho0: 0.6648, rho1: 0.2000\n",
            "Epoch 37, Loss: 0.7904, rho_lr: 0.020000\n",
            "rho0: 0.6673, rho1: 0.2000\n",
            "Epoch 38, Loss: 0.7811, rho_lr: 0.020000\n",
            "rho0: 0.6704, rho1: 0.2000\n",
            "Epoch 39, Loss: 0.7774, rho_lr: 0.020000\n",
            "rho0: 0.6735, rho1: 0.2000\n",
            "Epoch 40, Loss: 0.7573, rho_lr: 0.002000\n",
            "rho0: 0.6764, rho1: 0.2000\n",
            "Epoch 41, Loss: 0.7537, rho_lr: 0.002000\n",
            "rho0: 0.6768, rho1: 0.2000\n",
            "Epoch 42, Loss: 0.7497, rho_lr: 0.002000\n",
            "rho0: 0.6772, rho1: 0.2000\n",
            "Epoch 43, Loss: 0.7443, rho_lr: 0.002000\n",
            "rho0: 0.6778, rho1: 0.2000\n",
            "Epoch 44, Loss: 0.7349, rho_lr: 0.002000\n",
            "rho0: 0.6783, rho1: 0.2000\n",
            "Epoch 45, Loss: 0.7309, rho_lr: 0.002000\n",
            "rho0: 0.6789, rho1: 0.2000\n",
            "Epoch 46, Loss: 0.7305, rho_lr: 0.002000\n",
            "rho0: 0.6794, rho1: 0.2000\n",
            "Epoch 47, Loss: 0.7256, rho_lr: 0.002000\n",
            "rho0: 0.6800, rho1: 0.2000\n",
            "Epoch 48, Loss: 0.7237, rho_lr: 0.002000\n",
            "rho0: 0.6806, rho1: 0.2000\n",
            "Epoch 49, Loss: 0.7162, rho_lr: 0.002000\n",
            "rho0: 0.6812, rho1: 0.2000\n",
            "Epoch 50, Loss: 0.7129, rho_lr: 0.002000\n",
            "rho0: 0.6818, rho1: 0.2000\n",
            "Epoch 51, Loss: 0.7095, rho_lr: 0.002000\n",
            "rho0: 0.6825, rho1: 0.2000\n",
            "Epoch 52, Loss: 0.7060, rho_lr: 0.002000\n",
            "rho0: 0.6832, rho1: 0.2000\n",
            "Epoch 53, Loss: 0.7005, rho_lr: 0.002000\n",
            "rho0: 0.6839, rho1: 0.2000\n",
            "Epoch 54, Loss: 0.6956, rho_lr: 0.002000\n",
            "rho0: 0.6846, rho1: 0.2000\n",
            "Epoch 55, Loss: 0.6945, rho_lr: 0.002000\n",
            "rho0: 0.6852, rho1: 0.2000\n",
            "Epoch 56, Loss: 0.6920, rho_lr: 0.002000\n",
            "rho0: 0.6859, rho1: 0.2000\n",
            "Epoch 57, Loss: 0.6841, rho_lr: 0.002000\n",
            "rho0: 0.6866, rho1: 0.2000\n",
            "Epoch 58, Loss: 0.6829, rho_lr: 0.002000\n",
            "rho0: 0.6874, rho1: 0.2000\n",
            "Epoch 59, Loss: 0.6781, rho_lr: 0.002000\n",
            "rho0: 0.6881, rho1: 0.2000\n",
            "Epoch 60, Loss: 0.6723, rho_lr: 0.000200\n",
            "rho0: 0.6888, rho1: 0.2000\n",
            "Epoch 1, Loss: 26.8000, rho_lr: 0.200000\n",
            "rho0: 0.0475, rho1: 0.5000\n",
            "Epoch 2, Loss: 10.2771, rho_lr: 0.200000\n",
            "rho0: 0.2467, rho1: 0.5000\n",
            "Epoch 3, Loss: 6.0928, rho_lr: 0.200000\n",
            "rho0: 0.3877, rho1: 0.5000\n",
            "Epoch 4, Loss: 4.0415, rho_lr: 0.200000\n",
            "rho0: 0.3377, rho1: 0.5000\n",
            "Epoch 5, Loss: 2.8831, rho_lr: 0.200000\n",
            "rho0: 0.3838, rho1: 0.5000\n",
            "Epoch 6, Loss: 2.3003, rho_lr: 0.200000\n",
            "rho0: 0.3636, rho1: 0.5000\n",
            "Epoch 7, Loss: 2.0537, rho_lr: 0.200000\n",
            "rho0: 0.3587, rho1: 0.5000\n",
            "Epoch 8, Loss: 1.8850, rho_lr: 0.200000\n",
            "rho0: 0.3643, rho1: 0.5000\n",
            "Epoch 9, Loss: 1.7833, rho_lr: 0.200000\n",
            "rho0: 0.3669, rho1: 0.5000\n",
            "Epoch 10, Loss: 1.6877, rho_lr: 0.200000\n",
            "rho0: 0.3677, rho1: 0.5000\n",
            "Epoch 11, Loss: 1.7047, rho_lr: 0.200000\n",
            "rho0: 0.4395, rho1: 0.3962\n",
            "Epoch 12, Loss: 1.2842, rho_lr: 0.200000\n",
            "rho0: 0.5564, rho1: 0.2489\n",
            "Epoch 13, Loss: 0.9413, rho_lr: 0.200000\n",
            "rho0: 0.6148, rho1: 0.2000\n",
            "Epoch 14, Loss: 0.8987, rho_lr: 0.200000\n",
            "rho0: 0.6055, rho1: 0.2000\n",
            "Epoch 15, Loss: 0.8836, rho_lr: 0.200000\n",
            "rho0: 0.6112, rho1: 0.2000\n",
            "Epoch 16, Loss: 0.8623, rho_lr: 0.200000\n",
            "rho0: 0.6180, rho1: 0.2000\n",
            "Epoch 17, Loss: 0.8524, rho_lr: 0.200000\n",
            "rho0: 0.6263, rho1: 0.2000\n",
            "Epoch 18, Loss: 0.8328, rho_lr: 0.200000\n",
            "rho0: 0.6277, rho1: 0.2000\n",
            "Epoch 19, Loss: 0.8266, rho_lr: 0.200000\n",
            "rho0: 0.6358, rho1: 0.2000\n",
            "Epoch 20, Loss: 0.8208, rho_lr: 0.020000\n",
            "rho0: 0.6368, rho1: 0.2000\n",
            "Epoch 21, Loss: 0.8070, rho_lr: 0.020000\n",
            "rho0: 0.6378, rho1: 0.2000\n",
            "Epoch 22, Loss: 0.7939, rho_lr: 0.020000\n",
            "rho0: 0.6410, rho1: 0.2000\n",
            "Epoch 23, Loss: 0.7877, rho_lr: 0.020000\n",
            "rho0: 0.6447, rho1: 0.2000\n",
            "Epoch 24, Loss: 0.7799, rho_lr: 0.020000\n",
            "rho0: 0.6477, rho1: 0.2000\n",
            "Epoch 25, Loss: 0.7719, rho_lr: 0.020000\n",
            "rho0: 0.6509, rho1: 0.2000\n",
            "Epoch 26, Loss: 0.7638, rho_lr: 0.020000\n",
            "rho0: 0.6542, rho1: 0.2000\n",
            "Epoch 27, Loss: 0.7526, rho_lr: 0.020000\n",
            "rho0: 0.6577, rho1: 0.2000\n",
            "Epoch 28, Loss: 0.7444, rho_lr: 0.020000\n",
            "rho0: 0.6609, rho1: 0.2000\n",
            "Epoch 29, Loss: 0.7418, rho_lr: 0.020000\n",
            "rho0: 0.6636, rho1: 0.2000\n",
            "Epoch 30, Loss: 0.7296, rho_lr: 0.020000\n",
            "rho0: 0.6664, rho1: 0.2000\n",
            "Epoch 31, Loss: 0.7289, rho_lr: 0.020000\n",
            "rho0: 0.6694, rho1: 0.2000\n",
            "Epoch 32, Loss: 0.7174, rho_lr: 0.020000\n",
            "rho0: 0.6722, rho1: 0.2000\n",
            "Epoch 33, Loss: 0.7133, rho_lr: 0.020000\n",
            "rho0: 0.6753, rho1: 0.2000\n",
            "Epoch 34, Loss: 0.7096, rho_lr: 0.020000\n",
            "rho0: 0.6788, rho1: 0.2000\n",
            "Epoch 35, Loss: 0.7002, rho_lr: 0.020000\n",
            "rho0: 0.6816, rho1: 0.2000\n",
            "Epoch 36, Loss: 0.6961, rho_lr: 0.020000\n",
            "rho0: 0.6844, rho1: 0.2000\n",
            "Epoch 37, Loss: 0.6894, rho_lr: 0.020000\n",
            "rho0: 0.6874, rho1: 0.2000\n",
            "Epoch 38, Loss: 0.6828, rho_lr: 0.020000\n",
            "rho0: 0.6899, rho1: 0.2000\n",
            "Epoch 39, Loss: 0.6837, rho_lr: 0.020000\n",
            "rho0: 0.6927, rho1: 0.2000\n",
            "Epoch 40, Loss: 0.6731, rho_lr: 0.002000\n",
            "rho0: 0.6961, rho1: 0.2000\n",
            "Epoch 41, Loss: 0.6700, rho_lr: 0.002000\n",
            "rho0: 0.6963, rho1: 0.2000\n",
            "Epoch 42, Loss: 0.6658, rho_lr: 0.002000\n",
            "rho0: 0.6967, rho1: 0.2000\n",
            "Epoch 43, Loss: 0.6606, rho_lr: 0.002000\n",
            "rho0: 0.6973, rho1: 0.2000\n",
            "Epoch 44, Loss: 0.6559, rho_lr: 0.002000\n",
            "rho0: 0.6978, rho1: 0.2000\n",
            "Epoch 45, Loss: 0.6599, rho_lr: 0.002000\n",
            "rho0: 0.6984, rho1: 0.2000\n",
            "Epoch 46, Loss: 0.6558, rho_lr: 0.002000\n",
            "rho0: 0.6989, rho1: 0.2000\n",
            "Epoch 47, Loss: 0.6480, rho_lr: 0.002000\n",
            "rho0: 0.6995, rho1: 0.2000\n",
            "Epoch 48, Loss: 0.6495, rho_lr: 0.002000\n",
            "rho0: 0.7001, rho1: 0.2000\n",
            "Epoch 49, Loss: 0.6457, rho_lr: 0.002000\n",
            "rho0: 0.7007, rho1: 0.2000\n",
            "Epoch 50, Loss: 0.6412, rho_lr: 0.002000\n",
            "rho0: 0.7014, rho1: 0.2000\n",
            "Epoch 51, Loss: 0.6363, rho_lr: 0.002000\n",
            "rho0: 0.7020, rho1: 0.2000\n",
            "Epoch 52, Loss: 0.6329, rho_lr: 0.002000\n",
            "rho0: 0.7026, rho1: 0.2000\n",
            "Epoch 53, Loss: 0.6263, rho_lr: 0.002000\n",
            "rho0: 0.7032, rho1: 0.2000\n",
            "Epoch 54, Loss: 0.6299, rho_lr: 0.002000\n",
            "rho0: 0.7038, rho1: 0.2000\n",
            "Epoch 55, Loss: 0.6278, rho_lr: 0.002000\n",
            "rho0: 0.7044, rho1: 0.2000\n",
            "Epoch 56, Loss: 0.6232, rho_lr: 0.002000\n",
            "rho0: 0.7051, rho1: 0.2000\n",
            "Epoch 57, Loss: 0.6233, rho_lr: 0.002000\n",
            "rho0: 0.7058, rho1: 0.2000\n",
            "Epoch 58, Loss: 0.6190, rho_lr: 0.002000\n",
            "rho0: 0.7064, rho1: 0.2000\n",
            "Epoch 59, Loss: 0.6192, rho_lr: 0.002000\n",
            "rho0: 0.7070, rho1: 0.2000\n",
            "Epoch 60, Loss: 0.6138, rho_lr: 0.000200\n",
            "rho0: 0.7077, rho1: 0.2000\n",
            "Epoch 1, Loss: 43.1326, rho_lr: 0.200000\n",
            "rho0: 0.0000, rho1: 0.5000\n",
            "Epoch 2, Loss: 12.4724, rho_lr: 0.200000\n",
            "rho0: 0.3395, rho1: 0.5000\n",
            "Epoch 3, Loss: 6.5050, rho_lr: 0.200000\n",
            "rho0: 0.2992, rho1: 0.5000\n",
            "Epoch 4, Loss: 4.4152, rho_lr: 0.200000\n",
            "rho0: 0.3315, rho1: 0.5000\n",
            "Epoch 5, Loss: 3.6008, rho_lr: 0.200000\n",
            "rho0: 0.3347, rho1: 0.5000\n",
            "Epoch 6, Loss: 3.0138, rho_lr: 0.200000\n",
            "rho0: 0.3251, rho1: 0.5000\n",
            "Epoch 7, Loss: 2.7041, rho_lr: 0.200000\n",
            "rho0: 0.3266, rho1: 0.5000\n",
            "Epoch 8, Loss: 2.3785, rho_lr: 0.200000\n",
            "rho0: 0.3313, rho1: 0.5000\n",
            "Epoch 9, Loss: 2.2298, rho_lr: 0.200000\n",
            "rho0: 0.3324, rho1: 0.5000\n",
            "Epoch 10, Loss: 2.1272, rho_lr: 0.200000\n",
            "rho0: 0.3347, rho1: 0.5000\n",
            "Epoch 11, Loss: 2.0756, rho_lr: 0.200000\n",
            "rho0: 0.4080, rho1: 0.3615\n",
            "Epoch 12, Loss: 1.5600, rho_lr: 0.200000\n",
            "rho0: 0.5178, rho1: 0.2000\n",
            "Epoch 13, Loss: 1.1240, rho_lr: 0.200000\n",
            "rho0: 0.5828, rho1: 0.2000\n",
            "Epoch 14, Loss: 1.0628, rho_lr: 0.200000\n",
            "rho0: 0.5801, rho1: 0.2000\n",
            "Epoch 15, Loss: 1.0263, rho_lr: 0.200000\n",
            "rho0: 0.5967, rho1: 0.2000\n",
            "Epoch 16, Loss: 1.0081, rho_lr: 0.200000\n",
            "rho0: 0.6013, rho1: 0.2000\n",
            "Epoch 17, Loss: 0.9835, rho_lr: 0.200000\n",
            "rho0: 0.6057, rho1: 0.2000\n",
            "Epoch 18, Loss: 0.9512, rho_lr: 0.200000\n",
            "rho0: 0.6177, rho1: 0.2000\n",
            "Epoch 19, Loss: 0.9389, rho_lr: 0.200000\n",
            "rho0: 0.6217, rho1: 0.2000\n",
            "Epoch 20, Loss: 0.9206, rho_lr: 0.020000\n",
            "rho0: 0.6291, rho1: 0.2000\n",
            "Epoch 21, Loss: 0.8988, rho_lr: 0.020000\n",
            "rho0: 0.6303, rho1: 0.2000\n",
            "Epoch 22, Loss: 0.8933, rho_lr: 0.020000\n",
            "rho0: 0.6319, rho1: 0.2000\n",
            "Epoch 23, Loss: 0.8680, rho_lr: 0.020000\n",
            "rho0: 0.6350, rho1: 0.2000\n",
            "Epoch 24, Loss: 0.8572, rho_lr: 0.020000\n",
            "rho0: 0.6383, rho1: 0.2000\n",
            "Epoch 25, Loss: 0.8480, rho_lr: 0.020000\n",
            "rho0: 0.6418, rho1: 0.2000\n",
            "Epoch 26, Loss: 0.8325, rho_lr: 0.020000\n",
            "rho0: 0.6454, rho1: 0.2000\n",
            "Epoch 27, Loss: 0.8173, rho_lr: 0.020000\n",
            "rho0: 0.6486, rho1: 0.2000\n",
            "Epoch 28, Loss: 0.8147, rho_lr: 0.020000\n",
            "rho0: 0.6523, rho1: 0.2000\n",
            "Epoch 29, Loss: 0.7985, rho_lr: 0.020000\n",
            "rho0: 0.6563, rho1: 0.2000\n",
            "Epoch 30, Loss: 0.7924, rho_lr: 0.020000\n",
            "rho0: 0.6598, rho1: 0.2000\n",
            "Epoch 31, Loss: 0.7784, rho_lr: 0.020000\n",
            "rho0: 0.6634, rho1: 0.2000\n",
            "Epoch 32, Loss: 0.7625, rho_lr: 0.020000\n",
            "rho0: 0.6670, rho1: 0.2000\n",
            "Epoch 33, Loss: 0.7570, rho_lr: 0.020000\n",
            "rho0: 0.6707, rho1: 0.2000\n",
            "Epoch 34, Loss: 0.7502, rho_lr: 0.020000\n",
            "rho0: 0.6744, rho1: 0.2000\n",
            "Epoch 35, Loss: 0.7408, rho_lr: 0.020000\n",
            "rho0: 0.6781, rho1: 0.2000\n",
            "Epoch 36, Loss: 0.7334, rho_lr: 0.020000\n",
            "rho0: 0.6817, rho1: 0.2000\n",
            "Epoch 37, Loss: 0.7178, rho_lr: 0.020000\n",
            "rho0: 0.6853, rho1: 0.2000\n",
            "Epoch 38, Loss: 0.7176, rho_lr: 0.020000\n",
            "rho0: 0.6888, rho1: 0.2000\n",
            "Epoch 39, Loss: 0.7058, rho_lr: 0.020000\n",
            "rho0: 0.6921, rho1: 0.2000\n",
            "Epoch 40, Loss: 0.6991, rho_lr: 0.002000\n",
            "rho0: 0.6957, rho1: 0.2000\n",
            "Epoch 41, Loss: 0.6948, rho_lr: 0.002000\n",
            "rho0: 0.6961, rho1: 0.2000\n",
            "Epoch 42, Loss: 0.6909, rho_lr: 0.002000\n",
            "rho0: 0.6965, rho1: 0.2000\n",
            "Epoch 43, Loss: 0.6876, rho_lr: 0.002000\n",
            "rho0: 0.6970, rho1: 0.2000\n",
            "Epoch 44, Loss: 0.6795, rho_lr: 0.002000\n",
            "rho0: 0.6976, rho1: 0.2000\n",
            "Epoch 45, Loss: 0.6771, rho_lr: 0.002000\n",
            "rho0: 0.6982, rho1: 0.2000\n",
            "Epoch 46, Loss: 0.6739, rho_lr: 0.002000\n",
            "rho0: 0.6988, rho1: 0.2000\n",
            "Epoch 47, Loss: 0.6686, rho_lr: 0.002000\n",
            "rho0: 0.6994, rho1: 0.2000\n",
            "Epoch 48, Loss: 0.6657, rho_lr: 0.002000\n",
            "rho0: 0.7000, rho1: 0.2000\n",
            "Epoch 49, Loss: 0.6637, rho_lr: 0.002000\n",
            "rho0: 0.7006, rho1: 0.2000\n",
            "Epoch 50, Loss: 0.6583, rho_lr: 0.002000\n",
            "rho0: 0.7013, rho1: 0.2000\n",
            "Epoch 51, Loss: 0.6560, rho_lr: 0.002000\n",
            "rho0: 0.7019, rho1: 0.2000\n",
            "Epoch 52, Loss: 0.6546, rho_lr: 0.002000\n",
            "rho0: 0.7026, rho1: 0.2000\n",
            "Epoch 53, Loss: 0.6513, rho_lr: 0.002000\n",
            "rho0: 0.7033, rho1: 0.2000\n",
            "Epoch 54, Loss: 0.6455, rho_lr: 0.002000\n",
            "rho0: 0.7040, rho1: 0.2000\n",
            "Epoch 55, Loss: 0.6438, rho_lr: 0.002000\n",
            "rho0: 0.7047, rho1: 0.2000\n",
            "Epoch 56, Loss: 0.6396, rho_lr: 0.002000\n",
            "rho0: 0.7053, rho1: 0.2000\n",
            "Epoch 57, Loss: 0.6377, rho_lr: 0.002000\n",
            "rho0: 0.7060, rho1: 0.2000\n",
            "Epoch 58, Loss: 0.6374, rho_lr: 0.002000\n",
            "rho0: 0.7067, rho1: 0.2000\n",
            "Epoch 59, Loss: 0.6339, rho_lr: 0.002000\n",
            "rho0: 0.7075, rho1: 0.2000\n",
            "Epoch 60, Loss: 0.6349, rho_lr: 0.000200\n",
            "rho0: 0.7082, rho1: 0.2000\n",
            "Epoch 1, Loss: 82.1259, rho_lr: 0.200000\n",
            "rho0: 0.0000, rho1: 0.5000\n",
            "Epoch 2, Loss: 17.5138, rho_lr: 0.200000\n",
            "rho0: 0.0000, rho1: 0.5000\n",
            "Epoch 3, Loss: 16.5915, rho_lr: 0.200000\n",
            "rho0: 0.1455, rho1: 0.5000\n",
            "Epoch 4, Loss: 10.9353, rho_lr: 0.200000\n",
            "rho0: 0.2617, rho1: 0.5000\n",
            "Epoch 5, Loss: 8.0508, rho_lr: 0.200000\n",
            "rho0: 0.2635, rho1: 0.5000\n",
            "Epoch 6, Loss: 7.0563, rho_lr: 0.200000\n",
            "rho0: 0.2945, rho1: 0.5000\n",
            "Epoch 7, Loss: 5.6949, rho_lr: 0.200000\n",
            "rho0: 0.3175, rho1: 0.5000\n",
            "Epoch 8, Loss: 4.5474, rho_lr: 0.200000\n",
            "rho0: 0.3480, rho1: 0.5000\n",
            "Epoch 9, Loss: 3.6749, rho_lr: 0.200000\n",
            "rho0: 0.3663, rho1: 0.5000\n",
            "Epoch 10, Loss: 3.2599, rho_lr: 0.200000\n",
            "rho0: 0.3633, rho1: 0.5000\n",
            "Epoch 11, Loss: 2.8742, rho_lr: 0.200000\n",
            "rho0: 0.4029, rho1: 0.3341\n",
            "Epoch 12, Loss: 2.1912, rho_lr: 0.200000\n",
            "rho0: 0.4844, rho1: 0.2751\n",
            "Epoch 13, Loss: 1.5913, rho_lr: 0.200000\n",
            "rho0: 0.5633, rho1: 0.2000\n",
            "Epoch 14, Loss: 1.3941, rho_lr: 0.200000\n",
            "rho0: 0.5943, rho1: 0.2000\n",
            "Epoch 15, Loss: 1.3350, rho_lr: 0.200000\n",
            "rho0: 0.6026, rho1: 0.2000\n",
            "Epoch 16, Loss: 1.2678, rho_lr: 0.200000\n",
            "rho0: 0.6078, rho1: 0.2000\n",
            "Epoch 17, Loss: 1.2743, rho_lr: 0.200000\n",
            "rho0: 0.6141, rho1: 0.2000\n",
            "Epoch 18, Loss: 1.1972, rho_lr: 0.200000\n",
            "rho0: 0.6204, rho1: 0.2000\n",
            "Epoch 19, Loss: 1.1875, rho_lr: 0.200000\n",
            "rho0: 0.6234, rho1: 0.2000\n",
            "Epoch 20, Loss: 1.1718, rho_lr: 0.020000\n",
            "rho0: 0.6281, rho1: 0.2000\n",
            "Epoch 21, Loss: 1.1476, rho_lr: 0.020000\n",
            "rho0: 0.6286, rho1: 0.2000\n",
            "Epoch 22, Loss: 1.1382, rho_lr: 0.020000\n",
            "rho0: 0.6295, rho1: 0.2000\n",
            "Epoch 23, Loss: 1.1201, rho_lr: 0.020000\n",
            "rho0: 0.6305, rho1: 0.2000\n",
            "Epoch 24, Loss: 1.1173, rho_lr: 0.020000\n",
            "rho0: 0.6319, rho1: 0.2000\n",
            "Epoch 25, Loss: 1.0970, rho_lr: 0.020000\n",
            "rho0: 0.6333, rho1: 0.2000\n",
            "Epoch 26, Loss: 1.0799, rho_lr: 0.020000\n",
            "rho0: 0.6347, rho1: 0.2000\n",
            "Epoch 27, Loss: 1.0654, rho_lr: 0.020000\n",
            "rho0: 0.6359, rho1: 0.2000\n",
            "Epoch 28, Loss: 1.0338, rho_lr: 0.020000\n",
            "rho0: 0.6372, rho1: 0.2000\n",
            "Epoch 29, Loss: 1.0566, rho_lr: 0.020000\n",
            "rho0: 0.6385, rho1: 0.2000\n",
            "Epoch 30, Loss: 1.0381, rho_lr: 0.020000\n",
            "rho0: 0.6401, rho1: 0.2000\n",
            "Epoch 31, Loss: 1.0312, rho_lr: 0.020000\n",
            "rho0: 0.6417, rho1: 0.2000\n",
            "Epoch 32, Loss: 1.0207, rho_lr: 0.020000\n",
            "rho0: 0.6435, rho1: 0.2000\n",
            "Epoch 33, Loss: 0.9937, rho_lr: 0.020000\n",
            "rho0: 0.6451, rho1: 0.2000\n",
            "Epoch 34, Loss: 0.9998, rho_lr: 0.020000\n",
            "rho0: 0.6467, rho1: 0.2000\n",
            "Epoch 35, Loss: 0.9808, rho_lr: 0.020000\n",
            "rho0: 0.6482, rho1: 0.2000\n",
            "Epoch 36, Loss: 0.9696, rho_lr: 0.020000\n",
            "rho0: 0.6496, rho1: 0.2000\n",
            "Epoch 37, Loss: 0.9777, rho_lr: 0.020000\n",
            "rho0: 0.6512, rho1: 0.2000\n",
            "Epoch 38, Loss: 0.9644, rho_lr: 0.020000\n",
            "rho0: 0.6526, rho1: 0.2000\n",
            "Epoch 39, Loss: 0.9533, rho_lr: 0.020000\n",
            "rho0: 0.6542, rho1: 0.2000\n",
            "Epoch 40, Loss: 0.9502, rho_lr: 0.002000\n",
            "rho0: 0.6561, rho1: 0.2000\n",
            "Epoch 41, Loss: 0.9335, rho_lr: 0.002000\n",
            "rho0: 0.6562, rho1: 0.2000\n",
            "Epoch 42, Loss: 0.9490, rho_lr: 0.002000\n",
            "rho0: 0.6564, rho1: 0.2000\n",
            "Epoch 43, Loss: 0.9232, rho_lr: 0.002000\n",
            "rho0: 0.6566, rho1: 0.2000\n",
            "Epoch 44, Loss: 0.9219, rho_lr: 0.002000\n",
            "rho0: 0.6568, rho1: 0.2000\n",
            "Epoch 45, Loss: 0.9076, rho_lr: 0.002000\n",
            "rho0: 0.6571, rho1: 0.2000\n",
            "Epoch 46, Loss: 0.8978, rho_lr: 0.002000\n",
            "rho0: 0.6573, rho1: 0.2000\n",
            "Epoch 47, Loss: 0.9060, rho_lr: 0.002000\n",
            "rho0: 0.6576, rho1: 0.2000\n",
            "Epoch 48, Loss: 0.8984, rho_lr: 0.002000\n",
            "rho0: 0.6578, rho1: 0.2000\n",
            "Epoch 49, Loss: 0.8952, rho_lr: 0.002000\n",
            "rho0: 0.6581, rho1: 0.2000\n",
            "Epoch 50, Loss: 0.8958, rho_lr: 0.002000\n",
            "rho0: 0.6583, rho1: 0.2000\n",
            "Epoch 51, Loss: 0.8810, rho_lr: 0.002000\n",
            "rho0: 0.6586, rho1: 0.2000\n",
            "Epoch 52, Loss: 0.8810, rho_lr: 0.002000\n",
            "rho0: 0.6589, rho1: 0.2000\n",
            "Epoch 53, Loss: 0.8715, rho_lr: 0.002000\n",
            "rho0: 0.6591, rho1: 0.2000\n",
            "Epoch 54, Loss: 0.8700, rho_lr: 0.002000\n",
            "rho0: 0.6594, rho1: 0.2000\n",
            "Epoch 55, Loss: 0.8735, rho_lr: 0.002000\n",
            "rho0: 0.6597, rho1: 0.2000\n",
            "Epoch 56, Loss: 0.8614, rho_lr: 0.002000\n",
            "rho0: 0.6600, rho1: 0.2000\n",
            "Epoch 57, Loss: 0.8607, rho_lr: 0.002000\n",
            "rho0: 0.6603, rho1: 0.2000\n",
            "Epoch 58, Loss: 0.8486, rho_lr: 0.002000\n",
            "rho0: 0.6606, rho1: 0.2000\n",
            "Epoch 59, Loss: 0.8403, rho_lr: 0.002000\n",
            "rho0: 0.6609, rho1: 0.2000\n",
            "Epoch 60, Loss: 0.8406, rho_lr: 0.000200\n",
            "rho0: 0.6612, rho1: 0.2000\n",
            "Epoch 1, Loss: 34.8224, rho_lr: 0.200000\n",
            "rho0: 0.0046, rho1: 0.5000\n",
            "Epoch 2, Loss: 12.6716, rho_lr: 0.200000\n",
            "rho0: 0.2752, rho1: 0.5000\n",
            "Epoch 3, Loss: 8.4037, rho_lr: 0.200000\n",
            "rho0: 0.2492, rho1: 0.5000\n",
            "Epoch 4, Loss: 5.2199, rho_lr: 0.200000\n",
            "rho0: 0.2419, rho1: 0.5000\n",
            "Epoch 5, Loss: 3.6242, rho_lr: 0.200000\n",
            "rho0: 0.3279, rho1: 0.5000\n",
            "Epoch 6, Loss: 2.8351, rho_lr: 0.200000\n",
            "rho0: 0.3170, rho1: 0.5000\n",
            "Epoch 7, Loss: 2.4802, rho_lr: 0.200000\n",
            "rho0: 0.3202, rho1: 0.5000\n",
            "Epoch 8, Loss: 2.1573, rho_lr: 0.200000\n",
            "rho0: 0.3238, rho1: 0.5000\n",
            "Epoch 9, Loss: 2.0350, rho_lr: 0.200000\n",
            "rho0: 0.3255, rho1: 0.5000\n",
            "Epoch 10, Loss: 1.9931, rho_lr: 0.200000\n",
            "rho0: 0.3274, rho1: 0.5000\n",
            "Epoch 11, Loss: 2.1550, rho_lr: 0.200000\n",
            "rho0: 0.3714, rho1: 0.3404\n",
            "Epoch 12, Loss: 1.5899, rho_lr: 0.200000\n",
            "rho0: 0.4865, rho1: 0.2771\n",
            "Epoch 13, Loss: 1.1461, rho_lr: 0.200000\n",
            "rho0: 0.5613, rho1: 0.2000\n",
            "Epoch 14, Loss: 1.0613, rho_lr: 0.200000\n",
            "rho0: 0.5601, rho1: 0.2000\n",
            "Epoch 15, Loss: 1.0255, rho_lr: 0.200000\n",
            "rho0: 0.5604, rho1: 0.2000\n",
            "Epoch 16, Loss: 1.0087, rho_lr: 0.200000\n",
            "rho0: 0.5714, rho1: 0.2000\n",
            "Epoch 17, Loss: 0.9834, rho_lr: 0.200000\n",
            "rho0: 0.5774, rho1: 0.2000\n",
            "Epoch 18, Loss: 0.9742, rho_lr: 0.200000\n",
            "rho0: 0.5776, rho1: 0.2000\n",
            "Epoch 19, Loss: 0.9594, rho_lr: 0.200000\n",
            "rho0: 0.5854, rho1: 0.2000\n",
            "Epoch 20, Loss: 0.9405, rho_lr: 0.020000\n",
            "rho0: 0.5881, rho1: 0.2000\n",
            "Epoch 21, Loss: 0.9277, rho_lr: 0.020000\n",
            "rho0: 0.5888, rho1: 0.2000\n",
            "Epoch 22, Loss: 0.9224, rho_lr: 0.020000\n",
            "rho0: 0.5910, rho1: 0.2000\n",
            "Epoch 23, Loss: 0.9087, rho_lr: 0.020000\n",
            "rho0: 0.5945, rho1: 0.2000\n",
            "Epoch 24, Loss: 0.9015, rho_lr: 0.020000\n",
            "rho0: 0.5984, rho1: 0.2000\n",
            "Epoch 25, Loss: 0.8939, rho_lr: 0.020000\n",
            "rho0: 0.6013, rho1: 0.2000\n",
            "Epoch 26, Loss: 0.8853, rho_lr: 0.020000\n",
            "rho0: 0.6040, rho1: 0.2000\n",
            "Epoch 27, Loss: 0.8670, rho_lr: 0.020000\n",
            "rho0: 0.6069, rho1: 0.2000\n",
            "Epoch 28, Loss: 0.8618, rho_lr: 0.020000\n",
            "rho0: 0.6102, rho1: 0.2000\n",
            "Epoch 29, Loss: 0.8588, rho_lr: 0.020000\n",
            "rho0: 0.6131, rho1: 0.2000\n",
            "Epoch 30, Loss: 0.8394, rho_lr: 0.020000\n",
            "rho0: 0.6160, rho1: 0.2000\n",
            "Epoch 31, Loss: 0.8354, rho_lr: 0.020000\n",
            "rho0: 0.6198, rho1: 0.2000\n",
            "Epoch 32, Loss: 0.8284, rho_lr: 0.020000\n",
            "rho0: 0.6233, rho1: 0.2000\n",
            "Epoch 33, Loss: 0.8194, rho_lr: 0.020000\n",
            "rho0: 0.6264, rho1: 0.2000\n",
            "Epoch 34, Loss: 0.8109, rho_lr: 0.020000\n",
            "rho0: 0.6293, rho1: 0.2000\n",
            "Epoch 35, Loss: 0.7980, rho_lr: 0.020000\n",
            "rho0: 0.6326, rho1: 0.2000\n",
            "Epoch 36, Loss: 0.7937, rho_lr: 0.020000\n",
            "rho0: 0.6357, rho1: 0.2000\n",
            "Epoch 37, Loss: 0.7896, rho_lr: 0.020000\n",
            "rho0: 0.6381, rho1: 0.2000\n",
            "Epoch 38, Loss: 0.7764, rho_lr: 0.020000\n",
            "rho0: 0.6415, rho1: 0.2000\n",
            "Epoch 39, Loss: 0.7688, rho_lr: 0.020000\n",
            "rho0: 0.6452, rho1: 0.2000\n",
            "Epoch 40, Loss: 0.7638, rho_lr: 0.002000\n",
            "rho0: 0.6478, rho1: 0.2000\n",
            "Epoch 41, Loss: 0.7635, rho_lr: 0.002000\n",
            "rho0: 0.6481, rho1: 0.2000\n",
            "Epoch 42, Loss: 0.7511, rho_lr: 0.002000\n",
            "rho0: 0.6485, rho1: 0.2000\n",
            "Epoch 43, Loss: 0.7468, rho_lr: 0.002000\n",
            "rho0: 0.6490, rho1: 0.2000\n",
            "Epoch 44, Loss: 0.7474, rho_lr: 0.002000\n",
            "rho0: 0.6495, rho1: 0.2000\n",
            "Epoch 45, Loss: 0.7427, rho_lr: 0.002000\n",
            "rho0: 0.6502, rho1: 0.2000\n",
            "Epoch 46, Loss: 0.7333, rho_lr: 0.002000\n",
            "rho0: 0.6508, rho1: 0.2000\n",
            "Epoch 47, Loss: 0.7312, rho_lr: 0.002000\n",
            "rho0: 0.6514, rho1: 0.2000\n",
            "Epoch 48, Loss: 0.7258, rho_lr: 0.002000\n",
            "rho0: 0.6519, rho1: 0.2000\n",
            "Epoch 49, Loss: 0.7258, rho_lr: 0.002000\n",
            "rho0: 0.6526, rho1: 0.2000\n",
            "Epoch 50, Loss: 0.7184, rho_lr: 0.002000\n",
            "rho0: 0.6533, rho1: 0.2000\n",
            "Epoch 51, Loss: 0.7169, rho_lr: 0.002000\n",
            "rho0: 0.6539, rho1: 0.2000\n",
            "Epoch 52, Loss: 0.7140, rho_lr: 0.002000\n",
            "rho0: 0.6546, rho1: 0.2000\n",
            "Epoch 53, Loss: 0.7079, rho_lr: 0.002000\n",
            "rho0: 0.6554, rho1: 0.2000\n",
            "Epoch 54, Loss: 0.7059, rho_lr: 0.002000\n",
            "rho0: 0.6561, rho1: 0.2000\n",
            "Epoch 55, Loss: 0.7021, rho_lr: 0.002000\n",
            "rho0: 0.6569, rho1: 0.2000\n",
            "Epoch 56, Loss: 0.6975, rho_lr: 0.002000\n",
            "rho0: 0.6576, rho1: 0.2000\n",
            "Epoch 57, Loss: 0.6978, rho_lr: 0.002000\n",
            "rho0: 0.6583, rho1: 0.2000\n",
            "Epoch 58, Loss: 0.6875, rho_lr: 0.002000\n",
            "rho0: 0.6589, rho1: 0.2000\n",
            "Epoch 59, Loss: 0.6903, rho_lr: 0.002000\n",
            "rho0: 0.6596, rho1: 0.2000\n",
            "Epoch 60, Loss: 0.6876, rho_lr: 0.000200\n",
            "rho0: 0.6603, rho1: 0.2000\n",
            "Epoch 1, Loss: 47.0418, rho_lr: 0.200000\n",
            "rho0: 0.0000, rho1: 0.5000\n",
            "Epoch 2, Loss: 13.7982, rho_lr: 0.200000\n",
            "rho0: 0.2786, rho1: 0.5000\n",
            "Epoch 3, Loss: 8.6154, rho_lr: 0.200000\n",
            "rho0: 0.2919, rho1: 0.5000\n",
            "Epoch 4, Loss: 5.6967, rho_lr: 0.200000\n",
            "rho0: 0.3309, rho1: 0.5000\n",
            "Epoch 5, Loss: 4.0536, rho_lr: 0.200000\n",
            "rho0: 0.3181, rho1: 0.5000\n",
            "Epoch 6, Loss: 3.3217, rho_lr: 0.200000\n",
            "rho0: 0.3577, rho1: 0.5000\n",
            "Epoch 7, Loss: 2.9180, rho_lr: 0.200000\n",
            "rho0: 0.3285, rho1: 0.5000\n",
            "Epoch 8, Loss: 2.5701, rho_lr: 0.200000\n",
            "rho0: 0.3360, rho1: 0.5000\n",
            "Epoch 9, Loss: 2.4947, rho_lr: 0.200000\n",
            "rho0: 0.3354, rho1: 0.5000\n",
            "Epoch 10, Loss: 2.3039, rho_lr: 0.200000\n",
            "rho0: 0.3340, rho1: 0.5000\n",
            "Epoch 11, Loss: 2.2658, rho_lr: 0.200000\n",
            "rho0: 0.3825, rho1: 0.3863\n",
            "Epoch 12, Loss: 1.6344, rho_lr: 0.200000\n",
            "rho0: 0.4963, rho1: 0.2210\n",
            "Epoch 13, Loss: 1.2577, rho_lr: 0.200000\n",
            "rho0: 0.5792, rho1: 0.2000\n",
            "Epoch 14, Loss: 1.1910, rho_lr: 0.200000\n",
            "rho0: 0.5509, rho1: 0.2000\n",
            "Epoch 15, Loss: 1.1575, rho_lr: 0.200000\n",
            "rho0: 0.5816, rho1: 0.2000\n",
            "Epoch 16, Loss: 1.1172, rho_lr: 0.200000\n",
            "rho0: 0.5733, rho1: 0.2000\n",
            "Epoch 17, Loss: 1.0996, rho_lr: 0.200000\n",
            "rho0: 0.5836, rho1: 0.2000\n",
            "Epoch 18, Loss: 1.0762, rho_lr: 0.200000\n",
            "rho0: 0.5872, rho1: 0.2000\n",
            "Epoch 19, Loss: 1.0743, rho_lr: 0.200000\n",
            "rho0: 0.5882, rho1: 0.2000\n",
            "Epoch 20, Loss: 1.0440, rho_lr: 0.020000\n",
            "rho0: 0.5980, rho1: 0.2000\n",
            "Epoch 21, Loss: 1.0358, rho_lr: 0.020000\n",
            "rho0: 0.5986, rho1: 0.2000\n",
            "Epoch 22, Loss: 1.0206, rho_lr: 0.020000\n",
            "rho0: 0.6006, rho1: 0.2000\n",
            "Epoch 23, Loss: 0.9933, rho_lr: 0.020000\n",
            "rho0: 0.6028, rho1: 0.2000\n",
            "Epoch 24, Loss: 0.9899, rho_lr: 0.020000\n",
            "rho0: 0.6053, rho1: 0.2000\n",
            "Epoch 25, Loss: 0.9876, rho_lr: 0.020000\n",
            "rho0: 0.6077, rho1: 0.2000\n",
            "Epoch 26, Loss: 0.9719, rho_lr: 0.020000\n",
            "rho0: 0.6098, rho1: 0.2000\n",
            "Epoch 27, Loss: 0.9588, rho_lr: 0.020000\n",
            "rho0: 0.6126, rho1: 0.2000\n",
            "Epoch 28, Loss: 0.9455, rho_lr: 0.020000\n",
            "rho0: 0.6153, rho1: 0.2000\n",
            "Epoch 29, Loss: 0.9439, rho_lr: 0.020000\n",
            "rho0: 0.6174, rho1: 0.2000\n",
            "Epoch 30, Loss: 0.9385, rho_lr: 0.020000\n",
            "rho0: 0.6203, rho1: 0.2000\n",
            "Epoch 31, Loss: 0.9238, rho_lr: 0.020000\n",
            "rho0: 0.6230, rho1: 0.2000\n",
            "Epoch 32, Loss: 0.9071, rho_lr: 0.020000\n",
            "rho0: 0.6261, rho1: 0.2000\n",
            "Epoch 33, Loss: 0.9046, rho_lr: 0.020000\n",
            "rho0: 0.6288, rho1: 0.2000\n",
            "Epoch 34, Loss: 0.8939, rho_lr: 0.020000\n",
            "rho0: 0.6314, rho1: 0.2000\n",
            "Epoch 35, Loss: 0.8803, rho_lr: 0.020000\n",
            "rho0: 0.6342, rho1: 0.2000\n",
            "Epoch 36, Loss: 0.8797, rho_lr: 0.020000\n",
            "rho0: 0.6367, rho1: 0.2000\n",
            "Epoch 37, Loss: 0.8632, rho_lr: 0.020000\n",
            "rho0: 0.6394, rho1: 0.2000\n",
            "Epoch 38, Loss: 0.8579, rho_lr: 0.020000\n",
            "rho0: 0.6423, rho1: 0.2000\n",
            "Epoch 39, Loss: 0.8552, rho_lr: 0.020000\n",
            "rho0: 0.6446, rho1: 0.2000\n",
            "Epoch 40, Loss: 0.8424, rho_lr: 0.002000\n",
            "rho0: 0.6476, rho1: 0.2000\n",
            "Epoch 41, Loss: 0.8393, rho_lr: 0.002000\n",
            "rho0: 0.6478, rho1: 0.2000\n",
            "Epoch 42, Loss: 0.8232, rho_lr: 0.002000\n",
            "rho0: 0.6482, rho1: 0.2000\n",
            "Epoch 43, Loss: 0.8252, rho_lr: 0.002000\n",
            "rho0: 0.6486, rho1: 0.2000\n",
            "Epoch 44, Loss: 0.8228, rho_lr: 0.002000\n",
            "rho0: 0.6491, rho1: 0.2000\n",
            "Epoch 45, Loss: 0.8120, rho_lr: 0.002000\n",
            "rho0: 0.6496, rho1: 0.2000\n",
            "Epoch 46, Loss: 0.8046, rho_lr: 0.002000\n",
            "rho0: 0.6500, rho1: 0.2000\n",
            "Epoch 47, Loss: 0.8055, rho_lr: 0.002000\n",
            "rho0: 0.6505, rho1: 0.2000\n",
            "Epoch 48, Loss: 0.8025, rho_lr: 0.002000\n",
            "rho0: 0.6511, rho1: 0.2000\n",
            "Epoch 49, Loss: 0.7936, rho_lr: 0.002000\n",
            "rho0: 0.6516, rho1: 0.2000\n",
            "Epoch 50, Loss: 0.7907, rho_lr: 0.002000\n",
            "rho0: 0.6522, rho1: 0.2000\n",
            "Epoch 51, Loss: 0.7847, rho_lr: 0.002000\n",
            "rho0: 0.6527, rho1: 0.2000\n",
            "Epoch 52, Loss: 0.7809, rho_lr: 0.002000\n",
            "rho0: 0.6532, rho1: 0.2000\n",
            "Epoch 53, Loss: 0.7777, rho_lr: 0.002000\n",
            "rho0: 0.6538, rho1: 0.2000\n",
            "Epoch 54, Loss: 0.7751, rho_lr: 0.002000\n",
            "rho0: 0.6544, rho1: 0.2000\n",
            "Epoch 55, Loss: 0.7699, rho_lr: 0.002000\n",
            "rho0: 0.6550, rho1: 0.2000\n",
            "Epoch 56, Loss: 0.7637, rho_lr: 0.002000\n",
            "rho0: 0.6556, rho1: 0.2000\n",
            "Epoch 57, Loss: 0.7582, rho_lr: 0.002000\n",
            "rho0: 0.6562, rho1: 0.2000\n",
            "Epoch 58, Loss: 0.7597, rho_lr: 0.002000\n",
            "rho0: 0.6567, rho1: 0.2000\n",
            "Epoch 59, Loss: 0.7564, rho_lr: 0.002000\n",
            "rho0: 0.6574, rho1: 0.2000\n",
            "Epoch 60, Loss: 0.7501, rho_lr: 0.000200\n",
            "rho0: 0.6581, rho1: 0.2000\n",
            "Epoch 1, Loss: 30.8178, rho_lr: 0.200000\n",
            "rho0: 0.0112, rho1: 0.5000\n",
            "Epoch 2, Loss: 11.9510, rho_lr: 0.200000\n",
            "rho0: 0.2247, rho1: 0.5000\n",
            "Epoch 3, Loss: 5.9786, rho_lr: 0.200000\n",
            "rho0: 0.2935, rho1: 0.5000\n",
            "Epoch 4, Loss: 3.6998, rho_lr: 0.200000\n",
            "rho0: 0.3051, rho1: 0.5000\n",
            "Epoch 5, Loss: 3.0235, rho_lr: 0.200000\n",
            "rho0: 0.3193, rho1: 0.5000\n",
            "Epoch 6, Loss: 2.5404, rho_lr: 0.200000\n",
            "rho0: 0.3106, rho1: 0.5000\n",
            "Epoch 7, Loss: 2.3138, rho_lr: 0.200000\n",
            "rho0: 0.3156, rho1: 0.5000\n",
            "Epoch 8, Loss: 2.1313, rho_lr: 0.200000\n",
            "rho0: 0.3244, rho1: 0.5000\n",
            "Epoch 9, Loss: 2.0125, rho_lr: 0.200000\n",
            "rho0: 0.3254, rho1: 0.5000\n",
            "Epoch 10, Loss: 1.9389, rho_lr: 0.200000\n",
            "rho0: 0.3278, rho1: 0.5000\n",
            "Epoch 11, Loss: 1.9705, rho_lr: 0.200000\n",
            "rho0: 0.4181, rho1: 0.3920\n",
            "Epoch 12, Loss: 1.4243, rho_lr: 0.200000\n",
            "rho0: 0.5390, rho1: 0.2000\n",
            "Epoch 13, Loss: 1.0448, rho_lr: 0.200000\n",
            "rho0: 0.5848, rho1: 0.2000\n",
            "Epoch 14, Loss: 1.0063, rho_lr: 0.200000\n",
            "rho0: 0.5699, rho1: 0.2000\n",
            "Epoch 15, Loss: 0.9930, rho_lr: 0.200000\n",
            "rho0: 0.5919, rho1: 0.2000\n",
            "Epoch 16, Loss: 0.9594, rho_lr: 0.200000\n",
            "rho0: 0.5905, rho1: 0.2000\n",
            "Epoch 17, Loss: 0.9273, rho_lr: 0.200000\n",
            "rho0: 0.6018, rho1: 0.2000\n",
            "Epoch 18, Loss: 0.9189, rho_lr: 0.200000\n",
            "rho0: 0.6087, rho1: 0.2000\n",
            "Epoch 19, Loss: 0.9057, rho_lr: 0.200000\n",
            "rho0: 0.6150, rho1: 0.2000\n",
            "Epoch 20, Loss: 0.8825, rho_lr: 0.020000\n",
            "rho0: 0.6219, rho1: 0.2000\n",
            "Epoch 21, Loss: 0.8738, rho_lr: 0.020000\n",
            "rho0: 0.6232, rho1: 0.2000\n",
            "Epoch 22, Loss: 0.8585, rho_lr: 0.020000\n",
            "rho0: 0.6260, rho1: 0.2000\n",
            "Epoch 23, Loss: 0.8502, rho_lr: 0.020000\n",
            "rho0: 0.6291, rho1: 0.2000\n",
            "Epoch 24, Loss: 0.8353, rho_lr: 0.020000\n",
            "rho0: 0.6325, rho1: 0.2000\n",
            "Epoch 25, Loss: 0.8262, rho_lr: 0.020000\n",
            "rho0: 0.6359, rho1: 0.2000\n",
            "Epoch 26, Loss: 0.8157, rho_lr: 0.020000\n",
            "rho0: 0.6396, rho1: 0.2000\n",
            "Epoch 27, Loss: 0.8018, rho_lr: 0.020000\n",
            "rho0: 0.6432, rho1: 0.2000\n",
            "Epoch 28, Loss: 0.7917, rho_lr: 0.020000\n",
            "rho0: 0.6470, rho1: 0.2000\n",
            "Epoch 29, Loss: 0.7874, rho_lr: 0.020000\n",
            "rho0: 0.6514, rho1: 0.2000\n",
            "Epoch 30, Loss: 0.7760, rho_lr: 0.020000\n",
            "rho0: 0.6550, rho1: 0.2000\n",
            "Epoch 31, Loss: 0.7646, rho_lr: 0.020000\n",
            "rho0: 0.6586, rho1: 0.2000\n",
            "Epoch 32, Loss: 0.7600, rho_lr: 0.020000\n",
            "rho0: 0.6625, rho1: 0.2000\n",
            "Epoch 33, Loss: 0.7435, rho_lr: 0.020000\n",
            "rho0: 0.6663, rho1: 0.2000\n",
            "Epoch 34, Loss: 0.7406, rho_lr: 0.020000\n",
            "rho0: 0.6695, rho1: 0.2000\n",
            "Epoch 35, Loss: 0.7366, rho_lr: 0.020000\n",
            "rho0: 0.6730, rho1: 0.2000\n",
            "Epoch 36, Loss: 0.7279, rho_lr: 0.020000\n",
            "rho0: 0.6766, rho1: 0.2000\n",
            "Epoch 37, Loss: 0.7139, rho_lr: 0.020000\n",
            "rho0: 0.6805, rho1: 0.2000\n",
            "Epoch 38, Loss: 0.7130, rho_lr: 0.020000\n",
            "rho0: 0.6840, rho1: 0.2000\n",
            "Epoch 39, Loss: 0.7023, rho_lr: 0.020000\n",
            "rho0: 0.6874, rho1: 0.2000\n",
            "Epoch 40, Loss: 0.6986, rho_lr: 0.002000\n",
            "rho0: 0.6915, rho1: 0.2000\n",
            "Epoch 41, Loss: 0.6936, rho_lr: 0.002000\n",
            "rho0: 0.6919, rho1: 0.2000\n",
            "Epoch 42, Loss: 0.6888, rho_lr: 0.002000\n",
            "rho0: 0.6924, rho1: 0.2000\n",
            "Epoch 43, Loss: 0.6865, rho_lr: 0.002000\n",
            "rho0: 0.6930, rho1: 0.2000\n",
            "Epoch 44, Loss: 0.6787, rho_lr: 0.002000\n",
            "rho0: 0.6936, rho1: 0.2000\n",
            "Epoch 45, Loss: 0.6740, rho_lr: 0.002000\n",
            "rho0: 0.6943, rho1: 0.2000\n",
            "Epoch 46, Loss: 0.6740, rho_lr: 0.002000\n",
            "rho0: 0.6950, rho1: 0.2000\n",
            "Epoch 47, Loss: 0.6699, rho_lr: 0.002000\n",
            "rho0: 0.6958, rho1: 0.2000\n",
            "Epoch 48, Loss: 0.6612, rho_lr: 0.002000\n",
            "rho0: 0.6965, rho1: 0.2000\n",
            "Epoch 49, Loss: 0.6568, rho_lr: 0.002000\n",
            "rho0: 0.6973, rho1: 0.2000\n",
            "Epoch 50, Loss: 0.6565, rho_lr: 0.002000\n",
            "rho0: 0.6980, rho1: 0.2000\n",
            "Epoch 51, Loss: 0.6555, rho_lr: 0.002000\n",
            "rho0: 0.6988, rho1: 0.2000\n",
            "Epoch 52, Loss: 0.6532, rho_lr: 0.002000\n",
            "rho0: 0.6995, rho1: 0.2000\n",
            "Epoch 53, Loss: 0.6461, rho_lr: 0.002000\n",
            "rho0: 0.7003, rho1: 0.2000\n",
            "Epoch 54, Loss: 0.6397, rho_lr: 0.002000\n",
            "rho0: 0.7011, rho1: 0.2000\n",
            "Epoch 55, Loss: 0.6422, rho_lr: 0.002000\n",
            "rho0: 0.7019, rho1: 0.2000\n",
            "Epoch 56, Loss: 0.6364, rho_lr: 0.002000\n",
            "rho0: 0.7027, rho1: 0.2000\n",
            "Epoch 57, Loss: 0.6387, rho_lr: 0.002000\n",
            "rho0: 0.7035, rho1: 0.2000\n",
            "Epoch 58, Loss: 0.6326, rho_lr: 0.002000\n",
            "rho0: 0.7043, rho1: 0.2000\n",
            "Epoch 59, Loss: 0.6307, rho_lr: 0.002000\n",
            "rho0: 0.7052, rho1: 0.2000\n",
            "Epoch 60, Loss: 0.6261, rho_lr: 0.000200\n",
            "rho0: 0.7060, rho1: 0.2000\n",
            "Epoch 1, Loss: 38.4867, rho_lr: 0.200000\n",
            "rho0: 0.0000, rho1: 0.5000\n",
            "Epoch 2, Loss: 16.1881, rho_lr: 0.200000\n",
            "rho0: 0.2931, rho1: 0.5000\n",
            "Epoch 3, Loss: 9.0348, rho_lr: 0.200000\n",
            "rho0: 0.2664, rho1: 0.5000\n",
            "Epoch 4, Loss: 5.0518, rho_lr: 0.200000\n",
            "rho0: 0.3179, rho1: 0.5000\n",
            "Epoch 5, Loss: 3.9968, rho_lr: 0.200000\n",
            "rho0: 0.2924, rho1: 0.5000\n",
            "Epoch 6, Loss: 3.2214, rho_lr: 0.200000\n",
            "rho0: 0.3384, rho1: 0.5000\n",
            "Epoch 7, Loss: 2.8047, rho_lr: 0.200000\n",
            "rho0: 0.3207, rho1: 0.5000\n",
            "Epoch 8, Loss: 2.4905, rho_lr: 0.200000\n",
            "rho0: 0.3192, rho1: 0.5000\n",
            "Epoch 9, Loss: 2.3016, rho_lr: 0.200000\n",
            "rho0: 0.3192, rho1: 0.5000\n",
            "Epoch 10, Loss: 2.1721, rho_lr: 0.200000\n",
            "rho0: 0.3313, rho1: 0.5000\n",
            "Epoch 11, Loss: 2.1588, rho_lr: 0.200000\n",
            "rho0: 0.4056, rho1: 0.3446\n",
            "Epoch 12, Loss: 1.5913, rho_lr: 0.200000\n",
            "rho0: 0.5306, rho1: 0.2208\n",
            "Epoch 13, Loss: 1.1847, rho_lr: 0.200000\n",
            "rho0: 0.5727, rho1: 0.2000\n",
            "Epoch 14, Loss: 1.1427, rho_lr: 0.200000\n",
            "rho0: 0.5606, rho1: 0.2000\n",
            "Epoch 15, Loss: 1.0942, rho_lr: 0.200000\n",
            "rho0: 0.5815, rho1: 0.2000\n",
            "Epoch 16, Loss: 1.0690, rho_lr: 0.200000\n",
            "rho0: 0.5832, rho1: 0.2000\n",
            "Epoch 17, Loss: 1.0654, rho_lr: 0.200000\n",
            "rho0: 0.5933, rho1: 0.2000\n",
            "Epoch 18, Loss: 1.0440, rho_lr: 0.200000\n",
            "rho0: 0.5947, rho1: 0.2000\n",
            "Epoch 19, Loss: 1.0180, rho_lr: 0.200000\n",
            "rho0: 0.6008, rho1: 0.2000\n",
            "Epoch 20, Loss: 0.9977, rho_lr: 0.020000\n",
            "rho0: 0.6092, rho1: 0.2000\n",
            "Epoch 21, Loss: 0.9946, rho_lr: 0.020000\n",
            "rho0: 0.6096, rho1: 0.2000\n",
            "Epoch 22, Loss: 0.9553, rho_lr: 0.020000\n",
            "rho0: 0.6116, rho1: 0.2000\n",
            "Epoch 23, Loss: 0.9576, rho_lr: 0.020000\n",
            "rho0: 0.6145, rho1: 0.2000\n",
            "Epoch 24, Loss: 0.9391, rho_lr: 0.020000\n",
            "rho0: 0.6180, rho1: 0.2000\n",
            "Epoch 25, Loss: 0.9298, rho_lr: 0.020000\n",
            "rho0: 0.6208, rho1: 0.2000\n",
            "Epoch 26, Loss: 0.9224, rho_lr: 0.020000\n",
            "rho0: 0.6237, rho1: 0.2000\n",
            "Epoch 27, Loss: 0.9108, rho_lr: 0.020000\n",
            "rho0: 0.6270, rho1: 0.2000\n",
            "Epoch 28, Loss: 0.9038, rho_lr: 0.020000\n",
            "rho0: 0.6304, rho1: 0.2000\n",
            "Epoch 29, Loss: 0.8912, rho_lr: 0.020000\n",
            "rho0: 0.6341, rho1: 0.2000\n",
            "Epoch 30, Loss: 0.8839, rho_lr: 0.020000\n",
            "rho0: 0.6373, rho1: 0.2000\n",
            "Epoch 31, Loss: 0.8638, rho_lr: 0.020000\n",
            "rho0: 0.6407, rho1: 0.2000\n",
            "Epoch 32, Loss: 0.8507, rho_lr: 0.020000\n",
            "rho0: 0.6445, rho1: 0.2000\n",
            "Epoch 33, Loss: 0.8455, rho_lr: 0.020000\n",
            "rho0: 0.6483, rho1: 0.2000\n",
            "Epoch 34, Loss: 0.8387, rho_lr: 0.020000\n",
            "rho0: 0.6520, rho1: 0.2000\n",
            "Epoch 35, Loss: 0.8322, rho_lr: 0.020000\n",
            "rho0: 0.6555, rho1: 0.2000\n",
            "Epoch 36, Loss: 0.8213, rho_lr: 0.020000\n",
            "rho0: 0.6581, rho1: 0.2000\n",
            "Epoch 37, Loss: 0.8148, rho_lr: 0.020000\n",
            "rho0: 0.6616, rho1: 0.2000\n",
            "Epoch 38, Loss: 0.8047, rho_lr: 0.020000\n",
            "rho0: 0.6651, rho1: 0.2000\n",
            "Epoch 39, Loss: 0.8002, rho_lr: 0.020000\n",
            "rho0: 0.6684, rho1: 0.2000\n",
            "Epoch 40, Loss: 0.7904, rho_lr: 0.002000\n",
            "rho0: 0.6719, rho1: 0.2000\n",
            "Epoch 41, Loss: 0.7881, rho_lr: 0.002000\n",
            "rho0: 0.6722, rho1: 0.2000\n",
            "Epoch 42, Loss: 0.7798, rho_lr: 0.002000\n",
            "rho0: 0.6726, rho1: 0.2000\n",
            "Epoch 43, Loss: 0.7723, rho_lr: 0.002000\n",
            "rho0: 0.6732, rho1: 0.2000\n",
            "Epoch 44, Loss: 0.7635, rho_lr: 0.002000\n",
            "rho0: 0.6736, rho1: 0.2000\n",
            "Epoch 45, Loss: 0.7612, rho_lr: 0.002000\n",
            "rho0: 0.6742, rho1: 0.2000\n",
            "Epoch 46, Loss: 0.7498, rho_lr: 0.002000\n",
            "rho0: 0.6748, rho1: 0.2000\n",
            "Epoch 47, Loss: 0.7556, rho_lr: 0.002000\n",
            "rho0: 0.6754, rho1: 0.2000\n",
            "Epoch 48, Loss: 0.7486, rho_lr: 0.002000\n",
            "rho0: 0.6760, rho1: 0.2000\n",
            "Epoch 49, Loss: 0.7470, rho_lr: 0.002000\n",
            "rho0: 0.6766, rho1: 0.2000\n",
            "Epoch 50, Loss: 0.7389, rho_lr: 0.002000\n",
            "rho0: 0.6773, rho1: 0.2000\n",
            "Epoch 51, Loss: 0.7343, rho_lr: 0.002000\n",
            "rho0: 0.6779, rho1: 0.2000\n",
            "Epoch 52, Loss: 0.7266, rho_lr: 0.002000\n",
            "rho0: 0.6785, rho1: 0.2000\n",
            "Epoch 53, Loss: 0.7241, rho_lr: 0.002000\n",
            "rho0: 0.6791, rho1: 0.2000\n",
            "Epoch 54, Loss: 0.7249, rho_lr: 0.002000\n",
            "rho0: 0.6798, rho1: 0.2000\n",
            "Epoch 55, Loss: 0.7187, rho_lr: 0.002000\n",
            "rho0: 0.6805, rho1: 0.2000\n",
            "Epoch 56, Loss: 0.7157, rho_lr: 0.002000\n",
            "rho0: 0.6812, rho1: 0.2000\n",
            "Epoch 57, Loss: 0.7115, rho_lr: 0.002000\n",
            "rho0: 0.6818, rho1: 0.2000\n",
            "Epoch 58, Loss: 0.7058, rho_lr: 0.002000\n",
            "rho0: 0.6825, rho1: 0.2000\n",
            "Epoch 59, Loss: 0.7033, rho_lr: 0.002000\n",
            "rho0: 0.6832, rho1: 0.2000\n",
            "Epoch 60, Loss: 0.6959, rho_lr: 0.000200\n",
            "rho0: 0.6839, rho1: 0.2000\n",
            "Epoch 1, Loss: 40.2968, rho_lr: 0.200000\n",
            "rho0: 0.0000, rho1: 0.5000\n",
            "Epoch 2, Loss: 15.4467, rho_lr: 0.200000\n",
            "rho0: 0.2969, rho1: 0.5000\n",
            "Epoch 3, Loss: 8.4757, rho_lr: 0.200000\n",
            "rho0: 0.3530, rho1: 0.5000\n",
            "Epoch 4, Loss: 5.2279, rho_lr: 0.200000\n",
            "rho0: 0.3414, rho1: 0.5000\n",
            "Epoch 5, Loss: 3.7988, rho_lr: 0.200000\n",
            "rho0: 0.3348, rho1: 0.5000\n",
            "Epoch 6, Loss: 3.1460, rho_lr: 0.200000\n",
            "rho0: 0.3404, rho1: 0.5000\n",
            "Epoch 7, Loss: 2.7697, rho_lr: 0.200000\n",
            "rho0: 0.3302, rho1: 0.5000\n",
            "Epoch 8, Loss: 2.5477, rho_lr: 0.200000\n",
            "rho0: 0.3541, rho1: 0.5000\n",
            "Epoch 9, Loss: 2.3190, rho_lr: 0.200000\n",
            "rho0: 0.3533, rho1: 0.5000\n",
            "Epoch 10, Loss: 2.1483, rho_lr: 0.200000\n",
            "rho0: 0.3491, rho1: 0.5000\n",
            "Epoch 11, Loss: 2.1637, rho_lr: 0.200000\n",
            "rho0: 0.4149, rho1: 0.3592\n",
            "Epoch 12, Loss: 1.4950, rho_lr: 0.200000\n",
            "rho0: 0.5514, rho1: 0.2306\n",
            "Epoch 13, Loss: 1.1370, rho_lr: 0.200000\n",
            "rho0: 0.6058, rho1: 0.2000\n",
            "Epoch 14, Loss: 1.0758, rho_lr: 0.200000\n",
            "rho0: 0.5908, rho1: 0.2000\n",
            "Epoch 15, Loss: 1.0410, rho_lr: 0.200000\n",
            "rho0: 0.6224, rho1: 0.2000\n",
            "Epoch 16, Loss: 1.0237, rho_lr: 0.200000\n",
            "rho0: 0.6168, rho1: 0.2000\n",
            "Epoch 17, Loss: 0.9957, rho_lr: 0.200000\n",
            "rho0: 0.6348, rho1: 0.2000\n",
            "Epoch 18, Loss: 0.9702, rho_lr: 0.200000\n",
            "rho0: 0.6340, rho1: 0.2000\n",
            "Epoch 19, Loss: 0.9448, rho_lr: 0.200000\n",
            "rho0: 0.6492, rho1: 0.2000\n",
            "Epoch 20, Loss: 0.9448, rho_lr: 0.020000\n",
            "rho0: 0.6524, rho1: 0.2000\n",
            "Epoch 21, Loss: 0.9124, rho_lr: 0.020000\n",
            "rho0: 0.6542, rho1: 0.2000\n",
            "Epoch 22, Loss: 0.9131, rho_lr: 0.020000\n",
            "rho0: 0.6561, rho1: 0.2000\n",
            "Epoch 23, Loss: 0.8925, rho_lr: 0.020000\n",
            "rho0: 0.6592, rho1: 0.2000\n",
            "Epoch 24, Loss: 0.8838, rho_lr: 0.020000\n",
            "rho0: 0.6625, rho1: 0.2000\n",
            "Epoch 25, Loss: 0.8745, rho_lr: 0.020000\n",
            "rho0: 0.6654, rho1: 0.2000\n",
            "Epoch 26, Loss: 0.8550, rho_lr: 0.020000\n",
            "rho0: 0.6691, rho1: 0.2000\n",
            "Epoch 27, Loss: 0.8502, rho_lr: 0.020000\n",
            "rho0: 0.6721, rho1: 0.2000\n",
            "Epoch 28, Loss: 0.8332, rho_lr: 0.020000\n",
            "rho0: 0.6755, rho1: 0.2000\n",
            "Epoch 29, Loss: 0.8289, rho_lr: 0.020000\n",
            "rho0: 0.6789, rho1: 0.2000\n",
            "Epoch 30, Loss: 0.8167, rho_lr: 0.020000\n",
            "rho0: 0.6821, rho1: 0.2000\n",
            "Epoch 31, Loss: 0.8067, rho_lr: 0.020000\n",
            "rho0: 0.6852, rho1: 0.2000\n",
            "Epoch 32, Loss: 0.7962, rho_lr: 0.020000\n",
            "rho0: 0.6884, rho1: 0.2000\n",
            "Epoch 33, Loss: 0.7861, rho_lr: 0.020000\n",
            "rho0: 0.6918, rho1: 0.2000\n",
            "Epoch 34, Loss: 0.7834, rho_lr: 0.020000\n",
            "rho0: 0.6952, rho1: 0.2000\n",
            "Epoch 35, Loss: 0.7649, rho_lr: 0.020000\n",
            "rho0: 0.6986, rho1: 0.2000\n",
            "Epoch 36, Loss: 0.7583, rho_lr: 0.020000\n",
            "rho0: 0.7021, rho1: 0.2000\n",
            "Epoch 37, Loss: 0.7511, rho_lr: 0.020000\n",
            "rho0: 0.7055, rho1: 0.2000\n",
            "Epoch 38, Loss: 0.7481, rho_lr: 0.020000\n",
            "rho0: 0.7085, rho1: 0.2000\n",
            "Epoch 39, Loss: 0.7346, rho_lr: 0.020000\n",
            "rho0: 0.7115, rho1: 0.2000\n",
            "Epoch 40, Loss: 0.7281, rho_lr: 0.002000\n",
            "rho0: 0.7148, rho1: 0.2000\n",
            "Epoch 41, Loss: 0.7216, rho_lr: 0.002000\n",
            "rho0: 0.7152, rho1: 0.2000\n",
            "Epoch 42, Loss: 0.7096, rho_lr: 0.002000\n",
            "rho0: 0.7156, rho1: 0.2000\n",
            "Epoch 43, Loss: 0.7118, rho_lr: 0.002000\n",
            "rho0: 0.7161, rho1: 0.2000\n",
            "Epoch 44, Loss: 0.6990, rho_lr: 0.002000\n",
            "rho0: 0.7166, rho1: 0.2000\n",
            "Epoch 45, Loss: 0.7007, rho_lr: 0.002000\n",
            "rho0: 0.7171, rho1: 0.2000\n",
            "Epoch 46, Loss: 0.6951, rho_lr: 0.002000\n",
            "rho0: 0.7176, rho1: 0.2000\n",
            "Epoch 47, Loss: 0.6838, rho_lr: 0.002000\n",
            "rho0: 0.7182, rho1: 0.2000\n",
            "Epoch 48, Loss: 0.6768, rho_lr: 0.002000\n",
            "rho0: 0.7188, rho1: 0.2000\n",
            "Epoch 49, Loss: 0.6778, rho_lr: 0.002000\n",
            "rho0: 0.7194, rho1: 0.2000\n",
            "Epoch 50, Loss: 0.6734, rho_lr: 0.002000\n",
            "rho0: 0.7200, rho1: 0.2000\n",
            "Epoch 51, Loss: 0.6668, rho_lr: 0.002000\n",
            "rho0: 0.7206, rho1: 0.2000\n",
            "Epoch 52, Loss: 0.6656, rho_lr: 0.002000\n",
            "rho0: 0.7211, rho1: 0.2000\n",
            "Epoch 53, Loss: 0.6645, rho_lr: 0.002000\n",
            "rho0: 0.7218, rho1: 0.2000\n",
            "Epoch 54, Loss: 0.6572, rho_lr: 0.002000\n",
            "rho0: 0.7225, rho1: 0.2000\n",
            "Epoch 55, Loss: 0.6534, rho_lr: 0.002000\n",
            "rho0: 0.7232, rho1: 0.2000\n",
            "Epoch 56, Loss: 0.6531, rho_lr: 0.002000\n",
            "rho0: 0.7238, rho1: 0.2000\n",
            "Epoch 57, Loss: 0.6489, rho_lr: 0.002000\n",
            "rho0: 0.7245, rho1: 0.2000\n",
            "Epoch 58, Loss: 0.6440, rho_lr: 0.002000\n",
            "rho0: 0.7252, rho1: 0.2000\n",
            "Epoch 59, Loss: 0.6411, rho_lr: 0.002000\n",
            "rho0: 0.7258, rho1: 0.2000\n",
            "Epoch 60, Loss: 0.6373, rho_lr: 0.000200\n",
            "rho0: 0.7265, rho1: 0.2000\n",
            "Epoch 1, Loss: 43.8285, rho_lr: 0.200000\n",
            "rho0: 0.0000, rho1: 0.5000\n",
            "Epoch 2, Loss: 14.8535, rho_lr: 0.200000\n",
            "rho0: 0.2253, rho1: 0.5000\n",
            "Epoch 3, Loss: 10.3236, rho_lr: 0.200000\n",
            "rho0: 0.2399, rho1: 0.5000\n",
            "Epoch 4, Loss: 6.7580, rho_lr: 0.200000\n",
            "rho0: 0.3085, rho1: 0.5000\n",
            "Epoch 5, Loss: 4.8439, rho_lr: 0.200000\n",
            "rho0: 0.3000, rho1: 0.5000\n",
            "Epoch 6, Loss: 3.8141, rho_lr: 0.200000\n",
            "rho0: 0.3023, rho1: 0.5000\n",
            "Epoch 7, Loss: 3.2197, rho_lr: 0.200000\n",
            "rho0: 0.3181, rho1: 0.5000\n",
            "Epoch 8, Loss: 2.7900, rho_lr: 0.200000\n",
            "rho0: 0.3202, rho1: 0.5000\n",
            "Epoch 9, Loss: 2.6106, rho_lr: 0.200000\n",
            "rho0: 0.3247, rho1: 0.5000\n",
            "Epoch 10, Loss: 2.4160, rho_lr: 0.200000\n",
            "rho0: 0.3286, rho1: 0.5000\n",
            "Epoch 11, Loss: 2.5809, rho_lr: 0.200000\n",
            "rho0: 0.3751, rho1: 0.3679\n",
            "Epoch 12, Loss: 1.7564, rho_lr: 0.200000\n",
            "rho0: 0.4992, rho1: 0.2642\n",
            "Epoch 13, Loss: 1.2698, rho_lr: 0.200000\n",
            "rho0: 0.5864, rho1: 0.2000\n",
            "Epoch 14, Loss: 1.1622, rho_lr: 0.200000\n",
            "rho0: 0.5748, rho1: 0.2000\n",
            "Epoch 15, Loss: 1.0963, rho_lr: 0.200000\n",
            "rho0: 0.5939, rho1: 0.2000\n",
            "Epoch 16, Loss: 1.0671, rho_lr: 0.200000\n",
            "rho0: 0.6092, rho1: 0.2000\n",
            "Epoch 17, Loss: 1.0462, rho_lr: 0.200000\n",
            "rho0: 0.6128, rho1: 0.2000\n",
            "Epoch 18, Loss: 1.0309, rho_lr: 0.200000\n",
            "rho0: 0.6239, rho1: 0.2000\n",
            "Epoch 19, Loss: 0.9865, rho_lr: 0.200000\n",
            "rho0: 0.6306, rho1: 0.2000\n",
            "Epoch 20, Loss: 0.9761, rho_lr: 0.020000\n",
            "rho0: 0.6398, rho1: 0.2000\n",
            "Epoch 21, Loss: 0.9481, rho_lr: 0.020000\n",
            "rho0: 0.6407, rho1: 0.2000\n",
            "Epoch 22, Loss: 0.9433, rho_lr: 0.020000\n",
            "rho0: 0.6428, rho1: 0.2000\n",
            "Epoch 23, Loss: 0.9281, rho_lr: 0.020000\n",
            "rho0: 0.6455, rho1: 0.2000\n",
            "Epoch 24, Loss: 0.9017, rho_lr: 0.020000\n",
            "rho0: 0.6486, rho1: 0.2000\n",
            "Epoch 25, Loss: 0.9030, rho_lr: 0.020000\n",
            "rho0: 0.6518, rho1: 0.2000\n",
            "Epoch 26, Loss: 0.8859, rho_lr: 0.020000\n",
            "rho0: 0.6550, rho1: 0.2000\n",
            "Epoch 27, Loss: 0.8681, rho_lr: 0.020000\n",
            "rho0: 0.6578, rho1: 0.2000\n",
            "Epoch 28, Loss: 0.8585, rho_lr: 0.020000\n",
            "rho0: 0.6611, rho1: 0.2000\n",
            "Epoch 29, Loss: 0.8546, rho_lr: 0.020000\n",
            "rho0: 0.6640, rho1: 0.2000\n",
            "Epoch 30, Loss: 0.8402, rho_lr: 0.020000\n",
            "rho0: 0.6670, rho1: 0.2000\n",
            "Epoch 31, Loss: 0.8319, rho_lr: 0.020000\n",
            "rho0: 0.6702, rho1: 0.2000\n",
            "Epoch 32, Loss: 0.8160, rho_lr: 0.020000\n",
            "rho0: 0.6732, rho1: 0.2000\n",
            "Epoch 33, Loss: 0.8146, rho_lr: 0.020000\n",
            "rho0: 0.6763, rho1: 0.2000\n",
            "Epoch 34, Loss: 0.8048, rho_lr: 0.020000\n",
            "rho0: 0.6793, rho1: 0.2000\n",
            "Epoch 35, Loss: 0.7914, rho_lr: 0.020000\n",
            "rho0: 0.6824, rho1: 0.2000\n",
            "Epoch 36, Loss: 0.7903, rho_lr: 0.020000\n",
            "rho0: 0.6856, rho1: 0.2000\n",
            "Epoch 37, Loss: 0.7719, rho_lr: 0.020000\n",
            "rho0: 0.6888, rho1: 0.2000\n",
            "Epoch 38, Loss: 0.7666, rho_lr: 0.020000\n",
            "rho0: 0.6914, rho1: 0.2000\n",
            "Epoch 39, Loss: 0.7629, rho_lr: 0.020000\n",
            "rho0: 0.6940, rho1: 0.2000\n",
            "Epoch 40, Loss: 0.7604, rho_lr: 0.002000\n",
            "rho0: 0.6969, rho1: 0.2000\n",
            "Epoch 41, Loss: 0.7481, rho_lr: 0.002000\n",
            "rho0: 0.6972, rho1: 0.2000\n",
            "Epoch 42, Loss: 0.7430, rho_lr: 0.002000\n",
            "rho0: 0.6976, rho1: 0.2000\n",
            "Epoch 43, Loss: 0.7333, rho_lr: 0.002000\n",
            "rho0: 0.6980, rho1: 0.2000\n",
            "Epoch 44, Loss: 0.7288, rho_lr: 0.002000\n",
            "rho0: 0.6985, rho1: 0.2000\n",
            "Epoch 45, Loss: 0.7249, rho_lr: 0.002000\n",
            "rho0: 0.6989, rho1: 0.2000\n",
            "Epoch 46, Loss: 0.7210, rho_lr: 0.002000\n",
            "rho0: 0.6994, rho1: 0.2000\n",
            "Epoch 47, Loss: 0.7158, rho_lr: 0.002000\n",
            "rho0: 0.6998, rho1: 0.2000\n",
            "Epoch 48, Loss: 0.7090, rho_lr: 0.002000\n",
            "rho0: 0.7003, rho1: 0.2000\n",
            "Epoch 49, Loss: 0.7090, rho_lr: 0.002000\n",
            "rho0: 0.7008, rho1: 0.2000\n",
            "Epoch 50, Loss: 0.6987, rho_lr: 0.002000\n",
            "rho0: 0.7014, rho1: 0.2000\n",
            "Epoch 51, Loss: 0.6984, rho_lr: 0.002000\n",
            "rho0: 0.7018, rho1: 0.2000\n",
            "Epoch 52, Loss: 0.6912, rho_lr: 0.002000\n",
            "rho0: 0.7023, rho1: 0.2000\n",
            "Epoch 53, Loss: 0.6937, rho_lr: 0.002000\n",
            "rho0: 0.7029, rho1: 0.2000\n",
            "Epoch 54, Loss: 0.6845, rho_lr: 0.002000\n",
            "rho0: 0.7035, rho1: 0.2000\n",
            "Epoch 55, Loss: 0.6864, rho_lr: 0.002000\n",
            "rho0: 0.7040, rho1: 0.2000\n",
            "Epoch 56, Loss: 0.6792, rho_lr: 0.002000\n",
            "rho0: 0.7045, rho1: 0.2000\n",
            "Epoch 57, Loss: 0.6749, rho_lr: 0.002000\n",
            "rho0: 0.7051, rho1: 0.2000\n",
            "Epoch 58, Loss: 0.6705, rho_lr: 0.002000\n",
            "rho0: 0.7057, rho1: 0.2000\n",
            "Epoch 59, Loss: 0.6666, rho_lr: 0.002000\n",
            "rho0: 0.7063, rho1: 0.2000\n",
            "Epoch 60, Loss: 0.6633, rho_lr: 0.000200\n",
            "rho0: 0.7068, rho1: 0.2000\n",
            "Epoch 1, Loss: 40.1809, rho_lr: 0.200000\n",
            "rho0: 0.0000, rho1: 0.5000\n",
            "Epoch 2, Loss: 14.0399, rho_lr: 0.200000\n",
            "rho0: 0.2315, rho1: 0.5000\n",
            "Epoch 3, Loss: 9.7170, rho_lr: 0.200000\n",
            "rho0: 0.2115, rho1: 0.5000\n",
            "Epoch 4, Loss: 6.6255, rho_lr: 0.200000\n",
            "rho0: 0.2908, rho1: 0.5000\n",
            "Epoch 5, Loss: 4.4076, rho_lr: 0.200000\n",
            "rho0: 0.3345, rho1: 0.5000\n",
            "Epoch 6, Loss: 3.4716, rho_lr: 0.200000\n",
            "rho0: 0.3392, rho1: 0.5000\n",
            "Epoch 7, Loss: 2.7822, rho_lr: 0.200000\n",
            "rho0: 0.3426, rho1: 0.5000\n",
            "Epoch 8, Loss: 2.4727, rho_lr: 0.200000\n",
            "rho0: 0.3443, rho1: 0.5000\n",
            "Epoch 9, Loss: 2.3271, rho_lr: 0.200000\n",
            "rho0: 0.3489, rho1: 0.5000\n",
            "Epoch 10, Loss: 2.1053, rho_lr: 0.200000\n",
            "rho0: 0.3443, rho1: 0.5000\n",
            "Epoch 11, Loss: 2.2280, rho_lr: 0.200000\n",
            "rho0: 0.4019, rho1: 0.3743\n",
            "Epoch 12, Loss: 1.6117, rho_lr: 0.200000\n",
            "rho0: 0.5275, rho1: 0.2619\n",
            "Epoch 13, Loss: 1.1435, rho_lr: 0.200000\n",
            "rho0: 0.6008, rho1: 0.2000\n",
            "Epoch 14, Loss: 1.0778, rho_lr: 0.200000\n",
            "rho0: 0.5839, rho1: 0.2000\n",
            "Epoch 15, Loss: 1.0455, rho_lr: 0.200000\n",
            "rho0: 0.5992, rho1: 0.2000\n",
            "Epoch 16, Loss: 1.0157, rho_lr: 0.200000\n",
            "rho0: 0.6062, rho1: 0.2000\n",
            "Epoch 17, Loss: 1.0002, rho_lr: 0.200000\n",
            "rho0: 0.6145, rho1: 0.2000\n",
            "Epoch 18, Loss: 0.9842, rho_lr: 0.200000\n",
            "rho0: 0.6174, rho1: 0.2000\n",
            "Epoch 19, Loss: 0.9610, rho_lr: 0.200000\n",
            "rho0: 0.6262, rho1: 0.2000\n",
            "Epoch 20, Loss: 0.9534, rho_lr: 0.020000\n",
            "rho0: 0.6329, rho1: 0.2000\n",
            "Epoch 21, Loss: 0.9352, rho_lr: 0.020000\n",
            "rho0: 0.6332, rho1: 0.2000\n",
            "Epoch 22, Loss: 0.9319, rho_lr: 0.020000\n",
            "rho0: 0.6347, rho1: 0.2000\n",
            "Epoch 23, Loss: 0.9085, rho_lr: 0.020000\n",
            "rho0: 0.6375, rho1: 0.2000\n",
            "Epoch 24, Loss: 0.8979, rho_lr: 0.020000\n",
            "rho0: 0.6406, rho1: 0.2000\n",
            "Epoch 25, Loss: 0.8923, rho_lr: 0.020000\n",
            "rho0: 0.6426, rho1: 0.2000\n",
            "Epoch 26, Loss: 0.8820, rho_lr: 0.020000\n",
            "rho0: 0.6447, rho1: 0.2000\n",
            "Epoch 27, Loss: 0.8773, rho_lr: 0.020000\n",
            "rho0: 0.6475, rho1: 0.2000\n",
            "Epoch 28, Loss: 0.8639, rho_lr: 0.020000\n",
            "rho0: 0.6501, rho1: 0.2000\n",
            "Epoch 29, Loss: 0.8529, rho_lr: 0.020000\n",
            "rho0: 0.6534, rho1: 0.2000\n",
            "Epoch 30, Loss: 0.8536, rho_lr: 0.020000\n",
            "rho0: 0.6565, rho1: 0.2000\n",
            "Epoch 31, Loss: 0.8404, rho_lr: 0.020000\n",
            "rho0: 0.6588, rho1: 0.2000\n",
            "Epoch 32, Loss: 0.8244, rho_lr: 0.020000\n",
            "rho0: 0.6614, rho1: 0.2000\n",
            "Epoch 33, Loss: 0.8302, rho_lr: 0.020000\n",
            "rho0: 0.6643, rho1: 0.2000\n",
            "Epoch 34, Loss: 0.8180, rho_lr: 0.020000\n",
            "rho0: 0.6665, rho1: 0.2000\n",
            "Epoch 35, Loss: 0.8070, rho_lr: 0.020000\n",
            "rho0: 0.6694, rho1: 0.2000\n",
            "Epoch 36, Loss: 0.8037, rho_lr: 0.020000\n",
            "rho0: 0.6720, rho1: 0.2000\n",
            "Epoch 37, Loss: 0.7919, rho_lr: 0.020000\n",
            "rho0: 0.6742, rho1: 0.2000\n",
            "Epoch 38, Loss: 0.7867, rho_lr: 0.020000\n",
            "rho0: 0.6772, rho1: 0.2000\n",
            "Epoch 39, Loss: 0.7803, rho_lr: 0.020000\n",
            "rho0: 0.6798, rho1: 0.2000\n",
            "Epoch 40, Loss: 0.7802, rho_lr: 0.002000\n",
            "rho0: 0.6823, rho1: 0.2000\n",
            "Epoch 41, Loss: 0.7625, rho_lr: 0.002000\n",
            "rho0: 0.6826, rho1: 0.2000\n",
            "Epoch 42, Loss: 0.7646, rho_lr: 0.002000\n",
            "rho0: 0.6830, rho1: 0.2000\n",
            "Epoch 43, Loss: 0.7550, rho_lr: 0.002000\n",
            "rho0: 0.6834, rho1: 0.2000\n",
            "Epoch 44, Loss: 0.7548, rho_lr: 0.002000\n",
            "rho0: 0.6838, rho1: 0.2000\n",
            "Epoch 45, Loss: 0.7499, rho_lr: 0.002000\n",
            "rho0: 0.6842, rho1: 0.2000\n",
            "Epoch 46, Loss: 0.7494, rho_lr: 0.002000\n",
            "rho0: 0.6847, rho1: 0.2000\n",
            "Epoch 47, Loss: 0.7424, rho_lr: 0.002000\n",
            "rho0: 0.6853, rho1: 0.2000\n",
            "Epoch 48, Loss: 0.7356, rho_lr: 0.002000\n",
            "rho0: 0.6858, rho1: 0.2000\n",
            "Epoch 49, Loss: 0.7332, rho_lr: 0.002000\n",
            "rho0: 0.6863, rho1: 0.2000\n",
            "Epoch 50, Loss: 0.7342, rho_lr: 0.002000\n",
            "rho0: 0.6868, rho1: 0.2000\n",
            "Epoch 51, Loss: 0.7267, rho_lr: 0.002000\n",
            "rho0: 0.6873, rho1: 0.2000\n",
            "Epoch 52, Loss: 0.7225, rho_lr: 0.002000\n",
            "rho0: 0.6880, rho1: 0.2000\n",
            "Epoch 53, Loss: 0.7168, rho_lr: 0.002000\n",
            "rho0: 0.6886, rho1: 0.2000\n",
            "Epoch 54, Loss: 0.7113, rho_lr: 0.002000\n",
            "rho0: 0.6892, rho1: 0.2000\n",
            "Epoch 55, Loss: 0.7083, rho_lr: 0.002000\n",
            "rho0: 0.6897, rho1: 0.2000\n",
            "Epoch 56, Loss: 0.7050, rho_lr: 0.002000\n",
            "rho0: 0.6903, rho1: 0.2000\n",
            "Epoch 57, Loss: 0.7007, rho_lr: 0.002000\n",
            "rho0: 0.6909, rho1: 0.2000\n",
            "Epoch 58, Loss: 0.6988, rho_lr: 0.002000\n",
            "rho0: 0.6915, rho1: 0.2000\n",
            "Epoch 59, Loss: 0.6936, rho_lr: 0.002000\n",
            "rho0: 0.6922, rho1: 0.2000\n",
            "Epoch 60, Loss: 0.6955, rho_lr: 0.000200\n",
            "rho0: 0.6928, rho1: 0.2000\n",
            "Epoch 1, Loss: 44.4164, rho_lr: 0.200000\n",
            "rho0: 0.0000, rho1: 0.5000\n",
            "Epoch 2, Loss: 16.9243, rho_lr: 0.200000\n",
            "rho0: 0.1700, rho1: 0.5000\n",
            "Epoch 3, Loss: 13.7414, rho_lr: 0.200000\n",
            "rho0: 0.1786, rho1: 0.5000\n",
            "Epoch 4, Loss: 7.5878, rho_lr: 0.200000\n",
            "rho0: 0.2483, rho1: 0.5000\n",
            "Epoch 5, Loss: 5.9329, rho_lr: 0.200000\n",
            "rho0: 0.2566, rho1: 0.5000\n",
            "Epoch 6, Loss: 3.9481, rho_lr: 0.200000\n",
            "rho0: 0.3169, rho1: 0.5000\n",
            "Epoch 7, Loss: 3.3333, rho_lr: 0.200000\n",
            "rho0: 0.3119, rho1: 0.5000\n",
            "Epoch 8, Loss: 2.9147, rho_lr: 0.200000\n",
            "rho0: 0.3152, rho1: 0.5000\n",
            "Epoch 9, Loss: 2.6492, rho_lr: 0.200000\n",
            "rho0: 0.3202, rho1: 0.5000\n",
            "Epoch 10, Loss: 2.4246, rho_lr: 0.200000\n",
            "rho0: 0.3142, rho1: 0.5000\n",
            "Epoch 11, Loss: 2.3397, rho_lr: 0.200000\n",
            "rho0: 0.3806, rho1: 0.3697\n",
            "Epoch 12, Loss: 1.6829, rho_lr: 0.200000\n",
            "rho0: 0.5025, rho1: 0.2007\n",
            "Epoch 13, Loss: 1.2995, rho_lr: 0.200000\n",
            "rho0: 0.5645, rho1: 0.2000\n",
            "Epoch 14, Loss: 1.2322, rho_lr: 0.200000\n",
            "rho0: 0.5477, rho1: 0.2000\n",
            "Epoch 15, Loss: 1.1942, rho_lr: 0.200000\n",
            "rho0: 0.5746, rho1: 0.2000\n",
            "Epoch 16, Loss: 1.1507, rho_lr: 0.200000\n",
            "rho0: 0.5737, rho1: 0.2000\n",
            "Epoch 17, Loss: 1.1439, rho_lr: 0.200000\n",
            "rho0: 0.5862, rho1: 0.2000\n",
            "Epoch 18, Loss: 1.1206, rho_lr: 0.200000\n",
            "rho0: 0.5880, rho1: 0.2000\n",
            "Epoch 19, Loss: 1.0858, rho_lr: 0.200000\n",
            "rho0: 0.5968, rho1: 0.2000\n",
            "Epoch 20, Loss: 1.0845, rho_lr: 0.020000\n",
            "rho0: 0.6040, rho1: 0.2000\n",
            "Epoch 21, Loss: 1.0682, rho_lr: 0.020000\n",
            "rho0: 0.6051, rho1: 0.2000\n",
            "Epoch 22, Loss: 1.0415, rho_lr: 0.020000\n",
            "rho0: 0.6066, rho1: 0.2000\n",
            "Epoch 23, Loss: 1.0263, rho_lr: 0.020000\n",
            "rho0: 0.6092, rho1: 0.2000\n",
            "Epoch 24, Loss: 1.0028, rho_lr: 0.020000\n",
            "rho0: 0.6124, rho1: 0.2000\n",
            "Epoch 25, Loss: 1.0188, rho_lr: 0.020000\n",
            "rho0: 0.6152, rho1: 0.2000\n",
            "Epoch 26, Loss: 0.9975, rho_lr: 0.020000\n",
            "rho0: 0.6180, rho1: 0.2000\n",
            "Epoch 27, Loss: 0.9725, rho_lr: 0.020000\n",
            "rho0: 0.6213, rho1: 0.2000\n",
            "Epoch 28, Loss: 0.9845, rho_lr: 0.020000\n",
            "rho0: 0.6245, rho1: 0.2000\n",
            "Epoch 29, Loss: 0.9581, rho_lr: 0.020000\n",
            "rho0: 0.6275, rho1: 0.2000\n",
            "Epoch 30, Loss: 0.9510, rho_lr: 0.020000\n",
            "rho0: 0.6311, rho1: 0.2000\n",
            "Epoch 31, Loss: 0.9430, rho_lr: 0.020000\n",
            "rho0: 0.6340, rho1: 0.2000\n",
            "Epoch 32, Loss: 0.9411, rho_lr: 0.020000\n",
            "rho0: 0.6373, rho1: 0.2000\n",
            "Epoch 33, Loss: 0.9135, rho_lr: 0.020000\n",
            "rho0: 0.6403, rho1: 0.2000\n",
            "Epoch 34, Loss: 0.9051, rho_lr: 0.020000\n",
            "rho0: 0.6435, rho1: 0.2000\n",
            "Epoch 35, Loss: 0.9129, rho_lr: 0.020000\n",
            "rho0: 0.6463, rho1: 0.2000\n",
            "Epoch 36, Loss: 0.8928, rho_lr: 0.020000\n",
            "rho0: 0.6488, rho1: 0.2000\n",
            "Epoch 37, Loss: 0.8790, rho_lr: 0.020000\n",
            "rho0: 0.6518, rho1: 0.2000\n",
            "Epoch 38, Loss: 0.8814, rho_lr: 0.020000\n",
            "rho0: 0.6548, rho1: 0.2000\n",
            "Epoch 39, Loss: 0.8760, rho_lr: 0.020000\n",
            "rho0: 0.6579, rho1: 0.2000\n",
            "Epoch 40, Loss: 0.8626, rho_lr: 0.002000\n",
            "rho0: 0.6615, rho1: 0.2000\n",
            "Epoch 41, Loss: 0.8534, rho_lr: 0.002000\n",
            "rho0: 0.6618, rho1: 0.2000\n",
            "Epoch 42, Loss: 0.8491, rho_lr: 0.002000\n",
            "rho0: 0.6622, rho1: 0.2000\n",
            "Epoch 43, Loss: 0.8404, rho_lr: 0.002000\n",
            "rho0: 0.6627, rho1: 0.2000\n",
            "Epoch 44, Loss: 0.8270, rho_lr: 0.002000\n",
            "rho0: 0.6631, rho1: 0.2000\n",
            "Epoch 45, Loss: 0.8268, rho_lr: 0.002000\n",
            "rho0: 0.6636, rho1: 0.2000\n",
            "Epoch 46, Loss: 0.8183, rho_lr: 0.002000\n",
            "rho0: 0.6642, rho1: 0.2000\n",
            "Epoch 47, Loss: 0.8253, rho_lr: 0.002000\n",
            "rho0: 0.6647, rho1: 0.2000\n",
            "Epoch 48, Loss: 0.8162, rho_lr: 0.002000\n",
            "rho0: 0.6652, rho1: 0.2000\n",
            "Epoch 49, Loss: 0.8073, rho_lr: 0.002000\n",
            "rho0: 0.6657, rho1: 0.2000\n",
            "Epoch 50, Loss: 0.7940, rho_lr: 0.002000\n",
            "rho0: 0.6663, rho1: 0.2000\n",
            "Epoch 51, Loss: 0.7976, rho_lr: 0.002000\n",
            "rho0: 0.6668, rho1: 0.2000\n",
            "Epoch 52, Loss: 0.7995, rho_lr: 0.002000\n",
            "rho0: 0.6674, rho1: 0.2000\n",
            "Epoch 53, Loss: 0.7866, rho_lr: 0.002000\n",
            "rho0: 0.6681, rho1: 0.2000\n",
            "Epoch 54, Loss: 0.7794, rho_lr: 0.002000\n",
            "rho0: 0.6687, rho1: 0.2000\n",
            "Epoch 55, Loss: 0.7716, rho_lr: 0.002000\n",
            "rho0: 0.6693, rho1: 0.2000\n",
            "Epoch 56, Loss: 0.7725, rho_lr: 0.002000\n",
            "rho0: 0.6699, rho1: 0.2000\n",
            "Epoch 57, Loss: 0.7679, rho_lr: 0.002000\n",
            "rho0: 0.6704, rho1: 0.2000\n",
            "Epoch 58, Loss: 0.7646, rho_lr: 0.002000\n",
            "rho0: 0.6711, rho1: 0.2000\n",
            "Epoch 59, Loss: 0.7607, rho_lr: 0.002000\n",
            "rho0: 0.6718, rho1: 0.2000\n",
            "Epoch 60, Loss: 0.7544, rho_lr: 0.000200\n",
            "rho0: 0.6725, rho1: 0.2000\n",
            "Epoch 1, Loss: 30.0796, rho_lr: 0.200000\n",
            "rho0: 0.0000, rho1: 0.5000\n",
            "Epoch 2, Loss: 14.3173, rho_lr: 0.200000\n",
            "rho0: 0.3939, rho1: 0.5000\n",
            "Epoch 3, Loss: 7.8609, rho_lr: 0.200000\n",
            "rho0: 0.2887, rho1: 0.5000\n",
            "Epoch 4, Loss: 4.8577, rho_lr: 0.200000\n",
            "rho0: 0.3656, rho1: 0.5000\n",
            "Epoch 5, Loss: 3.4178, rho_lr: 0.200000\n",
            "rho0: 0.2994, rho1: 0.5000\n",
            "Epoch 6, Loss: 2.7561, rho_lr: 0.200000\n",
            "rho0: 0.3449, rho1: 0.5000\n",
            "Epoch 7, Loss: 2.4409, rho_lr: 0.200000\n",
            "rho0: 0.3476, rho1: 0.5000\n",
            "Epoch 8, Loss: 2.2074, rho_lr: 0.200000\n",
            "rho0: 0.3328, rho1: 0.5000\n",
            "Epoch 9, Loss: 2.0634, rho_lr: 0.200000\n",
            "rho0: 0.3374, rho1: 0.5000\n",
            "Epoch 10, Loss: 1.9306, rho_lr: 0.200000\n",
            "rho0: 0.3613, rho1: 0.5000\n",
            "Epoch 11, Loss: 1.8623, rho_lr: 0.200000\n",
            "rho0: 0.4317, rho1: 0.3677\n",
            "Epoch 12, Loss: 1.2727, rho_lr: 0.200000\n",
            "rho0: 0.5751, rho1: 0.2000\n",
            "Epoch 13, Loss: 1.0321, rho_lr: 0.200000\n",
            "rho0: 0.5937, rho1: 0.2000\n",
            "Epoch 14, Loss: 0.9537, rho_lr: 0.200000\n",
            "rho0: 0.6005, rho1: 0.2000\n",
            "Epoch 15, Loss: 0.9344, rho_lr: 0.200000\n",
            "rho0: 0.6134, rho1: 0.2000\n",
            "Epoch 16, Loss: 0.9154, rho_lr: 0.200000\n",
            "rho0: 0.6193, rho1: 0.2000\n",
            "Epoch 17, Loss: 0.8964, rho_lr: 0.200000\n",
            "rho0: 0.6243, rho1: 0.2000\n",
            "Epoch 18, Loss: 0.8868, rho_lr: 0.200000\n",
            "rho0: 0.6336, rho1: 0.2000\n",
            "Epoch 19, Loss: 0.8641, rho_lr: 0.200000\n",
            "rho0: 0.6358, rho1: 0.2000\n",
            "Epoch 20, Loss: 0.8550, rho_lr: 0.020000\n",
            "rho0: 0.6442, rho1: 0.2000\n",
            "Epoch 21, Loss: 0.8364, rho_lr: 0.020000\n",
            "rho0: 0.6456, rho1: 0.2000\n",
            "Epoch 22, Loss: 0.8244, rho_lr: 0.020000\n",
            "rho0: 0.6485, rho1: 0.2000\n",
            "Epoch 23, Loss: 0.8159, rho_lr: 0.020000\n",
            "rho0: 0.6520, rho1: 0.2000\n",
            "Epoch 24, Loss: 0.8051, rho_lr: 0.020000\n",
            "rho0: 0.6560, rho1: 0.2000\n",
            "Epoch 25, Loss: 0.7859, rho_lr: 0.020000\n",
            "rho0: 0.6596, rho1: 0.2000\n",
            "Epoch 26, Loss: 0.7852, rho_lr: 0.020000\n",
            "rho0: 0.6635, rho1: 0.2000\n",
            "Epoch 27, Loss: 0.7739, rho_lr: 0.020000\n",
            "rho0: 0.6674, rho1: 0.2000\n",
            "Epoch 28, Loss: 0.7642, rho_lr: 0.020000\n",
            "rho0: 0.6710, rho1: 0.2000\n",
            "Epoch 29, Loss: 0.7594, rho_lr: 0.020000\n",
            "rho0: 0.6742, rho1: 0.2000\n",
            "Epoch 30, Loss: 0.7438, rho_lr: 0.020000\n",
            "rho0: 0.6774, rho1: 0.2000\n",
            "Epoch 31, Loss: 0.7350, rho_lr: 0.020000\n",
            "rho0: 0.6811, rho1: 0.2000\n",
            "Epoch 32, Loss: 0.7278, rho_lr: 0.020000\n",
            "rho0: 0.6847, rho1: 0.2000\n",
            "Epoch 33, Loss: 0.7132, rho_lr: 0.020000\n",
            "rho0: 0.6881, rho1: 0.2000\n",
            "Epoch 34, Loss: 0.7156, rho_lr: 0.020000\n",
            "rho0: 0.6915, rho1: 0.2000\n",
            "Epoch 35, Loss: 0.7014, rho_lr: 0.020000\n",
            "rho0: 0.6948, rho1: 0.2000\n",
            "Epoch 36, Loss: 0.6993, rho_lr: 0.020000\n",
            "rho0: 0.6982, rho1: 0.2000\n",
            "Epoch 37, Loss: 0.6877, rho_lr: 0.020000\n",
            "rho0: 0.7016, rho1: 0.2000\n",
            "Epoch 38, Loss: 0.6877, rho_lr: 0.020000\n",
            "rho0: 0.7051, rho1: 0.2000\n",
            "Epoch 39, Loss: 0.6737, rho_lr: 0.020000\n",
            "rho0: 0.7085, rho1: 0.2000\n",
            "Epoch 40, Loss: 0.6749, rho_lr: 0.002000\n",
            "rho0: 0.7117, rho1: 0.2000\n",
            "Epoch 41, Loss: 0.6669, rho_lr: 0.002000\n",
            "rho0: 0.7121, rho1: 0.2000\n",
            "Epoch 42, Loss: 0.6604, rho_lr: 0.002000\n",
            "rho0: 0.7126, rho1: 0.2000\n",
            "Epoch 43, Loss: 0.6591, rho_lr: 0.002000\n",
            "rho0: 0.7132, rho1: 0.2000\n",
            "Epoch 44, Loss: 0.6579, rho_lr: 0.002000\n",
            "rho0: 0.7138, rho1: 0.2000\n",
            "Epoch 45, Loss: 0.6487, rho_lr: 0.002000\n",
            "rho0: 0.7145, rho1: 0.2000\n",
            "Epoch 46, Loss: 0.6494, rho_lr: 0.002000\n",
            "rho0: 0.7152, rho1: 0.2000\n",
            "Epoch 47, Loss: 0.6434, rho_lr: 0.002000\n",
            "rho0: 0.7159, rho1: 0.2000\n",
            "Epoch 48, Loss: 0.6386, rho_lr: 0.002000\n",
            "rho0: 0.7166, rho1: 0.2000\n",
            "Epoch 49, Loss: 0.6380, rho_lr: 0.002000\n",
            "rho0: 0.7173, rho1: 0.2000\n",
            "Epoch 50, Loss: 0.6346, rho_lr: 0.002000\n",
            "rho0: 0.7181, rho1: 0.2000\n",
            "Epoch 51, Loss: 0.6315, rho_lr: 0.002000\n",
            "rho0: 0.7189, rho1: 0.2000\n",
            "Epoch 52, Loss: 0.6314, rho_lr: 0.002000\n",
            "rho0: 0.7197, rho1: 0.2000\n",
            "Epoch 53, Loss: 0.6262, rho_lr: 0.002000\n",
            "rho0: 0.7204, rho1: 0.2000\n",
            "Epoch 54, Loss: 0.6250, rho_lr: 0.002000\n",
            "rho0: 0.7212, rho1: 0.2000\n",
            "Epoch 55, Loss: 0.6161, rho_lr: 0.002000\n",
            "rho0: 0.7220, rho1: 0.2000\n",
            "Epoch 56, Loss: 0.6159, rho_lr: 0.002000\n",
            "rho0: 0.7228, rho1: 0.2000\n",
            "Epoch 57, Loss: 0.6152, rho_lr: 0.002000\n",
            "rho0: 0.7236, rho1: 0.2000\n",
            "Epoch 58, Loss: 0.6155, rho_lr: 0.002000\n",
            "rho0: 0.7244, rho1: 0.2000\n",
            "Epoch 59, Loss: 0.6134, rho_lr: 0.002000\n",
            "rho0: 0.7252, rho1: 0.2000\n",
            "Epoch 60, Loss: 0.6093, rho_lr: 0.000200\n",
            "rho0: 0.7260, rho1: 0.2000\n",
            "Epoch 1, Loss: 36.6448, rho_lr: 0.200000\n",
            "rho0: 0.0000, rho1: 0.5000\n",
            "Epoch 2, Loss: 14.3433, rho_lr: 0.200000\n",
            "rho0: 0.2973, rho1: 0.5000\n",
            "Epoch 3, Loss: 7.6782, rho_lr: 0.200000\n",
            "rho0: 0.2495, rho1: 0.5000\n",
            "Epoch 4, Loss: 5.5575, rho_lr: 0.200000\n",
            "rho0: 0.2846, rho1: 0.5000\n",
            "Epoch 5, Loss: 4.3457, rho_lr: 0.200000\n",
            "rho0: 0.3133, rho1: 0.5000\n",
            "Epoch 6, Loss: 3.4275, rho_lr: 0.200000\n",
            "rho0: 0.3249, rho1: 0.5000\n",
            "Epoch 7, Loss: 3.0924, rho_lr: 0.200000\n",
            "rho0: 0.3358, rho1: 0.5000\n",
            "Epoch 8, Loss: 2.8179, rho_lr: 0.200000\n",
            "rho0: 0.3283, rho1: 0.5000\n",
            "Epoch 9, Loss: 2.4978, rho_lr: 0.200000\n",
            "rho0: 0.3320, rho1: 0.5000\n",
            "Epoch 10, Loss: 2.3507, rho_lr: 0.200000\n",
            "rho0: 0.3373, rho1: 0.5000\n",
            "Epoch 11, Loss: 2.2797, rho_lr: 0.200000\n",
            "rho0: 0.4146, rho1: 0.3792\n",
            "Epoch 12, Loss: 1.6087, rho_lr: 0.200000\n",
            "rho0: 0.5550, rho1: 0.2000\n",
            "Epoch 13, Loss: 1.2230, rho_lr: 0.200000\n",
            "rho0: 0.5798, rho1: 0.2000\n",
            "Epoch 14, Loss: 1.1488, rho_lr: 0.200000\n",
            "rho0: 0.5942, rho1: 0.2000\n",
            "Epoch 15, Loss: 1.1068, rho_lr: 0.200000\n",
            "rho0: 0.6079, rho1: 0.2000\n",
            "Epoch 16, Loss: 1.0889, rho_lr: 0.200000\n",
            "rho0: 0.6207, rho1: 0.2000\n",
            "Epoch 17, Loss: 1.0601, rho_lr: 0.200000\n",
            "rho0: 0.6220, rho1: 0.2000\n",
            "Epoch 18, Loss: 1.0408, rho_lr: 0.200000\n",
            "rho0: 0.6357, rho1: 0.2000\n",
            "Epoch 19, Loss: 1.0121, rho_lr: 0.200000\n",
            "rho0: 0.6389, rho1: 0.2000\n",
            "Epoch 20, Loss: 0.9952, rho_lr: 0.020000\n",
            "rho0: 0.6477, rho1: 0.2000\n",
            "Epoch 21, Loss: 0.9758, rho_lr: 0.020000\n",
            "rho0: 0.6490, rho1: 0.2000\n",
            "Epoch 22, Loss: 0.9491, rho_lr: 0.020000\n",
            "rho0: 0.6520, rho1: 0.2000\n",
            "Epoch 23, Loss: 0.9500, rho_lr: 0.020000\n",
            "rho0: 0.6549, rho1: 0.2000\n",
            "Epoch 24, Loss: 0.9303, rho_lr: 0.020000\n",
            "rho0: 0.6578, rho1: 0.2000\n",
            "Epoch 25, Loss: 0.9141, rho_lr: 0.020000\n",
            "rho0: 0.6612, rho1: 0.2000\n",
            "Epoch 26, Loss: 0.9054, rho_lr: 0.020000\n",
            "rho0: 0.6645, rho1: 0.2000\n",
            "Epoch 27, Loss: 0.8901, rho_lr: 0.020000\n",
            "rho0: 0.6685, rho1: 0.2000\n",
            "Epoch 28, Loss: 0.8779, rho_lr: 0.020000\n",
            "rho0: 0.6720, rho1: 0.2000\n",
            "Epoch 29, Loss: 0.8675, rho_lr: 0.020000\n",
            "rho0: 0.6761, rho1: 0.2000\n",
            "Epoch 30, Loss: 0.8535, rho_lr: 0.020000\n",
            "rho0: 0.6801, rho1: 0.2000\n",
            "Epoch 31, Loss: 0.8405, rho_lr: 0.020000\n",
            "rho0: 0.6834, rho1: 0.2000\n",
            "Epoch 32, Loss: 0.8214, rho_lr: 0.020000\n",
            "rho0: 0.6867, rho1: 0.2000\n",
            "Epoch 33, Loss: 0.8154, rho_lr: 0.020000\n",
            "rho0: 0.6907, rho1: 0.2000\n",
            "Epoch 34, Loss: 0.8025, rho_lr: 0.020000\n",
            "rho0: 0.6945, rho1: 0.2000\n",
            "Epoch 35, Loss: 0.7929, rho_lr: 0.020000\n",
            "rho0: 0.6979, rho1: 0.2000\n",
            "Epoch 36, Loss: 0.7846, rho_lr: 0.020000\n",
            "rho0: 0.7017, rho1: 0.2000\n",
            "Epoch 37, Loss: 0.7739, rho_lr: 0.020000\n",
            "rho0: 0.7054, rho1: 0.2000\n",
            "Epoch 38, Loss: 0.7689, rho_lr: 0.020000\n",
            "rho0: 0.7085, rho1: 0.2000\n",
            "Epoch 39, Loss: 0.7592, rho_lr: 0.020000\n",
            "rho0: 0.7124, rho1: 0.2000\n",
            "Epoch 40, Loss: 0.7488, rho_lr: 0.002000\n",
            "rho0: 0.7155, rho1: 0.2000\n",
            "Epoch 41, Loss: 0.7395, rho_lr: 0.002000\n",
            "rho0: 0.7159, rho1: 0.2000\n",
            "Epoch 42, Loss: 0.7362, rho_lr: 0.002000\n",
            "rho0: 0.7163, rho1: 0.2000\n",
            "Epoch 43, Loss: 0.7254, rho_lr: 0.002000\n",
            "rho0: 0.7167, rho1: 0.2000\n",
            "Epoch 44, Loss: 0.7196, rho_lr: 0.002000\n",
            "rho0: 0.7172, rho1: 0.2000\n",
            "Epoch 45, Loss: 0.7106, rho_lr: 0.002000\n",
            "rho0: 0.7178, rho1: 0.2000\n",
            "Epoch 46, Loss: 0.7047, rho_lr: 0.002000\n",
            "rho0: 0.7183, rho1: 0.2000\n",
            "Epoch 47, Loss: 0.7041, rho_lr: 0.002000\n",
            "rho0: 0.7189, rho1: 0.2000\n",
            "Epoch 48, Loss: 0.6921, rho_lr: 0.002000\n",
            "rho0: 0.7195, rho1: 0.2000\n",
            "Epoch 49, Loss: 0.6920, rho_lr: 0.002000\n",
            "rho0: 0.7201, rho1: 0.2000\n",
            "Epoch 50, Loss: 0.6881, rho_lr: 0.002000\n",
            "rho0: 0.7207, rho1: 0.2000\n",
            "Epoch 51, Loss: 0.6859, rho_lr: 0.002000\n",
            "rho0: 0.7214, rho1: 0.2000\n",
            "Epoch 52, Loss: 0.6798, rho_lr: 0.002000\n",
            "rho0: 0.7220, rho1: 0.2000\n",
            "Epoch 53, Loss: 0.6730, rho_lr: 0.002000\n",
            "rho0: 0.7226, rho1: 0.2000\n",
            "Epoch 54, Loss: 0.6717, rho_lr: 0.002000\n",
            "rho0: 0.7233, rho1: 0.2000\n",
            "Epoch 55, Loss: 0.6665, rho_lr: 0.002000\n",
            "rho0: 0.7240, rho1: 0.2000\n",
            "Epoch 56, Loss: 0.6651, rho_lr: 0.002000\n",
            "rho0: 0.7247, rho1: 0.2000\n",
            "Epoch 57, Loss: 0.6570, rho_lr: 0.002000\n",
            "rho0: 0.7253, rho1: 0.2000\n",
            "Epoch 58, Loss: 0.6529, rho_lr: 0.002000\n",
            "rho0: 0.7260, rho1: 0.2000\n",
            "Epoch 59, Loss: 0.6488, rho_lr: 0.002000\n",
            "rho0: 0.7267, rho1: 0.2000\n",
            "Epoch 60, Loss: 0.6438, rho_lr: 0.000200\n",
            "rho0: 0.7274, rho1: 0.2000\n",
            "Epoch 1, Loss: 40.4376, rho_lr: 0.200000\n",
            "rho0: 0.0000, rho1: 0.5000\n",
            "Epoch 2, Loss: 13.5525, rho_lr: 0.200000\n",
            "rho0: 0.3017, rho1: 0.5000\n",
            "Epoch 3, Loss: 7.8689, rho_lr: 0.200000\n",
            "rho0: 0.2705, rho1: 0.5000\n",
            "Epoch 4, Loss: 5.4475, rho_lr: 0.200000\n",
            "rho0: 0.3512, rho1: 0.5000\n",
            "Epoch 5, Loss: 3.9812, rho_lr: 0.200000\n",
            "rho0: 0.3320, rho1: 0.5000\n",
            "Epoch 6, Loss: 3.0301, rho_lr: 0.200000\n",
            "rho0: 0.3406, rho1: 0.5000\n",
            "Epoch 7, Loss: 2.6016, rho_lr: 0.200000\n",
            "rho0: 0.3650, rho1: 0.5000\n",
            "Epoch 8, Loss: 2.3992, rho_lr: 0.200000\n",
            "rho0: 0.3481, rho1: 0.5000\n",
            "Epoch 9, Loss: 2.1897, rho_lr: 0.200000\n",
            "rho0: 0.3559, rho1: 0.5000\n",
            "Epoch 10, Loss: 2.0997, rho_lr: 0.200000\n",
            "rho0: 0.3572, rho1: 0.5000\n",
            "Epoch 11, Loss: 2.0923, rho_lr: 0.200000\n",
            "rho0: 0.4261, rho1: 0.3895\n",
            "Epoch 12, Loss: 1.4718, rho_lr: 0.200000\n",
            "rho0: 0.5451, rho1: 0.2000\n",
            "Epoch 13, Loss: 1.1247, rho_lr: 0.200000\n",
            "rho0: 0.6019, rho1: 0.2000\n",
            "Epoch 14, Loss: 1.0889, rho_lr: 0.200000\n",
            "rho0: 0.5892, rho1: 0.2000\n",
            "Epoch 15, Loss: 1.0400, rho_lr: 0.200000\n",
            "rho0: 0.5947, rho1: 0.2000\n",
            "Epoch 16, Loss: 1.0086, rho_lr: 0.200000\n",
            "rho0: 0.6094, rho1: 0.2000\n",
            "Epoch 17, Loss: 0.9955, rho_lr: 0.200000\n",
            "rho0: 0.6204, rho1: 0.2000\n",
            "Epoch 18, Loss: 0.9924, rho_lr: 0.200000\n",
            "rho0: 0.6168, rho1: 0.2000\n",
            "Epoch 19, Loss: 0.9761, rho_lr: 0.200000\n",
            "rho0: 0.6250, rho1: 0.2000\n",
            "Epoch 20, Loss: 0.9711, rho_lr: 0.020000\n",
            "rho0: 0.6324, rho1: 0.2000\n",
            "Epoch 21, Loss: 0.9558, rho_lr: 0.020000\n",
            "rho0: 0.6323, rho1: 0.2000\n",
            "Epoch 22, Loss: 0.9369, rho_lr: 0.020000\n",
            "rho0: 0.6341, rho1: 0.2000\n",
            "Epoch 23, Loss: 0.9345, rho_lr: 0.020000\n",
            "rho0: 0.6360, rho1: 0.2000\n",
            "Epoch 24, Loss: 0.9056, rho_lr: 0.020000\n",
            "rho0: 0.6390, rho1: 0.2000\n",
            "Epoch 25, Loss: 0.8998, rho_lr: 0.020000\n",
            "rho0: 0.6419, rho1: 0.2000\n",
            "Epoch 26, Loss: 0.9093, rho_lr: 0.020000\n",
            "rho0: 0.6450, rho1: 0.2000\n",
            "Epoch 27, Loss: 0.8931, rho_lr: 0.020000\n",
            "rho0: 0.6474, rho1: 0.2000\n",
            "Epoch 28, Loss: 0.8858, rho_lr: 0.020000\n",
            "rho0: 0.6498, rho1: 0.2000\n",
            "Epoch 29, Loss: 0.8727, rho_lr: 0.020000\n",
            "rho0: 0.6523, rho1: 0.2000\n",
            "Epoch 30, Loss: 0.8693, rho_lr: 0.020000\n",
            "rho0: 0.6552, rho1: 0.2000\n",
            "Epoch 31, Loss: 0.8598, rho_lr: 0.020000\n",
            "rho0: 0.6582, rho1: 0.2000\n",
            "Epoch 32, Loss: 0.8415, rho_lr: 0.020000\n",
            "rho0: 0.6609, rho1: 0.2000\n",
            "Epoch 33, Loss: 0.8435, rho_lr: 0.020000\n",
            "rho0: 0.6640, rho1: 0.2000\n",
            "Epoch 34, Loss: 0.8362, rho_lr: 0.020000\n",
            "rho0: 0.6666, rho1: 0.2000\n",
            "Epoch 35, Loss: 0.8244, rho_lr: 0.020000\n",
            "rho0: 0.6692, rho1: 0.2000\n",
            "Epoch 36, Loss: 0.8216, rho_lr: 0.020000\n",
            "rho0: 0.6716, rho1: 0.2000\n",
            "Epoch 37, Loss: 0.8125, rho_lr: 0.020000\n",
            "rho0: 0.6742, rho1: 0.2000\n",
            "Epoch 38, Loss: 0.8118, rho_lr: 0.020000\n",
            "rho0: 0.6772, rho1: 0.2000\n",
            "Epoch 39, Loss: 0.8068, rho_lr: 0.020000\n",
            "rho0: 0.6793, rho1: 0.2000\n",
            "Epoch 40, Loss: 0.7950, rho_lr: 0.002000\n",
            "rho0: 0.6818, rho1: 0.2000\n",
            "Epoch 41, Loss: 0.7912, rho_lr: 0.002000\n",
            "rho0: 0.6822, rho1: 0.2000\n",
            "Epoch 42, Loss: 0.7784, rho_lr: 0.002000\n",
            "rho0: 0.6826, rho1: 0.2000\n",
            "Epoch 43, Loss: 0.7765, rho_lr: 0.002000\n",
            "rho0: 0.6830, rho1: 0.2000\n",
            "Epoch 44, Loss: 0.7757, rho_lr: 0.002000\n",
            "rho0: 0.6833, rho1: 0.2000\n",
            "Epoch 45, Loss: 0.7692, rho_lr: 0.002000\n",
            "rho0: 0.6838, rho1: 0.2000\n",
            "Epoch 46, Loss: 0.7650, rho_lr: 0.002000\n",
            "rho0: 0.6843, rho1: 0.2000\n",
            "Epoch 47, Loss: 0.7621, rho_lr: 0.002000\n",
            "rho0: 0.6848, rho1: 0.2000\n",
            "Epoch 48, Loss: 0.7484, rho_lr: 0.002000\n",
            "rho0: 0.6853, rho1: 0.2000\n",
            "Epoch 49, Loss: 0.7442, rho_lr: 0.002000\n",
            "rho0: 0.6858, rho1: 0.2000\n",
            "Epoch 50, Loss: 0.7517, rho_lr: 0.002000\n",
            "rho0: 0.6864, rho1: 0.2000\n",
            "Epoch 51, Loss: 0.7507, rho_lr: 0.002000\n",
            "rho0: 0.6869, rho1: 0.2000\n",
            "Epoch 52, Loss: 0.7409, rho_lr: 0.002000\n",
            "rho0: 0.6874, rho1: 0.2000\n",
            "Epoch 53, Loss: 0.7451, rho_lr: 0.002000\n",
            "rho0: 0.6880, rho1: 0.2000\n",
            "Epoch 54, Loss: 0.7392, rho_lr: 0.002000\n",
            "rho0: 0.6885, rho1: 0.2000\n",
            "Epoch 55, Loss: 0.7301, rho_lr: 0.002000\n",
            "rho0: 0.6891, rho1: 0.2000\n",
            "Epoch 56, Loss: 0.7203, rho_lr: 0.002000\n",
            "rho0: 0.6897, rho1: 0.2000\n",
            "Epoch 57, Loss: 0.7250, rho_lr: 0.002000\n",
            "rho0: 0.6903, rho1: 0.2000\n",
            "Epoch 58, Loss: 0.7218, rho_lr: 0.002000\n",
            "rho0: 0.6909, rho1: 0.2000\n",
            "Epoch 59, Loss: 0.7094, rho_lr: 0.002000\n",
            "rho0: 0.6914, rho1: 0.2000\n",
            "Epoch 60, Loss: 0.7101, rho_lr: 0.000200\n",
            "rho0: 0.6920, rho1: 0.2000\n",
            "Epoch 1, Loss: 32.9490, rho_lr: 0.200000\n",
            "rho0: 0.0000, rho1: 0.5000\n",
            "Epoch 2, Loss: 12.1551, rho_lr: 0.200000\n",
            "rho0: 0.3320, rho1: 0.5000\n",
            "Epoch 3, Loss: 6.3451, rho_lr: 0.200000\n",
            "rho0: 0.3274, rho1: 0.5000\n",
            "Epoch 4, Loss: 3.9474, rho_lr: 0.200000\n",
            "rho0: 0.3381, rho1: 0.5000\n",
            "Epoch 5, Loss: 3.0532, rho_lr: 0.200000\n",
            "rho0: 0.3559, rho1: 0.5000\n",
            "Epoch 6, Loss: 2.4803, rho_lr: 0.200000\n",
            "rho0: 0.3375, rho1: 0.5000\n",
            "Epoch 7, Loss: 2.2189, rho_lr: 0.200000\n",
            "rho0: 0.3458, rho1: 0.5000\n",
            "Epoch 8, Loss: 1.9886, rho_lr: 0.200000\n",
            "rho0: 0.3498, rho1: 0.5000\n",
            "Epoch 9, Loss: 1.8675, rho_lr: 0.200000\n",
            "rho0: 0.3465, rho1: 0.5000\n",
            "Epoch 10, Loss: 1.7130, rho_lr: 0.200000\n",
            "rho0: 0.3453, rho1: 0.5000\n",
            "Epoch 11, Loss: 1.7856, rho_lr: 0.200000\n",
            "rho0: 0.4171, rho1: 0.4031\n",
            "Epoch 12, Loss: 1.3541, rho_lr: 0.200000\n",
            "rho0: 0.5195, rho1: 0.2301\n",
            "Epoch 13, Loss: 0.9897, rho_lr: 0.200000\n",
            "rho0: 0.6014, rho1: 0.2000\n",
            "Epoch 14, Loss: 0.9477, rho_lr: 0.200000\n",
            "rho0: 0.5821, rho1: 0.2000\n",
            "Epoch 15, Loss: 0.9241, rho_lr: 0.200000\n",
            "rho0: 0.5856, rho1: 0.2000\n",
            "Epoch 16, Loss: 0.9027, rho_lr: 0.200000\n",
            "rho0: 0.5974, rho1: 0.2000\n",
            "Epoch 17, Loss: 0.8908, rho_lr: 0.200000\n",
            "rho0: 0.5942, rho1: 0.2000\n",
            "Epoch 18, Loss: 0.8719, rho_lr: 0.200000\n",
            "rho0: 0.6027, rho1: 0.2000\n",
            "Epoch 19, Loss: 0.8575, rho_lr: 0.200000\n",
            "rho0: 0.6091, rho1: 0.2000\n",
            "Epoch 20, Loss: 0.8505, rho_lr: 0.020000\n",
            "rho0: 0.6117, rho1: 0.2000\n",
            "Epoch 21, Loss: 0.8469, rho_lr: 0.020000\n",
            "rho0: 0.6119, rho1: 0.2000\n",
            "Epoch 22, Loss: 0.8287, rho_lr: 0.020000\n",
            "rho0: 0.6138, rho1: 0.2000\n",
            "Epoch 23, Loss: 0.8224, rho_lr: 0.020000\n",
            "rho0: 0.6176, rho1: 0.2000\n",
            "Epoch 24, Loss: 0.8118, rho_lr: 0.020000\n",
            "rho0: 0.6209, rho1: 0.2000\n",
            "Epoch 25, Loss: 0.8010, rho_lr: 0.020000\n",
            "rho0: 0.6239, rho1: 0.2000\n",
            "Epoch 26, Loss: 0.7945, rho_lr: 0.020000\n",
            "rho0: 0.6267, rho1: 0.2000\n",
            "Epoch 27, Loss: 0.7824, rho_lr: 0.020000\n",
            "rho0: 0.6299, rho1: 0.2000\n",
            "Epoch 28, Loss: 0.7745, rho_lr: 0.020000\n",
            "rho0: 0.6326, rho1: 0.2000\n",
            "Epoch 29, Loss: 0.7698, rho_lr: 0.020000\n",
            "rho0: 0.6352, rho1: 0.2000\n",
            "Epoch 30, Loss: 0.7634, rho_lr: 0.020000\n",
            "rho0: 0.6377, rho1: 0.2000\n",
            "Epoch 31, Loss: 0.7489, rho_lr: 0.020000\n",
            "rho0: 0.6409, rho1: 0.2000\n",
            "Epoch 32, Loss: 0.7425, rho_lr: 0.020000\n",
            "rho0: 0.6436, rho1: 0.2000\n",
            "Epoch 33, Loss: 0.7360, rho_lr: 0.020000\n",
            "rho0: 0.6464, rho1: 0.2000\n",
            "Epoch 34, Loss: 0.7333, rho_lr: 0.020000\n",
            "rho0: 0.6495, rho1: 0.2000\n",
            "Epoch 35, Loss: 0.7251, rho_lr: 0.020000\n",
            "rho0: 0.6523, rho1: 0.2000\n",
            "Epoch 36, Loss: 0.7145, rho_lr: 0.020000\n",
            "rho0: 0.6550, rho1: 0.2000\n",
            "Epoch 37, Loss: 0.7162, rho_lr: 0.020000\n",
            "rho0: 0.6574, rho1: 0.2000\n",
            "Epoch 38, Loss: 0.7076, rho_lr: 0.020000\n",
            "rho0: 0.6605, rho1: 0.2000\n",
            "Epoch 39, Loss: 0.6989, rho_lr: 0.020000\n",
            "rho0: 0.6633, rho1: 0.2000\n",
            "Epoch 40, Loss: 0.6967, rho_lr: 0.002000\n",
            "rho0: 0.6657, rho1: 0.2000\n",
            "Epoch 41, Loss: 0.6908, rho_lr: 0.002000\n",
            "rho0: 0.6660, rho1: 0.2000\n",
            "Epoch 42, Loss: 0.6875, rho_lr: 0.002000\n",
            "rho0: 0.6664, rho1: 0.2000\n",
            "Epoch 43, Loss: 0.6792, rho_lr: 0.002000\n",
            "rho0: 0.6669, rho1: 0.2000\n",
            "Epoch 44, Loss: 0.6780, rho_lr: 0.002000\n",
            "rho0: 0.6674, rho1: 0.2000\n",
            "Epoch 45, Loss: 0.6755, rho_lr: 0.002000\n",
            "rho0: 0.6680, rho1: 0.2000\n",
            "Epoch 46, Loss: 0.6737, rho_lr: 0.002000\n",
            "rho0: 0.6686, rho1: 0.2000\n",
            "Epoch 47, Loss: 0.6640, rho_lr: 0.002000\n",
            "rho0: 0.6691, rho1: 0.2000\n",
            "Epoch 48, Loss: 0.6669, rho_lr: 0.002000\n",
            "rho0: 0.6697, rho1: 0.2000\n",
            "Epoch 49, Loss: 0.6640, rho_lr: 0.002000\n",
            "rho0: 0.6703, rho1: 0.2000\n",
            "Epoch 50, Loss: 0.6632, rho_lr: 0.002000\n",
            "rho0: 0.6710, rho1: 0.2000\n",
            "Epoch 51, Loss: 0.6576, rho_lr: 0.002000\n",
            "rho0: 0.6717, rho1: 0.2000\n",
            "Epoch 52, Loss: 0.6497, rho_lr: 0.002000\n",
            "rho0: 0.6723, rho1: 0.2000\n",
            "Epoch 53, Loss: 0.6485, rho_lr: 0.002000\n",
            "rho0: 0.6729, rho1: 0.2000\n",
            "Epoch 54, Loss: 0.6489, rho_lr: 0.002000\n",
            "rho0: 0.6736, rho1: 0.2000\n",
            "Epoch 55, Loss: 0.6438, rho_lr: 0.002000\n",
            "rho0: 0.6742, rho1: 0.2000\n",
            "Epoch 56, Loss: 0.6414, rho_lr: 0.002000\n",
            "rho0: 0.6748, rho1: 0.2000\n",
            "Epoch 57, Loss: 0.6406, rho_lr: 0.002000\n",
            "rho0: 0.6755, rho1: 0.2000\n",
            "Epoch 58, Loss: 0.6369, rho_lr: 0.002000\n",
            "rho0: 0.6762, rho1: 0.2000\n",
            "Epoch 59, Loss: 0.6349, rho_lr: 0.002000\n",
            "rho0: 0.6769, rho1: 0.2000\n",
            "Epoch 60, Loss: 0.6330, rho_lr: 0.000200\n",
            "rho0: 0.6775, rho1: 0.2000\n",
            "Epoch 1, Loss: 47.2559, rho_lr: 0.200000\n",
            "rho0: 0.0000, rho1: 0.5000\n",
            "Epoch 2, Loss: 13.8381, rho_lr: 0.200000\n",
            "rho0: 0.2393, rho1: 0.5000\n",
            "Epoch 3, Loss: 10.8383, rho_lr: 0.200000\n",
            "rho0: 0.1576, rho1: 0.5000\n",
            "Epoch 4, Loss: 7.3227, rho_lr: 0.200000\n",
            "rho0: 0.3131, rho1: 0.5000\n",
            "Epoch 5, Loss: 5.2342, rho_lr: 0.200000\n",
            "rho0: 0.3034, rho1: 0.5000\n",
            "Epoch 6, Loss: 3.8698, rho_lr: 0.200000\n",
            "rho0: 0.3291, rho1: 0.5000\n",
            "Epoch 7, Loss: 3.3012, rho_lr: 0.200000\n",
            "rho0: 0.3398, rho1: 0.5000\n",
            "Epoch 8, Loss: 2.8130, rho_lr: 0.200000\n",
            "rho0: 0.3330, rho1: 0.5000\n",
            "Epoch 9, Loss: 2.5443, rho_lr: 0.200000\n",
            "rho0: 0.3324, rho1: 0.5000\n",
            "Epoch 10, Loss: 2.3208, rho_lr: 0.200000\n",
            "rho0: 0.3411, rho1: 0.5000\n",
            "Epoch 11, Loss: 2.2802, rho_lr: 0.200000\n",
            "rho0: 0.4009, rho1: 0.3863\n",
            "Epoch 12, Loss: 1.6867, rho_lr: 0.200000\n",
            "rho0: 0.5077, rho1: 0.2305\n",
            "Epoch 13, Loss: 1.2094, rho_lr: 0.200000\n",
            "rho0: 0.5818, rho1: 0.2000\n",
            "Epoch 14, Loss: 1.1493, rho_lr: 0.200000\n",
            "rho0: 0.5783, rho1: 0.2000\n",
            "Epoch 15, Loss: 1.1091, rho_lr: 0.200000\n",
            "rho0: 0.5873, rho1: 0.2000\n",
            "Epoch 16, Loss: 1.0915, rho_lr: 0.200000\n",
            "rho0: 0.5922, rho1: 0.2000\n",
            "Epoch 17, Loss: 1.0551, rho_lr: 0.200000\n",
            "rho0: 0.6071, rho1: 0.2000\n",
            "Epoch 18, Loss: 1.0284, rho_lr: 0.200000\n",
            "rho0: 0.6108, rho1: 0.2000\n",
            "Epoch 19, Loss: 1.0141, rho_lr: 0.200000\n",
            "rho0: 0.6177, rho1: 0.2000\n",
            "Epoch 20, Loss: 0.9812, rho_lr: 0.020000\n",
            "rho0: 0.6247, rho1: 0.2000\n",
            "Epoch 21, Loss: 0.9802, rho_lr: 0.020000\n",
            "rho0: 0.6255, rho1: 0.2000\n",
            "Epoch 22, Loss: 0.9837, rho_lr: 0.020000\n",
            "rho0: 0.6267, rho1: 0.2000\n",
            "Epoch 23, Loss: 0.9567, rho_lr: 0.020000\n",
            "rho0: 0.6286, rho1: 0.2000\n",
            "Epoch 24, Loss: 0.9506, rho_lr: 0.020000\n",
            "rho0: 0.6311, rho1: 0.2000\n",
            "Epoch 25, Loss: 0.9320, rho_lr: 0.020000\n",
            "rho0: 0.6336, rho1: 0.2000\n",
            "Epoch 26, Loss: 0.9313, rho_lr: 0.020000\n",
            "rho0: 0.6362, rho1: 0.2000\n",
            "Epoch 27, Loss: 0.9224, rho_lr: 0.020000\n",
            "rho0: 0.6389, rho1: 0.2000\n",
            "Epoch 28, Loss: 0.8976, rho_lr: 0.020000\n",
            "rho0: 0.6414, rho1: 0.2000\n",
            "Epoch 29, Loss: 0.9005, rho_lr: 0.020000\n",
            "rho0: 0.6441, rho1: 0.2000\n",
            "Epoch 30, Loss: 0.8856, rho_lr: 0.020000\n",
            "rho0: 0.6463, rho1: 0.2000\n",
            "Epoch 31, Loss: 0.8854, rho_lr: 0.020000\n",
            "rho0: 0.6487, rho1: 0.2000\n",
            "Epoch 32, Loss: 0.8662, rho_lr: 0.020000\n",
            "rho0: 0.6510, rho1: 0.2000\n",
            "Epoch 33, Loss: 0.8594, rho_lr: 0.020000\n",
            "rho0: 0.6534, rho1: 0.2000\n",
            "Epoch 34, Loss: 0.8544, rho_lr: 0.020000\n",
            "rho0: 0.6554, rho1: 0.2000\n",
            "Epoch 35, Loss: 0.8482, rho_lr: 0.020000\n",
            "rho0: 0.6578, rho1: 0.2000\n",
            "Epoch 36, Loss: 0.8405, rho_lr: 0.020000\n",
            "rho0: 0.6603, rho1: 0.2000\n",
            "Epoch 37, Loss: 0.8327, rho_lr: 0.020000\n",
            "rho0: 0.6623, rho1: 0.2000\n",
            "Epoch 38, Loss: 0.8246, rho_lr: 0.020000\n",
            "rho0: 0.6650, rho1: 0.2000\n",
            "Epoch 39, Loss: 0.8144, rho_lr: 0.020000\n",
            "rho0: 0.6671, rho1: 0.2000\n",
            "Epoch 40, Loss: 0.8081, rho_lr: 0.002000\n",
            "rho0: 0.6693, rho1: 0.2000\n",
            "Epoch 41, Loss: 0.7986, rho_lr: 0.002000\n",
            "rho0: 0.6696, rho1: 0.2000\n",
            "Epoch 42, Loss: 0.8032, rho_lr: 0.002000\n",
            "rho0: 0.6699, rho1: 0.2000\n",
            "Epoch 43, Loss: 0.7896, rho_lr: 0.002000\n",
            "rho0: 0.6702, rho1: 0.2000\n",
            "Epoch 44, Loss: 0.7863, rho_lr: 0.002000\n",
            "rho0: 0.6706, rho1: 0.2000\n",
            "Epoch 45, Loss: 0.7875, rho_lr: 0.002000\n",
            "rho0: 0.6710, rho1: 0.2000\n",
            "Epoch 46, Loss: 0.7769, rho_lr: 0.002000\n",
            "rho0: 0.6714, rho1: 0.2000\n",
            "Epoch 47, Loss: 0.7724, rho_lr: 0.002000\n",
            "rho0: 0.6718, rho1: 0.2000\n",
            "Epoch 48, Loss: 0.7695, rho_lr: 0.002000\n",
            "rho0: 0.6722, rho1: 0.2000\n",
            "Epoch 49, Loss: 0.7640, rho_lr: 0.002000\n",
            "rho0: 0.6726, rho1: 0.2000\n",
            "Epoch 50, Loss: 0.7552, rho_lr: 0.002000\n",
            "rho0: 0.6730, rho1: 0.2000\n",
            "Epoch 51, Loss: 0.7559, rho_lr: 0.002000\n",
            "rho0: 0.6735, rho1: 0.2000\n",
            "Epoch 52, Loss: 0.7589, rho_lr: 0.002000\n",
            "rho0: 0.6739, rho1: 0.2000\n",
            "Epoch 53, Loss: 0.7500, rho_lr: 0.002000\n",
            "rho0: 0.6744, rho1: 0.2000\n",
            "Epoch 54, Loss: 0.7427, rho_lr: 0.002000\n",
            "rho0: 0.6749, rho1: 0.2000\n",
            "Epoch 55, Loss: 0.7459, rho_lr: 0.002000\n",
            "rho0: 0.6753, rho1: 0.2000\n",
            "Epoch 56, Loss: 0.7361, rho_lr: 0.002000\n",
            "rho0: 0.6758, rho1: 0.2000\n",
            "Epoch 57, Loss: 0.7317, rho_lr: 0.002000\n",
            "rho0: 0.6762, rho1: 0.2000\n",
            "Epoch 58, Loss: 0.7277, rho_lr: 0.002000\n",
            "rho0: 0.6767, rho1: 0.2000\n",
            "Epoch 59, Loss: 0.7232, rho_lr: 0.002000\n",
            "rho0: 0.6772, rho1: 0.2000\n",
            "Epoch 60, Loss: 0.7197, rho_lr: 0.000200\n",
            "rho0: 0.6777, rho1: 0.2000\n",
            "Epoch 1, Loss: 37.0127, rho_lr: 0.200000\n",
            "rho0: 0.0000, rho1: 0.5000\n",
            "Epoch 2, Loss: 14.6818, rho_lr: 0.200000\n",
            "rho0: 0.1641, rho1: 0.5000\n",
            "Epoch 3, Loss: 9.2542, rho_lr: 0.200000\n",
            "rho0: 0.2433, rho1: 0.5000\n",
            "Epoch 4, Loss: 6.4224, rho_lr: 0.200000\n",
            "rho0: 0.2585, rho1: 0.5000\n",
            "Epoch 5, Loss: 4.5848, rho_lr: 0.200000\n",
            "rho0: 0.3006, rho1: 0.5000\n",
            "Epoch 6, Loss: 3.4007, rho_lr: 0.200000\n",
            "rho0: 0.2891, rho1: 0.5000\n",
            "Epoch 7, Loss: 2.8778, rho_lr: 0.200000\n",
            "rho0: 0.3002, rho1: 0.5000\n",
            "Epoch 8, Loss: 2.5335, rho_lr: 0.200000\n",
            "rho0: 0.3003, rho1: 0.5000\n",
            "Epoch 9, Loss: 2.3530, rho_lr: 0.200000\n",
            "rho0: 0.3055, rho1: 0.5000\n",
            "Epoch 10, Loss: 2.2298, rho_lr: 0.200000\n",
            "rho0: 0.3080, rho1: 0.5000\n",
            "Epoch 11, Loss: 2.2921, rho_lr: 0.200000\n",
            "rho0: 0.3578, rho1: 0.4254\n",
            "Epoch 12, Loss: 1.7498, rho_lr: 0.200000\n",
            "rho0: 0.4669, rho1: 0.2509\n",
            "Epoch 13, Loss: 1.2211, rho_lr: 0.200000\n",
            "rho0: 0.5666, rho1: 0.2000\n",
            "Epoch 14, Loss: 1.1512, rho_lr: 0.200000\n",
            "rho0: 0.5385, rho1: 0.2000\n",
            "Epoch 15, Loss: 1.0964, rho_lr: 0.200000\n",
            "rho0: 0.5609, rho1: 0.2000\n",
            "Epoch 16, Loss: 1.0660, rho_lr: 0.200000\n",
            "rho0: 0.5695, rho1: 0.2000\n",
            "Epoch 17, Loss: 1.0465, rho_lr: 0.200000\n",
            "rho0: 0.5759, rho1: 0.2000\n",
            "Epoch 18, Loss: 1.0272, rho_lr: 0.200000\n",
            "rho0: 0.5814, rho1: 0.2000\n",
            "Epoch 19, Loss: 1.0080, rho_lr: 0.200000\n",
            "rho0: 0.5888, rho1: 0.2000\n",
            "Epoch 20, Loss: 0.9982, rho_lr: 0.020000\n",
            "rho0: 0.5925, rho1: 0.2000\n",
            "Epoch 21, Loss: 0.9871, rho_lr: 0.020000\n",
            "rho0: 0.5938, rho1: 0.2000\n",
            "Epoch 22, Loss: 0.9681, rho_lr: 0.020000\n",
            "rho0: 0.5963, rho1: 0.2000\n",
            "Epoch 23, Loss: 0.9493, rho_lr: 0.020000\n",
            "rho0: 0.5995, rho1: 0.2000\n",
            "Epoch 24, Loss: 0.9303, rho_lr: 0.020000\n",
            "rho0: 0.6030, rho1: 0.2000\n",
            "Epoch 25, Loss: 0.9165, rho_lr: 0.020000\n",
            "rho0: 0.6066, rho1: 0.2000\n",
            "Epoch 26, Loss: 0.9149, rho_lr: 0.020000\n",
            "rho0: 0.6101, rho1: 0.2000\n",
            "Epoch 27, Loss: 0.8926, rho_lr: 0.020000\n",
            "rho0: 0.6138, rho1: 0.2000\n",
            "Epoch 28, Loss: 0.8875, rho_lr: 0.020000\n",
            "rho0: 0.6172, rho1: 0.2000\n",
            "Epoch 29, Loss: 0.8725, rho_lr: 0.020000\n",
            "rho0: 0.6204, rho1: 0.2000\n",
            "Epoch 30, Loss: 0.8740, rho_lr: 0.020000\n",
            "rho0: 0.6242, rho1: 0.2000\n",
            "Epoch 31, Loss: 0.8543, rho_lr: 0.020000\n",
            "rho0: 0.6280, rho1: 0.2000\n",
            "Epoch 32, Loss: 0.8421, rho_lr: 0.020000\n",
            "rho0: 0.6313, rho1: 0.2000\n",
            "Epoch 33, Loss: 0.8467, rho_lr: 0.020000\n",
            "rho0: 0.6350, rho1: 0.2000\n",
            "Epoch 34, Loss: 0.8229, rho_lr: 0.020000\n",
            "rho0: 0.6386, rho1: 0.2000\n",
            "Epoch 35, Loss: 0.8162, rho_lr: 0.020000\n",
            "rho0: 0.6414, rho1: 0.2000\n",
            "Epoch 36, Loss: 0.8102, rho_lr: 0.020000\n",
            "rho0: 0.6448, rho1: 0.2000\n",
            "Epoch 37, Loss: 0.8065, rho_lr: 0.020000\n",
            "rho0: 0.6481, rho1: 0.2000\n",
            "Epoch 38, Loss: 0.7909, rho_lr: 0.020000\n",
            "rho0: 0.6514, rho1: 0.2000\n",
            "Epoch 39, Loss: 0.7843, rho_lr: 0.020000\n",
            "rho0: 0.6550, rho1: 0.2000\n",
            "Epoch 40, Loss: 0.7827, rho_lr: 0.002000\n",
            "rho0: 0.6582, rho1: 0.2000\n",
            "Epoch 41, Loss: 0.7724, rho_lr: 0.002000\n",
            "rho0: 0.6585, rho1: 0.2000\n",
            "Epoch 42, Loss: 0.7701, rho_lr: 0.002000\n",
            "rho0: 0.6589, rho1: 0.2000\n",
            "Epoch 43, Loss: 0.7582, rho_lr: 0.002000\n",
            "rho0: 0.6594, rho1: 0.2000\n",
            "Epoch 44, Loss: 0.7499, rho_lr: 0.002000\n",
            "rho0: 0.6600, rho1: 0.2000\n",
            "Epoch 45, Loss: 0.7470, rho_lr: 0.002000\n",
            "rho0: 0.6605, rho1: 0.2000\n",
            "Epoch 46, Loss: 0.7461, rho_lr: 0.002000\n",
            "rho0: 0.6611, rho1: 0.2000\n",
            "Epoch 47, Loss: 0.7382, rho_lr: 0.002000\n",
            "rho0: 0.6616, rho1: 0.2000\n",
            "Epoch 48, Loss: 0.7331, rho_lr: 0.002000\n",
            "rho0: 0.6622, rho1: 0.2000\n",
            "Epoch 49, Loss: 0.7319, rho_lr: 0.002000\n",
            "rho0: 0.6628, rho1: 0.2000\n",
            "Epoch 50, Loss: 0.7270, rho_lr: 0.002000\n",
            "rho0: 0.6635, rho1: 0.2000\n",
            "Epoch 51, Loss: 0.7240, rho_lr: 0.002000\n",
            "rho0: 0.6641, rho1: 0.2000\n",
            "Epoch 52, Loss: 0.7116, rho_lr: 0.002000\n",
            "rho0: 0.6648, rho1: 0.2000\n",
            "Epoch 53, Loss: 0.7157, rho_lr: 0.002000\n",
            "rho0: 0.6655, rho1: 0.2000\n",
            "Epoch 54, Loss: 0.7097, rho_lr: 0.002000\n",
            "rho0: 0.6662, rho1: 0.2000\n",
            "Epoch 55, Loss: 0.6989, rho_lr: 0.002000\n",
            "rho0: 0.6668, rho1: 0.2000\n",
            "Epoch 56, Loss: 0.6968, rho_lr: 0.002000\n",
            "rho0: 0.6675, rho1: 0.2000\n",
            "Epoch 57, Loss: 0.6973, rho_lr: 0.002000\n",
            "rho0: 0.6682, rho1: 0.2000\n",
            "Epoch 58, Loss: 0.6936, rho_lr: 0.002000\n",
            "rho0: 0.6688, rho1: 0.2000\n",
            "Epoch 59, Loss: 0.6918, rho_lr: 0.002000\n",
            "rho0: 0.6695, rho1: 0.2000\n",
            "Epoch 60, Loss: 0.6844, rho_lr: 0.000200\n",
            "rho0: 0.6702, rho1: 0.2000\n",
            "Epoch 1, Loss: 37.3289, rho_lr: 0.200000\n",
            "rho0: 0.0000, rho1: 0.5000\n",
            "Epoch 2, Loss: 13.2946, rho_lr: 0.200000\n",
            "rho0: 0.3017, rho1: 0.5000\n",
            "Epoch 3, Loss: 6.6577, rho_lr: 0.200000\n",
            "rho0: 0.2893, rho1: 0.5000\n",
            "Epoch 4, Loss: 4.2872, rho_lr: 0.200000\n",
            "rho0: 0.3162, rho1: 0.5000\n",
            "Epoch 5, Loss: 2.9853, rho_lr: 0.200000\n",
            "rho0: 0.3508, rho1: 0.5000\n",
            "Epoch 6, Loss: 2.5750, rho_lr: 0.200000\n",
            "rho0: 0.3340, rho1: 0.5000\n",
            "Epoch 7, Loss: 2.2528, rho_lr: 0.200000\n",
            "rho0: 0.3330, rho1: 0.5000\n",
            "Epoch 8, Loss: 2.0694, rho_lr: 0.200000\n",
            "rho0: 0.3358, rho1: 0.5000\n",
            "Epoch 9, Loss: 1.9805, rho_lr: 0.200000\n",
            "rho0: 0.3367, rho1: 0.5000\n",
            "Epoch 10, Loss: 1.8977, rho_lr: 0.200000\n",
            "rho0: 0.3321, rho1: 0.5000\n",
            "Epoch 11, Loss: 2.0143, rho_lr: 0.200000\n",
            "rho0: 0.3748, rho1: 0.3915\n",
            "Epoch 12, Loss: 1.5417, rho_lr: 0.200000\n",
            "rho0: 0.4720, rho1: 0.3037\n",
            "Epoch 13, Loss: 1.1509, rho_lr: 0.200000\n",
            "rho0: 0.5692, rho1: 0.2000\n",
            "Epoch 14, Loss: 1.0110, rho_lr: 0.200000\n",
            "rho0: 0.5750, rho1: 0.2000\n",
            "Epoch 15, Loss: 0.9811, rho_lr: 0.200000\n",
            "rho0: 0.5716, rho1: 0.2000\n",
            "Epoch 16, Loss: 0.9517, rho_lr: 0.200000\n",
            "rho0: 0.5809, rho1: 0.2000\n",
            "Epoch 17, Loss: 0.9544, rho_lr: 0.200000\n",
            "rho0: 0.5848, rho1: 0.2000\n",
            "Epoch 18, Loss: 0.9344, rho_lr: 0.200000\n",
            "rho0: 0.5886, rho1: 0.2000\n",
            "Epoch 19, Loss: 0.9160, rho_lr: 0.200000\n",
            "rho0: 0.5957, rho1: 0.2000\n",
            "Epoch 20, Loss: 0.8976, rho_lr: 0.020000\n",
            "rho0: 0.5980, rho1: 0.2000\n",
            "Epoch 21, Loss: 0.8935, rho_lr: 0.020000\n",
            "rho0: 0.5992, rho1: 0.2000\n",
            "Epoch 22, Loss: 0.8819, rho_lr: 0.020000\n",
            "rho0: 0.6009, rho1: 0.2000\n",
            "Epoch 23, Loss: 0.8802, rho_lr: 0.020000\n",
            "rho0: 0.6033, rho1: 0.2000\n",
            "Epoch 24, Loss: 0.8636, rho_lr: 0.020000\n",
            "rho0: 0.6053, rho1: 0.2000\n",
            "Epoch 25, Loss: 0.8618, rho_lr: 0.020000\n",
            "rho0: 0.6079, rho1: 0.2000\n",
            "Epoch 26, Loss: 0.8439, rho_lr: 0.020000\n",
            "rho0: 0.6108, rho1: 0.2000\n",
            "Epoch 27, Loss: 0.8294, rho_lr: 0.020000\n",
            "rho0: 0.6134, rho1: 0.2000\n",
            "Epoch 28, Loss: 0.8318, rho_lr: 0.020000\n",
            "rho0: 0.6162, rho1: 0.2000\n",
            "Epoch 29, Loss: 0.8176, rho_lr: 0.020000\n",
            "rho0: 0.6185, rho1: 0.2000\n",
            "Epoch 30, Loss: 0.8118, rho_lr: 0.020000\n",
            "rho0: 0.6208, rho1: 0.2000\n",
            "Epoch 31, Loss: 0.8064, rho_lr: 0.020000\n",
            "rho0: 0.6235, rho1: 0.2000\n",
            "Epoch 32, Loss: 0.7925, rho_lr: 0.020000\n",
            "rho0: 0.6262, rho1: 0.2000\n",
            "Epoch 33, Loss: 0.7861, rho_lr: 0.020000\n",
            "rho0: 0.6289, rho1: 0.2000\n",
            "Epoch 34, Loss: 0.7774, rho_lr: 0.020000\n",
            "rho0: 0.6314, rho1: 0.2000\n",
            "Epoch 35, Loss: 0.7777, rho_lr: 0.020000\n",
            "rho0: 0.6347, rho1: 0.2000\n",
            "Epoch 36, Loss: 0.7666, rho_lr: 0.020000\n",
            "rho0: 0.6371, rho1: 0.2000\n",
            "Epoch 37, Loss: 0.7664, rho_lr: 0.020000\n",
            "rho0: 0.6397, rho1: 0.2000\n",
            "Epoch 38, Loss: 0.7575, rho_lr: 0.020000\n",
            "rho0: 0.6426, rho1: 0.2000\n",
            "Epoch 39, Loss: 0.7521, rho_lr: 0.020000\n",
            "rho0: 0.6448, rho1: 0.2000\n",
            "Epoch 40, Loss: 0.7474, rho_lr: 0.002000\n",
            "rho0: 0.6473, rho1: 0.2000\n",
            "Epoch 41, Loss: 0.7345, rho_lr: 0.002000\n",
            "rho0: 0.6475, rho1: 0.2000\n",
            "Epoch 42, Loss: 0.7377, rho_lr: 0.002000\n",
            "rho0: 0.6479, rho1: 0.2000\n",
            "Epoch 43, Loss: 0.7295, rho_lr: 0.002000\n",
            "rho0: 0.6483, rho1: 0.2000\n",
            "Epoch 44, Loss: 0.7240, rho_lr: 0.002000\n",
            "rho0: 0.6488, rho1: 0.2000\n",
            "Epoch 45, Loss: 0.7242, rho_lr: 0.002000\n",
            "rho0: 0.6493, rho1: 0.2000\n",
            "Epoch 46, Loss: 0.7160, rho_lr: 0.002000\n",
            "rho0: 0.6498, rho1: 0.2000\n",
            "Epoch 47, Loss: 0.7142, rho_lr: 0.002000\n",
            "rho0: 0.6503, rho1: 0.2000\n",
            "Epoch 48, Loss: 0.7037, rho_lr: 0.002000\n",
            "rho0: 0.6509, rho1: 0.2000\n",
            "Epoch 49, Loss: 0.7022, rho_lr: 0.002000\n",
            "rho0: 0.6514, rho1: 0.2000\n",
            "Epoch 50, Loss: 0.7011, rho_lr: 0.002000\n",
            "rho0: 0.6520, rho1: 0.2000\n",
            "Epoch 51, Loss: 0.7019, rho_lr: 0.002000\n",
            "rho0: 0.6525, rho1: 0.2000\n",
            "Epoch 52, Loss: 0.6934, rho_lr: 0.002000\n",
            "rho0: 0.6531, rho1: 0.2000\n",
            "Epoch 53, Loss: 0.6913, rho_lr: 0.002000\n",
            "rho0: 0.6537, rho1: 0.2000\n",
            "Epoch 54, Loss: 0.6859, rho_lr: 0.002000\n",
            "rho0: 0.6542, rho1: 0.2000\n",
            "Epoch 55, Loss: 0.6828, rho_lr: 0.002000\n",
            "rho0: 0.6548, rho1: 0.2000\n",
            "Epoch 56, Loss: 0.6846, rho_lr: 0.002000\n",
            "rho0: 0.6554, rho1: 0.2000\n",
            "Epoch 57, Loss: 0.6783, rho_lr: 0.002000\n",
            "rho0: 0.6560, rho1: 0.2000\n",
            "Epoch 58, Loss: 0.6753, rho_lr: 0.002000\n",
            "rho0: 0.6566, rho1: 0.2000\n",
            "Epoch 59, Loss: 0.6695, rho_lr: 0.002000\n",
            "rho0: 0.6572, rho1: 0.2000\n",
            "Epoch 60, Loss: 0.6671, rho_lr: 0.000200\n",
            "rho0: 0.6578, rho1: 0.2000\n",
            "Epoch 1, Loss: 57.0359, rho_lr: 0.200000\n",
            "rho0: 0.0000, rho1: 0.5000\n",
            "Epoch 2, Loss: 15.0338, rho_lr: 0.200000\n",
            "rho0: 0.2335, rho1: 0.5000\n",
            "Epoch 3, Loss: 9.5357, rho_lr: 0.200000\n",
            "rho0: 0.2623, rho1: 0.5000\n",
            "Epoch 4, Loss: 6.2353, rho_lr: 0.200000\n",
            "rho0: 0.3077, rho1: 0.5000\n",
            "Epoch 5, Loss: 4.7644, rho_lr: 0.200000\n",
            "rho0: 0.2938, rho1: 0.5000\n",
            "Epoch 6, Loss: 3.6532, rho_lr: 0.200000\n",
            "rho0: 0.3194, rho1: 0.5000\n",
            "Epoch 7, Loss: 3.0005, rho_lr: 0.200000\n",
            "rho0: 0.3121, rho1: 0.5000\n",
            "Epoch 8, Loss: 2.8035, rho_lr: 0.200000\n",
            "rho0: 0.3088, rho1: 0.5000\n",
            "Epoch 9, Loss: 2.6009, rho_lr: 0.200000\n",
            "rho0: 0.3035, rho1: 0.5000\n",
            "Epoch 10, Loss: 2.4597, rho_lr: 0.200000\n",
            "rho0: 0.3145, rho1: 0.5000\n",
            "Epoch 11, Loss: 2.5325, rho_lr: 0.200000\n",
            "rho0: 0.3461, rho1: 0.4012\n",
            "Epoch 12, Loss: 2.0792, rho_lr: 0.200000\n",
            "rho0: 0.4284, rho1: 0.3097\n",
            "Epoch 13, Loss: 1.4787, rho_lr: 0.200000\n",
            "rho0: 0.5263, rho1: 0.2000\n",
            "Epoch 14, Loss: 1.2603, rho_lr: 0.200000\n",
            "rho0: 0.5592, rho1: 0.2000\n",
            "Epoch 15, Loss: 1.1919, rho_lr: 0.200000\n",
            "rho0: 0.5547, rho1: 0.2000\n",
            "Epoch 16, Loss: 1.1573, rho_lr: 0.200000\n",
            "rho0: 0.5728, rho1: 0.2000\n",
            "Epoch 17, Loss: 1.1386, rho_lr: 0.200000\n",
            "rho0: 0.5774, rho1: 0.2000\n",
            "Epoch 18, Loss: 1.1170, rho_lr: 0.200000\n",
            "rho0: 0.5865, rho1: 0.2000\n",
            "Epoch 19, Loss: 1.0824, rho_lr: 0.200000\n",
            "rho0: 0.5921, rho1: 0.2000\n",
            "Epoch 20, Loss: 1.0712, rho_lr: 0.020000\n",
            "rho0: 0.5990, rho1: 0.2000\n",
            "Epoch 21, Loss: 1.0492, rho_lr: 0.020000\n",
            "rho0: 0.6000, rho1: 0.2000\n",
            "Epoch 22, Loss: 1.0217, rho_lr: 0.020000\n",
            "rho0: 0.6016, rho1: 0.2000\n",
            "Epoch 23, Loss: 1.0119, rho_lr: 0.020000\n",
            "rho0: 0.6039, rho1: 0.2000\n",
            "Epoch 24, Loss: 1.0110, rho_lr: 0.020000\n",
            "rho0: 0.6064, rho1: 0.2000\n",
            "Epoch 25, Loss: 0.9929, rho_lr: 0.020000\n",
            "rho0: 0.6089, rho1: 0.2000\n",
            "Epoch 26, Loss: 0.9822, rho_lr: 0.020000\n",
            "rho0: 0.6115, rho1: 0.2000\n",
            "Epoch 27, Loss: 0.9639, rho_lr: 0.020000\n",
            "rho0: 0.6142, rho1: 0.2000\n",
            "Epoch 28, Loss: 0.9546, rho_lr: 0.020000\n",
            "rho0: 0.6167, rho1: 0.2000\n",
            "Epoch 29, Loss: 0.9361, rho_lr: 0.020000\n",
            "rho0: 0.6194, rho1: 0.2000\n",
            "Epoch 30, Loss: 0.9345, rho_lr: 0.020000\n",
            "rho0: 0.6221, rho1: 0.2000\n",
            "Epoch 31, Loss: 0.9136, rho_lr: 0.020000\n",
            "rho0: 0.6251, rho1: 0.2000\n",
            "Epoch 32, Loss: 0.9081, rho_lr: 0.020000\n",
            "rho0: 0.6280, rho1: 0.2000\n",
            "Epoch 33, Loss: 0.8978, rho_lr: 0.020000\n",
            "rho0: 0.6307, rho1: 0.2000\n",
            "Epoch 34, Loss: 0.8910, rho_lr: 0.020000\n",
            "rho0: 0.6334, rho1: 0.2000\n",
            "Epoch 35, Loss: 0.8872, rho_lr: 0.020000\n",
            "rho0: 0.6363, rho1: 0.2000\n",
            "Epoch 36, Loss: 0.8651, rho_lr: 0.020000\n",
            "rho0: 0.6393, rho1: 0.2000\n",
            "Epoch 37, Loss: 0.8700, rho_lr: 0.020000\n",
            "rho0: 0.6424, rho1: 0.2000\n",
            "Epoch 38, Loss: 0.8534, rho_lr: 0.020000\n",
            "rho0: 0.6453, rho1: 0.2000\n",
            "Epoch 39, Loss: 0.8361, rho_lr: 0.020000\n",
            "rho0: 0.6480, rho1: 0.2000\n",
            "Epoch 40, Loss: 0.8306, rho_lr: 0.002000\n",
            "rho0: 0.6510, rho1: 0.2000\n",
            "Epoch 41, Loss: 0.8248, rho_lr: 0.002000\n",
            "rho0: 0.6514, rho1: 0.2000\n",
            "Epoch 42, Loss: 0.8090, rho_lr: 0.002000\n",
            "rho0: 0.6517, rho1: 0.2000\n",
            "Epoch 43, Loss: 0.8124, rho_lr: 0.002000\n",
            "rho0: 0.6521, rho1: 0.2000\n",
            "Epoch 44, Loss: 0.8028, rho_lr: 0.002000\n",
            "rho0: 0.6524, rho1: 0.2000\n",
            "Epoch 45, Loss: 0.7978, rho_lr: 0.002000\n",
            "rho0: 0.6528, rho1: 0.2000\n",
            "Epoch 46, Loss: 0.7855, rho_lr: 0.002000\n",
            "rho0: 0.6533, rho1: 0.2000\n",
            "Epoch 47, Loss: 0.7901, rho_lr: 0.002000\n",
            "rho0: 0.6537, rho1: 0.2000\n",
            "Epoch 48, Loss: 0.7822, rho_lr: 0.002000\n",
            "rho0: 0.6542, rho1: 0.2000\n",
            "Epoch 49, Loss: 0.7741, rho_lr: 0.002000\n",
            "rho0: 0.6546, rho1: 0.2000\n",
            "Epoch 50, Loss: 0.7687, rho_lr: 0.002000\n",
            "rho0: 0.6551, rho1: 0.2000\n",
            "Epoch 51, Loss: 0.7608, rho_lr: 0.002000\n",
            "rho0: 0.6556, rho1: 0.2000\n",
            "Epoch 52, Loss: 0.7565, rho_lr: 0.002000\n",
            "rho0: 0.6561, rho1: 0.2000\n",
            "Epoch 53, Loss: 0.7526, rho_lr: 0.002000\n",
            "rho0: 0.6565, rho1: 0.2000\n",
            "Epoch 54, Loss: 0.7480, rho_lr: 0.002000\n",
            "rho0: 0.6570, rho1: 0.2000\n",
            "Epoch 55, Loss: 0.7467, rho_lr: 0.002000\n",
            "rho0: 0.6575, rho1: 0.2000\n",
            "Epoch 56, Loss: 0.7413, rho_lr: 0.002000\n",
            "rho0: 0.6580, rho1: 0.2000\n",
            "Epoch 57, Loss: 0.7374, rho_lr: 0.002000\n",
            "rho0: 0.6585, rho1: 0.2000\n",
            "Epoch 58, Loss: 0.7263, rho_lr: 0.002000\n",
            "rho0: 0.6590, rho1: 0.2000\n",
            "Epoch 59, Loss: 0.7256, rho_lr: 0.002000\n",
            "rho0: 0.6595, rho1: 0.2000\n",
            "Epoch 60, Loss: 0.7236, rho_lr: 0.000200\n",
            "rho0: 0.6600, rho1: 0.2000\n",
            "Epoch 1, Loss: 41.1657, rho_lr: 0.200000\n",
            "rho0: 0.0000, rho1: 0.5000\n",
            "Epoch 2, Loss: 16.5572, rho_lr: 0.200000\n",
            "rho0: 0.1587, rho1: 0.5000\n",
            "Epoch 3, Loss: 10.5144, rho_lr: 0.200000\n",
            "rho0: 0.3577, rho1: 0.5000\n",
            "Epoch 4, Loss: 7.5389, rho_lr: 0.200000\n",
            "rho0: 0.2900, rho1: 0.5000\n",
            "Epoch 5, Loss: 4.9581, rho_lr: 0.200000\n",
            "rho0: 0.3147, rho1: 0.5000\n",
            "Epoch 6, Loss: 3.9125, rho_lr: 0.200000\n",
            "rho0: 0.3346, rho1: 0.5000\n",
            "Epoch 7, Loss: 3.0603, rho_lr: 0.200000\n",
            "rho0: 0.3253, rho1: 0.5000\n",
            "Epoch 8, Loss: 2.7791, rho_lr: 0.200000\n",
            "rho0: 0.3348, rho1: 0.5000\n",
            "Epoch 9, Loss: 2.5274, rho_lr: 0.200000\n",
            "rho0: 0.3306, rho1: 0.5000\n",
            "Epoch 10, Loss: 2.2744, rho_lr: 0.200000\n",
            "rho0: 0.3490, rho1: 0.5000\n",
            "Epoch 11, Loss: 2.2480, rho_lr: 0.200000\n",
            "rho0: 0.4064, rho1: 0.3215\n",
            "Epoch 12, Loss: 1.5612, rho_lr: 0.200000\n",
            "rho0: 0.5400, rho1: 0.2166\n",
            "Epoch 13, Loss: 1.2174, rho_lr: 0.200000\n",
            "rho0: 0.5963, rho1: 0.2000\n",
            "Epoch 14, Loss: 1.1810, rho_lr: 0.200000\n",
            "rho0: 0.5780, rho1: 0.2000\n",
            "Epoch 15, Loss: 1.1039, rho_lr: 0.200000\n",
            "rho0: 0.6045, rho1: 0.2000\n",
            "Epoch 16, Loss: 1.0845, rho_lr: 0.200000\n",
            "rho0: 0.6025, rho1: 0.2000\n",
            "Epoch 17, Loss: 1.0382, rho_lr: 0.200000\n",
            "rho0: 0.6205, rho1: 0.2000\n",
            "Epoch 18, Loss: 1.0379, rho_lr: 0.200000\n",
            "rho0: 0.6193, rho1: 0.2000\n",
            "Epoch 19, Loss: 1.0365, rho_lr: 0.200000\n",
            "rho0: 0.6241, rho1: 0.2000\n",
            "Epoch 20, Loss: 1.0145, rho_lr: 0.020000\n",
            "rho0: 0.6328, rho1: 0.2000\n",
            "Epoch 21, Loss: 0.9883, rho_lr: 0.020000\n",
            "rho0: 0.6333, rho1: 0.2000\n",
            "Epoch 22, Loss: 0.9745, rho_lr: 0.020000\n",
            "rho0: 0.6351, rho1: 0.2000\n",
            "Epoch 23, Loss: 0.9659, rho_lr: 0.020000\n",
            "rho0: 0.6380, rho1: 0.2000\n",
            "Epoch 24, Loss: 0.9541, rho_lr: 0.020000\n",
            "rho0: 0.6413, rho1: 0.2000\n",
            "Epoch 25, Loss: 0.9411, rho_lr: 0.020000\n",
            "rho0: 0.6440, rho1: 0.2000\n",
            "Epoch 26, Loss: 0.9219, rho_lr: 0.020000\n",
            "rho0: 0.6472, rho1: 0.2000\n",
            "Epoch 27, Loss: 0.9159, rho_lr: 0.020000\n",
            "rho0: 0.6498, rho1: 0.2000\n",
            "Epoch 28, Loss: 0.9028, rho_lr: 0.020000\n",
            "rho0: 0.6527, rho1: 0.2000\n",
            "Epoch 29, Loss: 0.8888, rho_lr: 0.020000\n",
            "rho0: 0.6561, rho1: 0.2000\n",
            "Epoch 30, Loss: 0.8806, rho_lr: 0.020000\n",
            "rho0: 0.6597, rho1: 0.2000\n",
            "Epoch 31, Loss: 0.8687, rho_lr: 0.020000\n",
            "rho0: 0.6624, rho1: 0.2000\n",
            "Epoch 32, Loss: 0.8616, rho_lr: 0.020000\n",
            "rho0: 0.6643, rho1: 0.2000\n",
            "Epoch 33, Loss: 0.8580, rho_lr: 0.020000\n",
            "rho0: 0.6670, rho1: 0.2000\n",
            "Epoch 34, Loss: 0.8353, rho_lr: 0.020000\n",
            "rho0: 0.6699, rho1: 0.2000\n",
            "Epoch 35, Loss: 0.8267, rho_lr: 0.020000\n",
            "rho0: 0.6728, rho1: 0.2000\n",
            "Epoch 36, Loss: 0.8211, rho_lr: 0.020000\n",
            "rho0: 0.6762, rho1: 0.2000\n",
            "Epoch 37, Loss: 0.8144, rho_lr: 0.020000\n",
            "rho0: 0.6789, rho1: 0.2000\n",
            "Epoch 38, Loss: 0.8186, rho_lr: 0.020000\n",
            "rho0: 0.6818, rho1: 0.2000\n",
            "Epoch 39, Loss: 0.8064, rho_lr: 0.020000\n",
            "rho0: 0.6839, rho1: 0.2000\n",
            "Epoch 40, Loss: 0.7912, rho_lr: 0.002000\n",
            "rho0: 0.6865, rho1: 0.2000\n",
            "Epoch 41, Loss: 0.7880, rho_lr: 0.002000\n",
            "rho0: 0.6868, rho1: 0.2000\n",
            "Epoch 42, Loss: 0.7791, rho_lr: 0.002000\n",
            "rho0: 0.6873, rho1: 0.2000\n",
            "Epoch 43, Loss: 0.7773, rho_lr: 0.002000\n",
            "rho0: 0.6878, rho1: 0.2000\n",
            "Epoch 44, Loss: 0.7655, rho_lr: 0.002000\n",
            "rho0: 0.6882, rho1: 0.2000\n",
            "Epoch 45, Loss: 0.7640, rho_lr: 0.002000\n",
            "rho0: 0.6887, rho1: 0.2000\n",
            "Epoch 46, Loss: 0.7676, rho_lr: 0.002000\n",
            "rho0: 0.6892, rho1: 0.2000\n",
            "Epoch 47, Loss: 0.7561, rho_lr: 0.002000\n",
            "rho0: 0.6897, rho1: 0.2000\n",
            "Epoch 48, Loss: 0.7548, rho_lr: 0.002000\n",
            "rho0: 0.6902, rho1: 0.2000\n",
            "Epoch 49, Loss: 0.7397, rho_lr: 0.002000\n",
            "rho0: 0.6908, rho1: 0.2000\n",
            "Epoch 50, Loss: 0.7386, rho_lr: 0.002000\n",
            "rho0: 0.6914, rho1: 0.2000\n",
            "Epoch 51, Loss: 0.7402, rho_lr: 0.002000\n",
            "rho0: 0.6919, rho1: 0.2000\n",
            "Epoch 52, Loss: 0.7328, rho_lr: 0.002000\n",
            "rho0: 0.6924, rho1: 0.2000\n",
            "Epoch 53, Loss: 0.7270, rho_lr: 0.002000\n",
            "rho0: 0.6930, rho1: 0.2000\n",
            "Epoch 54, Loss: 0.7259, rho_lr: 0.002000\n",
            "rho0: 0.6936, rho1: 0.2000\n",
            "Epoch 55, Loss: 0.7240, rho_lr: 0.002000\n",
            "rho0: 0.6941, rho1: 0.2000\n",
            "Epoch 56, Loss: 0.7079, rho_lr: 0.002000\n",
            "rho0: 0.6947, rho1: 0.2000\n",
            "Epoch 57, Loss: 0.7129, rho_lr: 0.002000\n",
            "rho0: 0.6952, rho1: 0.2000\n",
            "Epoch 58, Loss: 0.7089, rho_lr: 0.002000\n",
            "rho0: 0.6958, rho1: 0.2000\n",
            "Epoch 59, Loss: 0.7041, rho_lr: 0.002000\n",
            "rho0: 0.6964, rho1: 0.2000\n",
            "Epoch 60, Loss: 0.6974, rho_lr: 0.000200\n",
            "rho0: 0.6971, rho1: 0.2000\n",
            "Epoch 1, Loss: 37.3710, rho_lr: 0.200000\n",
            "rho0: 0.0263, rho1: 0.5000\n",
            "Epoch 2, Loss: 11.8096, rho_lr: 0.200000\n",
            "rho0: 0.1150, rho1: 0.5000\n",
            "Epoch 3, Loss: 9.1078, rho_lr: 0.200000\n",
            "rho0: 0.2217, rho1: 0.5000\n",
            "Epoch 4, Loss: 6.1643, rho_lr: 0.200000\n",
            "rho0: 0.2790, rho1: 0.5000\n",
            "Epoch 5, Loss: 4.6282, rho_lr: 0.200000\n",
            "rho0: 0.2956, rho1: 0.5000\n",
            "Epoch 6, Loss: 3.6670, rho_lr: 0.200000\n",
            "rho0: 0.3242, rho1: 0.5000\n",
            "Epoch 7, Loss: 3.0310, rho_lr: 0.200000\n",
            "rho0: 0.3138, rho1: 0.5000\n",
            "Epoch 8, Loss: 2.7261, rho_lr: 0.200000\n",
            "rho0: 0.3124, rho1: 0.5000\n",
            "Epoch 9, Loss: 2.4649, rho_lr: 0.200000\n",
            "rho0: 0.3110, rho1: 0.5000\n",
            "Epoch 10, Loss: 2.3101, rho_lr: 0.200000\n",
            "rho0: 0.3166, rho1: 0.5000\n",
            "Epoch 11, Loss: 2.2666, rho_lr: 0.200000\n",
            "rho0: 0.3860, rho1: 0.3827\n",
            "Epoch 12, Loss: 1.6325, rho_lr: 0.200000\n",
            "rho0: 0.5009, rho1: 0.2000\n",
            "Epoch 13, Loss: 1.2245, rho_lr: 0.200000\n",
            "rho0: 0.5575, rho1: 0.2000\n",
            "Epoch 14, Loss: 1.1756, rho_lr: 0.200000\n",
            "rho0: 0.5493, rho1: 0.2000\n",
            "Epoch 15, Loss: 1.1112, rho_lr: 0.200000\n",
            "rho0: 0.5557, rho1: 0.2000\n",
            "Epoch 16, Loss: 1.0996, rho_lr: 0.200000\n",
            "rho0: 0.5755, rho1: 0.2000\n",
            "Epoch 17, Loss: 1.0768, rho_lr: 0.200000\n",
            "rho0: 0.5746, rho1: 0.2000\n",
            "Epoch 18, Loss: 1.0662, rho_lr: 0.200000\n",
            "rho0: 0.5783, rho1: 0.2000\n",
            "Epoch 19, Loss: 1.0453, rho_lr: 0.200000\n",
            "rho0: 0.5865, rho1: 0.2000\n",
            "Epoch 20, Loss: 1.0240, rho_lr: 0.020000\n",
            "rho0: 0.5918, rho1: 0.2000\n",
            "Epoch 21, Loss: 1.0139, rho_lr: 0.020000\n",
            "rho0: 0.5921, rho1: 0.2000\n",
            "Epoch 22, Loss: 0.9870, rho_lr: 0.020000\n",
            "rho0: 0.5937, rho1: 0.2000\n",
            "Epoch 23, Loss: 0.9855, rho_lr: 0.020000\n",
            "rho0: 0.5963, rho1: 0.2000\n",
            "Epoch 24, Loss: 0.9678, rho_lr: 0.020000\n",
            "rho0: 0.5990, rho1: 0.2000\n",
            "Epoch 25, Loss: 0.9588, rho_lr: 0.020000\n",
            "rho0: 0.6025, rho1: 0.2000\n",
            "Epoch 26, Loss: 0.9394, rho_lr: 0.020000\n",
            "rho0: 0.6053, rho1: 0.2000\n",
            "Epoch 27, Loss: 0.9343, rho_lr: 0.020000\n",
            "rho0: 0.6083, rho1: 0.2000\n",
            "Epoch 28, Loss: 0.9293, rho_lr: 0.020000\n",
            "rho0: 0.6109, rho1: 0.2000\n",
            "Epoch 29, Loss: 0.9194, rho_lr: 0.020000\n",
            "rho0: 0.6141, rho1: 0.2000\n",
            "Epoch 30, Loss: 0.9128, rho_lr: 0.020000\n",
            "rho0: 0.6173, rho1: 0.2000\n",
            "Epoch 31, Loss: 0.8852, rho_lr: 0.020000\n",
            "rho0: 0.6205, rho1: 0.2000\n",
            "Epoch 32, Loss: 0.8822, rho_lr: 0.020000\n",
            "rho0: 0.6234, rho1: 0.2000\n",
            "Epoch 33, Loss: 0.8748, rho_lr: 0.020000\n",
            "rho0: 0.6265, rho1: 0.2000\n",
            "Epoch 34, Loss: 0.8657, rho_lr: 0.020000\n",
            "rho0: 0.6290, rho1: 0.2000\n",
            "Epoch 35, Loss: 0.8550, rho_lr: 0.020000\n",
            "rho0: 0.6314, rho1: 0.2000\n",
            "Epoch 36, Loss: 0.8491, rho_lr: 0.020000\n",
            "rho0: 0.6342, rho1: 0.2000\n",
            "Epoch 37, Loss: 0.8400, rho_lr: 0.020000\n",
            "rho0: 0.6370, rho1: 0.2000\n",
            "Epoch 38, Loss: 0.8398, rho_lr: 0.020000\n",
            "rho0: 0.6397, rho1: 0.2000\n",
            "Epoch 39, Loss: 0.8264, rho_lr: 0.020000\n",
            "rho0: 0.6432, rho1: 0.2000\n",
            "Epoch 40, Loss: 0.8226, rho_lr: 0.002000\n",
            "rho0: 0.6461, rho1: 0.2000\n",
            "Epoch 41, Loss: 0.8084, rho_lr: 0.002000\n",
            "rho0: 0.6464, rho1: 0.2000\n",
            "Epoch 42, Loss: 0.7983, rho_lr: 0.002000\n",
            "rho0: 0.6469, rho1: 0.2000\n",
            "Epoch 43, Loss: 0.7874, rho_lr: 0.002000\n",
            "rho0: 0.6473, rho1: 0.2000\n",
            "Epoch 44, Loss: 0.7902, rho_lr: 0.002000\n",
            "rho0: 0.6477, rho1: 0.2000\n",
            "Epoch 45, Loss: 0.7912, rho_lr: 0.002000\n",
            "rho0: 0.6482, rho1: 0.2000\n",
            "Epoch 46, Loss: 0.7781, rho_lr: 0.002000\n",
            "rho0: 0.6487, rho1: 0.2000\n",
            "Epoch 47, Loss: 0.7695, rho_lr: 0.002000\n",
            "rho0: 0.6492, rho1: 0.2000\n",
            "Epoch 48, Loss: 0.7671, rho_lr: 0.002000\n",
            "rho0: 0.6498, rho1: 0.2000\n",
            "Epoch 49, Loss: 0.7653, rho_lr: 0.002000\n",
            "rho0: 0.6503, rho1: 0.2000\n",
            "Epoch 50, Loss: 0.7582, rho_lr: 0.002000\n",
            "rho0: 0.6508, rho1: 0.2000\n",
            "Epoch 51, Loss: 0.7587, rho_lr: 0.002000\n",
            "rho0: 0.6514, rho1: 0.2000\n",
            "Epoch 52, Loss: 0.7531, rho_lr: 0.002000\n",
            "rho0: 0.6520, rho1: 0.2000\n",
            "Epoch 53, Loss: 0.7512, rho_lr: 0.002000\n",
            "rho0: 0.6526, rho1: 0.2000\n",
            "Epoch 54, Loss: 0.7455, rho_lr: 0.002000\n",
            "rho0: 0.6532, rho1: 0.2000\n",
            "Epoch 55, Loss: 0.7322, rho_lr: 0.002000\n",
            "rho0: 0.6538, rho1: 0.2000\n",
            "Epoch 56, Loss: 0.7331, rho_lr: 0.002000\n",
            "rho0: 0.6544, rho1: 0.2000\n",
            "Epoch 57, Loss: 0.7245, rho_lr: 0.002000\n",
            "rho0: 0.6550, rho1: 0.2000\n",
            "Epoch 58, Loss: 0.7213, rho_lr: 0.002000\n",
            "rho0: 0.6557, rho1: 0.2000\n",
            "Epoch 59, Loss: 0.7202, rho_lr: 0.002000\n",
            "rho0: 0.6564, rho1: 0.2000\n",
            "Epoch 60, Loss: 0.7194, rho_lr: 0.000200\n",
            "rho0: 0.6570, rho1: 0.2000\n",
            "Epoch 1, Loss: 40.3958, rho_lr: 0.200000\n",
            "rho0: 0.0000, rho1: 0.5000\n",
            "Epoch 2, Loss: 15.0724, rho_lr: 0.200000\n",
            "rho0: 0.2994, rho1: 0.5000\n",
            "Epoch 3, Loss: 7.9924, rho_lr: 0.200000\n",
            "rho0: 0.2329, rho1: 0.5000\n",
            "Epoch 4, Loss: 5.5873, rho_lr: 0.200000\n",
            "rho0: 0.2987, rho1: 0.5000\n",
            "Epoch 5, Loss: 4.4621, rho_lr: 0.200000\n",
            "rho0: 0.3067, rho1: 0.5000\n",
            "Epoch 6, Loss: 3.6383, rho_lr: 0.200000\n",
            "rho0: 0.3078, rho1: 0.5000\n",
            "Epoch 7, Loss: 3.2641, rho_lr: 0.200000\n",
            "rho0: 0.3057, rho1: 0.5000\n",
            "Epoch 8, Loss: 2.8883, rho_lr: 0.200000\n",
            "rho0: 0.2972, rho1: 0.5000\n",
            "Epoch 9, Loss: 2.6530, rho_lr: 0.200000\n",
            "rho0: 0.3040, rho1: 0.5000\n",
            "Epoch 10, Loss: 2.4818, rho_lr: 0.200000\n",
            "rho0: 0.3076, rho1: 0.5000\n",
            "Epoch 11, Loss: 2.3038, rho_lr: 0.200000\n",
            "rho0: 0.3931, rho1: 0.3327\n",
            "Epoch 12, Loss: 1.5957, rho_lr: 0.200000\n",
            "rho0: 0.5285, rho1: 0.2000\n",
            "Epoch 13, Loss: 1.3344, rho_lr: 0.200000\n",
            "rho0: 0.5329, rho1: 0.2000\n",
            "Epoch 14, Loss: 1.2464, rho_lr: 0.200000\n",
            "rho0: 0.5492, rho1: 0.2000\n",
            "Epoch 15, Loss: 1.2277, rho_lr: 0.200000\n",
            "rho0: 0.5499, rho1: 0.2000\n",
            "Epoch 16, Loss: 1.1688, rho_lr: 0.200000\n",
            "rho0: 0.5646, rho1: 0.2000\n",
            "Epoch 17, Loss: 1.1725, rho_lr: 0.200000\n",
            "rho0: 0.5693, rho1: 0.2000\n",
            "Epoch 18, Loss: 1.1401, rho_lr: 0.200000\n",
            "rho0: 0.5756, rho1: 0.2000\n",
            "Epoch 19, Loss: 1.1226, rho_lr: 0.200000\n",
            "rho0: 0.5841, rho1: 0.2000\n",
            "Epoch 20, Loss: 1.0982, rho_lr: 0.020000\n",
            "rho0: 0.5849, rho1: 0.2000\n",
            "Epoch 21, Loss: 1.0550, rho_lr: 0.020000\n",
            "rho0: 0.5873, rho1: 0.2000\n",
            "Epoch 22, Loss: 1.0568, rho_lr: 0.020000\n",
            "rho0: 0.5894, rho1: 0.2000\n",
            "Epoch 23, Loss: 1.0344, rho_lr: 0.020000\n",
            "rho0: 0.5926, rho1: 0.2000\n",
            "Epoch 24, Loss: 1.0093, rho_lr: 0.020000\n",
            "rho0: 0.5964, rho1: 0.2000\n",
            "Epoch 25, Loss: 1.0108, rho_lr: 0.020000\n",
            "rho0: 0.6002, rho1: 0.2000\n",
            "Epoch 26, Loss: 0.9797, rho_lr: 0.020000\n",
            "rho0: 0.6035, rho1: 0.2000\n",
            "Epoch 27, Loss: 0.9645, rho_lr: 0.020000\n",
            "rho0: 0.6070, rho1: 0.2000\n",
            "Epoch 28, Loss: 0.9648, rho_lr: 0.020000\n",
            "rho0: 0.6104, rho1: 0.2000\n",
            "Epoch 29, Loss: 0.9555, rho_lr: 0.020000\n",
            "rho0: 0.6140, rho1: 0.2000\n",
            "Epoch 30, Loss: 0.9359, rho_lr: 0.020000\n",
            "rho0: 0.6183, rho1: 0.2000\n",
            "Epoch 31, Loss: 0.9148, rho_lr: 0.020000\n",
            "rho0: 0.6216, rho1: 0.2000\n",
            "Epoch 32, Loss: 0.9055, rho_lr: 0.020000\n",
            "rho0: 0.6249, rho1: 0.2000\n",
            "Epoch 33, Loss: 0.8939, rho_lr: 0.020000\n",
            "rho0: 0.6287, rho1: 0.2000\n",
            "Epoch 34, Loss: 0.8896, rho_lr: 0.020000\n",
            "rho0: 0.6324, rho1: 0.2000\n",
            "Epoch 35, Loss: 0.8714, rho_lr: 0.020000\n",
            "rho0: 0.6361, rho1: 0.2000\n",
            "Epoch 36, Loss: 0.8493, rho_lr: 0.020000\n",
            "rho0: 0.6392, rho1: 0.2000\n",
            "Epoch 37, Loss: 0.8554, rho_lr: 0.020000\n",
            "rho0: 0.6425, rho1: 0.2000\n",
            "Epoch 38, Loss: 0.8455, rho_lr: 0.020000\n",
            "rho0: 0.6460, rho1: 0.2000\n",
            "Epoch 39, Loss: 0.8284, rho_lr: 0.020000\n",
            "rho0: 0.6490, rho1: 0.2000\n",
            "Epoch 40, Loss: 0.8247, rho_lr: 0.002000\n",
            "rho0: 0.6524, rho1: 0.2000\n",
            "Epoch 41, Loss: 0.8114, rho_lr: 0.002000\n",
            "rho0: 0.6527, rho1: 0.2000\n",
            "Epoch 42, Loss: 0.7962, rho_lr: 0.002000\n",
            "rho0: 0.6532, rho1: 0.2000\n",
            "Epoch 43, Loss: 0.7963, rho_lr: 0.002000\n",
            "rho0: 0.6537, rho1: 0.2000\n",
            "Epoch 44, Loss: 0.8029, rho_lr: 0.002000\n",
            "rho0: 0.6542, rho1: 0.2000\n",
            "Epoch 45, Loss: 0.7855, rho_lr: 0.002000\n",
            "rho0: 0.6548, rho1: 0.2000\n",
            "Epoch 46, Loss: 0.7842, rho_lr: 0.002000\n",
            "rho0: 0.6553, rho1: 0.2000\n",
            "Epoch 47, Loss: 0.7715, rho_lr: 0.002000\n",
            "rho0: 0.6559, rho1: 0.2000\n",
            "Epoch 48, Loss: 0.7682, rho_lr: 0.002000\n",
            "rho0: 0.6565, rho1: 0.2000\n",
            "Epoch 49, Loss: 0.7634, rho_lr: 0.002000\n",
            "rho0: 0.6572, rho1: 0.2000\n",
            "Epoch 50, Loss: 0.7539, rho_lr: 0.002000\n",
            "rho0: 0.6578, rho1: 0.2000\n",
            "Epoch 51, Loss: 0.7527, rho_lr: 0.002000\n",
            "rho0: 0.6585, rho1: 0.2000\n",
            "Epoch 52, Loss: 0.7568, rho_lr: 0.002000\n",
            "rho0: 0.6591, rho1: 0.2000\n",
            "Epoch 53, Loss: 0.7458, rho_lr: 0.002000\n",
            "rho0: 0.6598, rho1: 0.2000\n",
            "Epoch 54, Loss: 0.7369, rho_lr: 0.002000\n",
            "rho0: 0.6605, rho1: 0.2000\n",
            "Epoch 55, Loss: 0.7310, rho_lr: 0.002000\n",
            "rho0: 0.6611, rho1: 0.2000\n",
            "Epoch 56, Loss: 0.7364, rho_lr: 0.002000\n",
            "rho0: 0.6618, rho1: 0.2000\n",
            "Epoch 57, Loss: 0.7238, rho_lr: 0.002000\n",
            "rho0: 0.6624, rho1: 0.2000\n",
            "Epoch 58, Loss: 0.7169, rho_lr: 0.002000\n",
            "rho0: 0.6631, rho1: 0.2000\n",
            "Epoch 59, Loss: 0.7221, rho_lr: 0.002000\n",
            "rho0: 0.6638, rho1: 0.2000\n",
            "Epoch 60, Loss: 0.7141, rho_lr: 0.000200\n",
            "rho0: 0.6645, rho1: 0.2000\n",
            "Epoch 1, Loss: 34.8080, rho_lr: 0.200000\n",
            "rho0: 0.0129, rho1: 0.5000\n",
            "Epoch 2, Loss: 11.2729, rho_lr: 0.200000\n",
            "rho0: 0.2625, rho1: 0.5000\n",
            "Epoch 3, Loss: 6.4289, rho_lr: 0.200000\n",
            "rho0: 0.2642, rho1: 0.5000\n",
            "Epoch 4, Loss: 4.6131, rho_lr: 0.200000\n",
            "rho0: 0.2971, rho1: 0.5000\n",
            "Epoch 5, Loss: 3.3880, rho_lr: 0.200000\n",
            "rho0: 0.3433, rho1: 0.5000\n",
            "Epoch 6, Loss: 2.8046, rho_lr: 0.200000\n",
            "rho0: 0.3220, rho1: 0.5000\n",
            "Epoch 7, Loss: 2.3556, rho_lr: 0.200000\n",
            "rho0: 0.3340, rho1: 0.5000\n",
            "Epoch 8, Loss: 2.1639, rho_lr: 0.200000\n",
            "rho0: 0.3404, rho1: 0.5000\n",
            "Epoch 9, Loss: 2.0090, rho_lr: 0.200000\n",
            "rho0: 0.3243, rho1: 0.5000\n",
            "Epoch 10, Loss: 1.9393, rho_lr: 0.200000\n",
            "rho0: 0.3324, rho1: 0.5000\n",
            "Epoch 11, Loss: 2.0714, rho_lr: 0.200000\n",
            "rho0: 0.3665, rho1: 0.4299\n",
            "Epoch 12, Loss: 1.5889, rho_lr: 0.200000\n",
            "rho0: 0.4708, rho1: 0.2924\n",
            "Epoch 13, Loss: 1.1436, rho_lr: 0.200000\n",
            "rho0: 0.5782, rho1: 0.2000\n",
            "Epoch 14, Loss: 1.0327, rho_lr: 0.200000\n",
            "rho0: 0.5624, rho1: 0.2000\n",
            "Epoch 15, Loss: 0.9909, rho_lr: 0.200000\n",
            "rho0: 0.5775, rho1: 0.2000\n",
            "Epoch 16, Loss: 0.9738, rho_lr: 0.200000\n",
            "rho0: 0.5774, rho1: 0.2000\n",
            "Epoch 17, Loss: 0.9512, rho_lr: 0.200000\n",
            "rho0: 0.5825, rho1: 0.2000\n",
            "Epoch 18, Loss: 0.9466, rho_lr: 0.200000\n",
            "rho0: 0.5860, rho1: 0.2000\n",
            "Epoch 19, Loss: 0.9230, rho_lr: 0.200000\n",
            "rho0: 0.5916, rho1: 0.2000\n",
            "Epoch 20, Loss: 0.9152, rho_lr: 0.020000\n",
            "rho0: 0.5965, rho1: 0.2000\n",
            "Epoch 21, Loss: 0.9034, rho_lr: 0.020000\n",
            "rho0: 0.5978, rho1: 0.2000\n",
            "Epoch 22, Loss: 0.9011, rho_lr: 0.020000\n",
            "rho0: 0.5996, rho1: 0.2000\n",
            "Epoch 23, Loss: 0.8778, rho_lr: 0.020000\n",
            "rho0: 0.6020, rho1: 0.2000\n",
            "Epoch 24, Loss: 0.8797, rho_lr: 0.020000\n",
            "rho0: 0.6045, rho1: 0.2000\n",
            "Epoch 25, Loss: 0.8682, rho_lr: 0.020000\n",
            "rho0: 0.6076, rho1: 0.2000\n",
            "Epoch 26, Loss: 0.8577, rho_lr: 0.020000\n",
            "rho0: 0.6111, rho1: 0.2000\n",
            "Epoch 27, Loss: 0.8561, rho_lr: 0.020000\n",
            "rho0: 0.6135, rho1: 0.2000\n",
            "Epoch 28, Loss: 0.8388, rho_lr: 0.020000\n",
            "rho0: 0.6158, rho1: 0.2000\n",
            "Epoch 29, Loss: 0.8327, rho_lr: 0.020000\n",
            "rho0: 0.6179, rho1: 0.2000\n",
            "Epoch 30, Loss: 0.8325, rho_lr: 0.020000\n",
            "rho0: 0.6207, rho1: 0.2000\n",
            "Epoch 31, Loss: 0.8151, rho_lr: 0.020000\n",
            "rho0: 0.6236, rho1: 0.2000\n",
            "Epoch 32, Loss: 0.8112, rho_lr: 0.020000\n",
            "rho0: 0.6266, rho1: 0.2000\n",
            "Epoch 33, Loss: 0.7995, rho_lr: 0.020000\n",
            "rho0: 0.6293, rho1: 0.2000\n",
            "Epoch 34, Loss: 0.8050, rho_lr: 0.020000\n",
            "rho0: 0.6315, rho1: 0.2000\n",
            "Epoch 35, Loss: 0.7879, rho_lr: 0.020000\n",
            "rho0: 0.6340, rho1: 0.2000\n",
            "Epoch 36, Loss: 0.7809, rho_lr: 0.020000\n",
            "rho0: 0.6366, rho1: 0.2000\n",
            "Epoch 37, Loss: 0.7816, rho_lr: 0.020000\n",
            "rho0: 0.6395, rho1: 0.2000\n",
            "Epoch 38, Loss: 0.7775, rho_lr: 0.020000\n",
            "rho0: 0.6415, rho1: 0.2000\n",
            "Epoch 39, Loss: 0.7640, rho_lr: 0.020000\n",
            "rho0: 0.6440, rho1: 0.2000\n",
            "Epoch 40, Loss: 0.7631, rho_lr: 0.002000\n",
            "rho0: 0.6471, rho1: 0.2000\n",
            "Epoch 41, Loss: 0.7518, rho_lr: 0.002000\n",
            "rho0: 0.6474, rho1: 0.2000\n",
            "Epoch 42, Loss: 0.7504, rho_lr: 0.002000\n",
            "rho0: 0.6478, rho1: 0.2000\n",
            "Epoch 43, Loss: 0.7412, rho_lr: 0.002000\n",
            "rho0: 0.6483, rho1: 0.2000\n",
            "Epoch 44, Loss: 0.7433, rho_lr: 0.002000\n",
            "rho0: 0.6487, rho1: 0.2000\n",
            "Epoch 45, Loss: 0.7420, rho_lr: 0.002000\n",
            "rho0: 0.6492, rho1: 0.2000\n",
            "Epoch 46, Loss: 0.7351, rho_lr: 0.002000\n",
            "rho0: 0.6496, rho1: 0.2000\n",
            "Epoch 47, Loss: 0.7300, rho_lr: 0.002000\n",
            "rho0: 0.6502, rho1: 0.2000\n",
            "Epoch 48, Loss: 0.7242, rho_lr: 0.002000\n",
            "rho0: 0.6507, rho1: 0.2000\n",
            "Epoch 49, Loss: 0.7172, rho_lr: 0.002000\n",
            "rho0: 0.6513, rho1: 0.2000\n",
            "Epoch 50, Loss: 0.7169, rho_lr: 0.002000\n",
            "rho0: 0.6518, rho1: 0.2000\n",
            "Epoch 51, Loss: 0.7177, rho_lr: 0.002000\n",
            "rho0: 0.6524, rho1: 0.2000\n",
            "Epoch 52, Loss: 0.7094, rho_lr: 0.002000\n",
            "rho0: 0.6530, rho1: 0.2000\n",
            "Epoch 53, Loss: 0.7116, rho_lr: 0.002000\n",
            "rho0: 0.6536, rho1: 0.2000\n",
            "Epoch 54, Loss: 0.7050, rho_lr: 0.002000\n",
            "rho0: 0.6542, rho1: 0.2000\n",
            "Epoch 55, Loss: 0.7024, rho_lr: 0.002000\n",
            "rho0: 0.6548, rho1: 0.2000\n",
            "Epoch 56, Loss: 0.6995, rho_lr: 0.002000\n",
            "rho0: 0.6555, rho1: 0.2000\n",
            "Epoch 57, Loss: 0.6933, rho_lr: 0.002000\n",
            "rho0: 0.6561, rho1: 0.2000\n",
            "Epoch 58, Loss: 0.6916, rho_lr: 0.002000\n",
            "rho0: 0.6568, rho1: 0.2000\n",
            "Epoch 59, Loss: 0.6829, rho_lr: 0.002000\n",
            "rho0: 0.6574, rho1: 0.2000\n",
            "Epoch 60, Loss: 0.6858, rho_lr: 0.000200\n",
            "rho0: 0.6580, rho1: 0.2000\n",
            "Epoch 1, Loss: 31.2172, rho_lr: 0.200000\n",
            "rho0: 0.0000, rho1: 0.5000\n",
            "Epoch 2, Loss: 13.9521, rho_lr: 0.200000\n",
            "rho0: 0.2716, rho1: 0.5000\n",
            "Epoch 3, Loss: 7.3157, rho_lr: 0.200000\n",
            "rho0: 0.2746, rho1: 0.5000\n",
            "Epoch 4, Loss: 5.2218, rho_lr: 0.200000\n",
            "rho0: 0.2499, rho1: 0.5000\n",
            "Epoch 5, Loss: 3.8343, rho_lr: 0.200000\n",
            "rho0: 0.3170, rho1: 0.5000\n",
            "Epoch 6, Loss: 3.1216, rho_lr: 0.200000\n",
            "rho0: 0.3198, rho1: 0.5000\n",
            "Epoch 7, Loss: 2.6547, rho_lr: 0.200000\n",
            "rho0: 0.3076, rho1: 0.5000\n",
            "Epoch 8, Loss: 2.4590, rho_lr: 0.200000\n",
            "rho0: 0.3111, rho1: 0.5000\n",
            "Epoch 9, Loss: 2.3387, rho_lr: 0.200000\n",
            "rho0: 0.3219, rho1: 0.5000\n",
            "Epoch 10, Loss: 2.1305, rho_lr: 0.200000\n",
            "rho0: 0.3120, rho1: 0.5000\n",
            "Epoch 11, Loss: 2.1940, rho_lr: 0.200000\n",
            "rho0: 0.3990, rho1: 0.3874\n",
            "Epoch 12, Loss: 1.4871, rho_lr: 0.200000\n",
            "rho0: 0.5336, rho1: 0.2000\n",
            "Epoch 13, Loss: 1.1524, rho_lr: 0.200000\n",
            "rho0: 0.5620, rho1: 0.2000\n",
            "Epoch 14, Loss: 1.0902, rho_lr: 0.200000\n",
            "rho0: 0.5665, rho1: 0.2000\n",
            "Epoch 15, Loss: 1.0727, rho_lr: 0.200000\n",
            "rho0: 0.5762, rho1: 0.2000\n",
            "Epoch 16, Loss: 1.0329, rho_lr: 0.200000\n",
            "rho0: 0.5859, rho1: 0.2000\n",
            "Epoch 17, Loss: 1.0343, rho_lr: 0.200000\n",
            "rho0: 0.5849, rho1: 0.2000\n",
            "Epoch 18, Loss: 0.9979, rho_lr: 0.200000\n",
            "rho0: 0.5986, rho1: 0.2000\n",
            "Epoch 19, Loss: 0.9772, rho_lr: 0.200000\n",
            "rho0: 0.6038, rho1: 0.2000\n",
            "Epoch 20, Loss: 0.9663, rho_lr: 0.020000\n",
            "rho0: 0.6130, rho1: 0.2000\n",
            "Epoch 21, Loss: 0.9515, rho_lr: 0.020000\n",
            "rho0: 0.6133, rho1: 0.2000\n",
            "Epoch 22, Loss: 0.9287, rho_lr: 0.020000\n",
            "rho0: 0.6155, rho1: 0.2000\n",
            "Epoch 23, Loss: 0.9161, rho_lr: 0.020000\n",
            "rho0: 0.6195, rho1: 0.2000\n",
            "Epoch 24, Loss: 0.8889, rho_lr: 0.020000\n",
            "rho0: 0.6233, rho1: 0.2000\n",
            "Epoch 25, Loss: 0.8949, rho_lr: 0.020000\n",
            "rho0: 0.6277, rho1: 0.2000\n",
            "Epoch 26, Loss: 0.8809, rho_lr: 0.020000\n",
            "rho0: 0.6314, rho1: 0.2000\n",
            "Epoch 27, Loss: 0.8631, rho_lr: 0.020000\n",
            "rho0: 0.6344, rho1: 0.2000\n",
            "Epoch 28, Loss: 0.8462, rho_lr: 0.020000\n",
            "rho0: 0.6375, rho1: 0.2000\n",
            "Epoch 29, Loss: 0.8347, rho_lr: 0.020000\n",
            "rho0: 0.6411, rho1: 0.2000\n",
            "Epoch 30, Loss: 0.8311, rho_lr: 0.020000\n",
            "rho0: 0.6448, rho1: 0.2000\n",
            "Epoch 31, Loss: 0.8219, rho_lr: 0.020000\n",
            "rho0: 0.6488, rho1: 0.2000\n",
            "Epoch 32, Loss: 0.8051, rho_lr: 0.020000\n",
            "rho0: 0.6531, rho1: 0.2000\n",
            "Epoch 33, Loss: 0.8061, rho_lr: 0.020000\n",
            "rho0: 0.6569, rho1: 0.2000\n",
            "Epoch 34, Loss: 0.7910, rho_lr: 0.020000\n",
            "rho0: 0.6606, rho1: 0.2000\n",
            "Epoch 35, Loss: 0.7777, rho_lr: 0.020000\n",
            "rho0: 0.6645, rho1: 0.2000\n",
            "Epoch 36, Loss: 0.7657, rho_lr: 0.020000\n",
            "rho0: 0.6684, rho1: 0.2000\n",
            "Epoch 37, Loss: 0.7661, rho_lr: 0.020000\n",
            "rho0: 0.6716, rho1: 0.2000\n",
            "Epoch 38, Loss: 0.7501, rho_lr: 0.020000\n",
            "rho0: 0.6754, rho1: 0.2000\n",
            "Epoch 39, Loss: 0.7442, rho_lr: 0.020000\n",
            "rho0: 0.6785, rho1: 0.2000\n",
            "Epoch 40, Loss: 0.7352, rho_lr: 0.002000\n",
            "rho0: 0.6821, rho1: 0.2000\n",
            "Epoch 41, Loss: 0.7347, rho_lr: 0.002000\n",
            "rho0: 0.6825, rho1: 0.2000\n",
            "Epoch 42, Loss: 0.7210, rho_lr: 0.002000\n",
            "rho0: 0.6830, rho1: 0.2000\n",
            "Epoch 43, Loss: 0.7095, rho_lr: 0.002000\n",
            "rho0: 0.6836, rho1: 0.2000\n",
            "Epoch 44, Loss: 0.7112, rho_lr: 0.002000\n",
            "rho0: 0.6843, rho1: 0.2000\n",
            "Epoch 45, Loss: 0.7113, rho_lr: 0.002000\n",
            "rho0: 0.6850, rho1: 0.2000\n",
            "Epoch 46, Loss: 0.6975, rho_lr: 0.002000\n",
            "rho0: 0.6857, rho1: 0.2000\n",
            "Epoch 47, Loss: 0.6999, rho_lr: 0.002000\n",
            "rho0: 0.6864, rho1: 0.2000\n",
            "Epoch 48, Loss: 0.6925, rho_lr: 0.002000\n",
            "rho0: 0.6871, rho1: 0.2000\n",
            "Epoch 49, Loss: 0.6926, rho_lr: 0.002000\n",
            "rho0: 0.6879, rho1: 0.2000\n",
            "Epoch 50, Loss: 0.6829, rho_lr: 0.002000\n",
            "rho0: 0.6887, rho1: 0.2000\n",
            "Epoch 51, Loss: 0.6790, rho_lr: 0.002000\n",
            "rho0: 0.6895, rho1: 0.2000\n",
            "Epoch 52, Loss: 0.6764, rho_lr: 0.002000\n",
            "rho0: 0.6903, rho1: 0.2000\n",
            "Epoch 53, Loss: 0.6720, rho_lr: 0.002000\n",
            "rho0: 0.6910, rho1: 0.2000\n",
            "Epoch 54, Loss: 0.6698, rho_lr: 0.002000\n",
            "rho0: 0.6919, rho1: 0.2000\n",
            "Epoch 55, Loss: 0.6666, rho_lr: 0.002000\n",
            "rho0: 0.6927, rho1: 0.2000\n",
            "Epoch 56, Loss: 0.6616, rho_lr: 0.002000\n",
            "rho0: 0.6935, rho1: 0.2000\n",
            "Epoch 57, Loss: 0.6589, rho_lr: 0.002000\n",
            "rho0: 0.6942, rho1: 0.2000\n",
            "Epoch 58, Loss: 0.6559, rho_lr: 0.002000\n",
            "rho0: 0.6951, rho1: 0.2000\n",
            "Epoch 59, Loss: 0.6543, rho_lr: 0.002000\n",
            "rho0: 0.6960, rho1: 0.2000\n",
            "Epoch 60, Loss: 0.6478, rho_lr: 0.000200\n",
            "rho0: 0.6968, rho1: 0.2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.std(default_losses))\n",
        "print(np.std(adjusted_losses))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6b_lVQ2GX19",
        "outputId": "e6cc664a-0352-4f88-e1f4-8f4e7b0bdd2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.05590826899283577\n",
            "0.03608545019663142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The variability of the loss over the 20 validation sets is slightly lower for (b) than for (a)"
      ],
      "metadata": {
        "id": "VKKI8Ej_hJPC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7\n",
        "I will use the adjusted loss proposed in problem 5 because there is less variability of the loss over the 20 training sets."
      ],
      "metadata": {
        "id": "v-35kXbLICuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SmallNet(nn.Module):\n",
        "  def __init__(self, channels=[32, 64], embed_dim=256, group_num=4):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.embed = nn.Sequential(GaussianFourierProjection(embed_dim=embed_dim),\n",
        "         nn.Linear(embed_dim, embed_dim))\n",
        "\n",
        "    self.conv1 = nn.Conv2d(1, channels[0], 3, stride=1, bias=False)\n",
        "    self.dense1 = Dense(embed_dim, channels[0])\n",
        "    self.gnorm1 = nn.GroupNorm(group_num, num_channels=channels[0])\n",
        "\n",
        "    self.conv2 = nn.Conv2d(channels[0], channels[1], 3, stride=2, bias=False)\n",
        "    self.dense2 = Dense(embed_dim, channels[1])\n",
        "    self.gnorm2 = nn.GroupNorm(group_num, num_channels=channels[1])\n",
        "\n",
        "\n",
        "    self.tconv2 = nn.ConvTranspose2d(channels[1], channels[0], 3, stride=2, bias=False, output_padding=1)\n",
        "    self.dense7 = Dense(embed_dim, channels[0])\n",
        "    self.tgnorm2 = nn.GroupNorm(group_num, num_channels=channels[0])\n",
        "    self.tconv1 = nn.ConvTranspose2d(channels[0] + channels[0], 1, 3, stride=1)\n",
        "\n",
        "    # The swish activation function\n",
        "    self.act = lambda x: x * torch.sigmoid(x)\n",
        "\n",
        "    self.rho0 = nn.Parameter(torch.tensor(1.0))\n",
        "    self.rho1 = nn.Parameter(torch.tensor(0.5))\n",
        "\n",
        "  def forward(self, x, t):\n",
        "    embed = self.act(self.embed(t))\n",
        "\n",
        "    h1 = self.conv1(x) # ...\n",
        "    h1 += self.dense1(embed) #...\n",
        "    h1 = self.gnorm1(h1) # ...\n",
        "    h1 = self.act(h1) # ...\n",
        "\n",
        "    h2 = self.conv2(h1) # ...\n",
        "    h2 += self.dense2(embed)\n",
        "    h2 = self.gnorm2(h2)\n",
        "    h2 = self.act(h2)\n",
        "\n",
        "    h = self.tconv2(h2)\n",
        "    h += self.dense7(embed)\n",
        "    h = self.tgnorm2(h)\n",
        "    h = self.act(h)\n",
        "    h = self.tconv1(torch.cat([h, h1], dim=1))\n",
        "\n",
        "    mu = self.rho0 * (x - self.rho1 * h)\n",
        "\n",
        "    return mu"
      ],
      "metadata": {
        "id": "34gsVL5uN5tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_a = ScoreNet()\n",
        "model_a.to(device)\n",
        "diffusion_a = Diffusion(model_a, n_steps=200, device=device, min_beta=1e-4, max_beta=0.01)\n",
        "\n",
        "rho_params = [model_a.rho0, model_a.rho1]\n",
        "rho_ids = set(id(p) for p in rho_params)\n",
        "base_params = [p for p in model_a.parameters() if id(p) not in rho_ids]\n",
        "optimizer_a = torch.optim.Adam([\n",
        "    {\"params\": base_params, \"lr\": 0.001},\n",
        "    {\"params\": rho_params, \"lr\": 0.2}\n",
        "])\n",
        "\n",
        "train(model_a, diffusion_a, train_data, optimizer_a, compute_adjusted_loss, num_epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixgTAI50SwpF",
        "outputId": "f06b36b8-102a-4c21-9cd3-5b3a6835577b",
        "collapsed": true
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 2.3672, rho_lr: 0.200000\n",
            "rho0: 0.5447, rho1: 0.5000\n",
            "Epoch 2, Loss: 0.6401, rho_lr: 0.200000\n",
            "rho0: 0.7446, rho1: 0.5000\n",
            "Epoch 3, Loss: 0.5445, rho_lr: 0.200000\n",
            "rho0: 0.8175, rho1: 0.5000\n",
            "Epoch 4, Loss: 0.5200, rho_lr: 0.200000\n",
            "rho0: 0.8508, rho1: 0.5000\n",
            "Epoch 5, Loss: 0.5117, rho_lr: 0.200000\n",
            "rho0: 0.8748, rho1: 0.5000\n",
            "Epoch 6, Loss: 0.5095, rho_lr: 0.200000\n",
            "rho0: 0.8663, rho1: 0.5000\n",
            "Epoch 7, Loss: 0.5004, rho_lr: 0.200000\n",
            "rho0: 0.8976, rho1: 0.5000\n",
            "Epoch 8, Loss: 0.4997, rho_lr: 0.200000\n",
            "rho0: 0.9042, rho1: 0.5000\n",
            "Epoch 9, Loss: 0.4962, rho_lr: 0.200000\n",
            "rho0: 0.9107, rho1: 0.5000\n",
            "Epoch 10, Loss: 0.5035, rho_lr: 0.200000\n",
            "rho0: 0.9159, rho1: 0.5000\n",
            "Epoch 11, Loss: 0.4926, rho_lr: 0.200000\n",
            "rho0: 0.9364, rho1: 0.3082\n",
            "Epoch 12, Loss: 0.4922, rho_lr: 0.200000\n",
            "rho0: 0.9337, rho1: 0.2939\n",
            "Epoch 13, Loss: 0.4925, rho_lr: 0.200000\n",
            "rho0: 0.9368, rho1: 0.2823\n",
            "Epoch 14, Loss: 0.4927, rho_lr: 0.200000\n",
            "rho0: 0.9428, rho1: 0.2663\n",
            "Epoch 15, Loss: 0.4923, rho_lr: 0.200000\n",
            "rho0: 0.9311, rho1: 0.2263\n",
            "Epoch 16, Loss: 0.4934, rho_lr: 0.200000\n",
            "rho0: 0.9516, rho1: 0.2321\n",
            "Epoch 17, Loss: 0.4924, rho_lr: 0.200000\n",
            "rho0: 0.9513, rho1: 0.2000\n",
            "Epoch 18, Loss: 0.4940, rho_lr: 0.200000\n",
            "rho0: 0.9516, rho1: 0.2310\n",
            "Epoch 19, Loss: 0.4911, rho_lr: 0.200000\n",
            "rho0: 0.9525, rho1: 0.2091\n",
            "Epoch 20, Loss: 0.4907, rho_lr: 0.020000\n",
            "rho0: 0.9680, rho1: 0.2091\n",
            "Epoch 21, Loss: 0.4893, rho_lr: 0.020000\n",
            "rho0: 0.9549, rho1: 0.2098\n",
            "Epoch 22, Loss: 0.4889, rho_lr: 0.020000\n",
            "rho0: 0.9584, rho1: 0.2000\n",
            "Epoch 23, Loss: 0.4890, rho_lr: 0.020000\n",
            "rho0: 0.9579, rho1: 0.2031\n",
            "Epoch 24, Loss: 0.4888, rho_lr: 0.020000\n",
            "rho0: 0.9589, rho1: 0.2000\n",
            "Epoch 25, Loss: 0.4956, rho_lr: 0.020000\n",
            "rho0: 0.9744, rho1: 0.2339\n",
            "Epoch 26, Loss: 0.4898, rho_lr: 0.020000\n",
            "rho0: 0.9657, rho1: 0.3170\n",
            "Epoch 27, Loss: 0.4889, rho_lr: 0.020000\n",
            "rho0: 0.9684, rho1: 0.2893\n",
            "Epoch 28, Loss: 0.4882, rho_lr: 0.020000\n",
            "rho0: 0.9661, rho1: 0.3152\n",
            "Epoch 29, Loss: 0.4876, rho_lr: 0.020000\n",
            "rho0: 0.9693, rho1: 0.2506\n",
            "Epoch 30, Loss: 0.4873, rho_lr: 0.020000\n",
            "rho0: 0.9703, rho1: 0.2477\n",
            "Epoch 31, Loss: 0.4867, rho_lr: 0.020000\n",
            "rho0: 0.9697, rho1: 0.3071\n",
            "Epoch 32, Loss: 0.4868, rho_lr: 0.020000\n",
            "rho0: 0.9735, rho1: 0.2623\n",
            "Epoch 33, Loss: 0.4868, rho_lr: 0.020000\n",
            "rho0: 0.9744, rho1: 0.2661\n",
            "Epoch 34, Loss: 0.4865, rho_lr: 0.020000\n",
            "rho0: 0.9721, rho1: 0.3626\n",
            "Epoch 35, Loss: 0.4889, rho_lr: 0.020000\n",
            "rho0: 0.9820, rho1: 0.3027\n",
            "Epoch 36, Loss: 0.4879, rho_lr: 0.020000\n",
            "rho0: 0.9808, rho1: 0.2650\n",
            "Epoch 37, Loss: 0.4871, rho_lr: 0.020000\n",
            "rho0: 0.9829, rho1: 0.3135\n",
            "Epoch 38, Loss: 0.4870, rho_lr: 0.020000\n",
            "rho0: 0.9819, rho1: 0.3096\n",
            "Epoch 39, Loss: 0.4870, rho_lr: 0.020000\n",
            "rho0: 0.9791, rho1: 0.3519\n",
            "Epoch 40, Loss: 0.4870, rho_lr: 0.002000\n",
            "rho0: 0.9779, rho1: 0.3101\n",
            "Epoch 41, Loss: 0.4867, rho_lr: 0.002000\n",
            "rho0: 0.9778, rho1: 0.3301\n",
            "Epoch 42, Loss: 0.4864, rho_lr: 0.002000\n",
            "rho0: 0.9776, rho1: 0.3312\n",
            "Epoch 43, Loss: 0.4863, rho_lr: 0.002000\n",
            "rho0: 0.9778, rho1: 0.3586\n",
            "Epoch 44, Loss: 0.4863, rho_lr: 0.002000\n",
            "rho0: 0.9768, rho1: 0.3496\n",
            "Epoch 45, Loss: 0.4862, rho_lr: 0.002000\n",
            "rho0: 0.9762, rho1: 0.3203\n",
            "Epoch 46, Loss: 0.4855, rho_lr: 0.002000\n",
            "rho0: 0.9765, rho1: 0.3583\n",
            "Epoch 47, Loss: 0.4851, rho_lr: 0.002000\n",
            "rho0: 0.9765, rho1: 0.3533\n",
            "Epoch 48, Loss: 0.4854, rho_lr: 0.002000\n",
            "rho0: 0.9769, rho1: 0.3461\n",
            "Epoch 49, Loss: 0.4852, rho_lr: 0.002000\n",
            "rho0: 0.9768, rho1: 0.3834\n",
            "Epoch 50, Loss: 0.4851, rho_lr: 0.002000\n",
            "rho0: 0.9760, rho1: 0.3986\n",
            "Epoch 51, Loss: 0.4849, rho_lr: 0.002000\n",
            "rho0: 0.9767, rho1: 0.3922\n",
            "Epoch 52, Loss: 0.4852, rho_lr: 0.002000\n",
            "rho0: 0.9765, rho1: 0.3838\n",
            "Epoch 53, Loss: 0.4851, rho_lr: 0.002000\n",
            "rho0: 0.9773, rho1: 0.4190\n",
            "Epoch 54, Loss: 0.4848, rho_lr: 0.002000\n",
            "rho0: 0.9773, rho1: 0.3984\n",
            "Epoch 55, Loss: 0.4851, rho_lr: 0.002000\n",
            "rho0: 0.9772, rho1: 0.3955\n",
            "Epoch 56, Loss: 0.4848, rho_lr: 0.002000\n",
            "rho0: 0.9769, rho1: 0.4153\n",
            "Epoch 57, Loss: 0.4851, rho_lr: 0.002000\n",
            "rho0: 0.9771, rho1: 0.4148\n",
            "Epoch 58, Loss: 0.4849, rho_lr: 0.002000\n",
            "rho0: 0.9773, rho1: 0.4127\n",
            "Epoch 59, Loss: 0.4847, rho_lr: 0.002000\n",
            "rho0: 0.9771, rho1: 0.3903\n",
            "Epoch 60, Loss: 0.4846, rho_lr: 0.000200\n",
            "rho0: 0.9771, rho1: 0.4238\n",
            "Epoch 61, Loss: 0.4846, rho_lr: 0.000200\n",
            "rho0: 0.9770, rho1: 0.4164\n",
            "Epoch 62, Loss: 0.4845, rho_lr: 0.000200\n",
            "rho0: 0.9771, rho1: 0.4099\n",
            "Epoch 63, Loss: 0.4843, rho_lr: 0.000200\n",
            "rho0: 0.9770, rho1: 0.4208\n",
            "Epoch 64, Loss: 0.4843, rho_lr: 0.000200\n",
            "rho0: 0.9770, rho1: 0.4220\n",
            "Epoch 65, Loss: 0.4843, rho_lr: 0.000200\n",
            "rho0: 0.9770, rho1: 0.4163\n",
            "Epoch 66, Loss: 0.4845, rho_lr: 0.000200\n",
            "rho0: 0.9770, rho1: 0.4186\n",
            "Epoch 67, Loss: 0.4846, rho_lr: 0.000200\n",
            "rho0: 0.9771, rho1: 0.4263\n",
            "Epoch 68, Loss: 0.4842, rho_lr: 0.000200\n",
            "rho0: 0.9770, rho1: 0.4180\n",
            "Epoch 69, Loss: 0.4845, rho_lr: 0.000200\n",
            "rho0: 0.9772, rho1: 0.4246\n",
            "Epoch 70, Loss: 0.4845, rho_lr: 0.000200\n",
            "rho0: 0.9770, rho1: 0.4207\n",
            "Epoch 71, Loss: 0.4844, rho_lr: 0.000200\n",
            "rho0: 0.9773, rho1: 0.4266\n",
            "Epoch 72, Loss: 0.4845, rho_lr: 0.000200\n",
            "rho0: 0.9772, rho1: 0.4297\n",
            "Epoch 73, Loss: 0.4840, rho_lr: 0.000200\n",
            "rho0: 0.9771, rho1: 0.4308\n",
            "Epoch 74, Loss: 0.4844, rho_lr: 0.000200\n",
            "rho0: 0.9771, rho1: 0.4327\n",
            "Epoch 75, Loss: 0.4843, rho_lr: 0.000200\n",
            "rho0: 0.9771, rho1: 0.4427\n",
            "Epoch 76, Loss: 0.4846, rho_lr: 0.000200\n",
            "rho0: 0.9770, rho1: 0.4363\n",
            "Epoch 77, Loss: 0.4844, rho_lr: 0.000200\n",
            "rho0: 0.9768, rho1: 0.4362\n",
            "Epoch 78, Loss: 0.4843, rho_lr: 0.000200\n",
            "rho0: 0.9772, rho1: 0.4333\n",
            "Epoch 79, Loss: 0.4841, rho_lr: 0.000200\n",
            "rho0: 0.9772, rho1: 0.4373\n",
            "Epoch 80, Loss: 0.4839, rho_lr: 0.000020\n",
            "rho0: 0.9771, rho1: 0.4405\n",
            "Epoch 81, Loss: 0.4841, rho_lr: 0.000020\n",
            "rho0: 0.9771, rho1: 0.4391\n",
            "Epoch 82, Loss: 0.4842, rho_lr: 0.000020\n",
            "rho0: 0.9770, rho1: 0.4397\n",
            "Epoch 83, Loss: 0.4842, rho_lr: 0.000020\n",
            "rho0: 0.9770, rho1: 0.4404\n",
            "Epoch 84, Loss: 0.4839, rho_lr: 0.000020\n",
            "rho0: 0.9770, rho1: 0.4411\n",
            "Epoch 85, Loss: 0.4840, rho_lr: 0.000020\n",
            "rho0: 0.9769, rho1: 0.4416\n",
            "Epoch 86, Loss: 0.4843, rho_lr: 0.000020\n",
            "rho0: 0.9769, rho1: 0.4422\n",
            "Epoch 87, Loss: 0.4841, rho_lr: 0.000020\n",
            "rho0: 0.9770, rho1: 0.4421\n",
            "Epoch 88, Loss: 0.4837, rho_lr: 0.000020\n",
            "rho0: 0.9769, rho1: 0.4432\n",
            "Epoch 89, Loss: 0.4840, rho_lr: 0.000020\n",
            "rho0: 0.9769, rho1: 0.4438\n",
            "Epoch 90, Loss: 0.4840, rho_lr: 0.000020\n",
            "rho0: 0.9770, rho1: 0.4452\n",
            "Epoch 91, Loss: 0.4841, rho_lr: 0.000020\n",
            "rho0: 0.9770, rho1: 0.4441\n",
            "Epoch 92, Loss: 0.4841, rho_lr: 0.000020\n",
            "rho0: 0.9770, rho1: 0.4452\n",
            "Epoch 93, Loss: 0.4841, rho_lr: 0.000020\n",
            "rho0: 0.9769, rho1: 0.4464\n",
            "Epoch 94, Loss: 0.4839, rho_lr: 0.000020\n",
            "rho0: 0.9769, rho1: 0.4471\n",
            "Epoch 95, Loss: 0.4840, rho_lr: 0.000020\n",
            "rho0: 0.9769, rho1: 0.4474\n",
            "Epoch 96, Loss: 0.4837, rho_lr: 0.000020\n",
            "rho0: 0.9769, rho1: 0.4483\n",
            "Epoch 97, Loss: 0.4838, rho_lr: 0.000020\n",
            "rho0: 0.9769, rho1: 0.4477\n",
            "Epoch 98, Loss: 0.4838, rho_lr: 0.000020\n",
            "rho0: 0.9769, rho1: 0.4488\n",
            "Epoch 99, Loss: 0.4839, rho_lr: 0.000020\n",
            "rho0: 0.9769, rho1: 0.4494\n",
            "Epoch 100, Loss: 0.4838, rho_lr: 0.000002\n",
            "rho0: 0.9769, rho1: 0.4498\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.48383496057987213"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a_loss = evaluate_model(model_a, diffusion_a, val_data, compute_adjusted_loss)\n",
        "print(f\"Original architecture loss on validation set: {a_loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JF1UIc2p5A6m",
        "outputId": "49585acd-281c-4982-cf60-adb1c586e666"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original architecture loss on validation set: 0.4844866880774498\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_b = SmallNet()\n",
        "model_b.to(device)\n",
        "diffusion_b = Diffusion(model_b, n_steps=200, device=device, min_beta=1e-4, max_beta=0.01)\n",
        "rho_params = [model_b.rho0, model_b.rho1]\n",
        "rho_ids = set(id(p) for p in rho_params)\n",
        "base_params = [p for p in model_b.parameters() if id(p) not in rho_ids]\n",
        "optimizer_b = torch.optim.Adam([\n",
        "    {\"params\": base_params, \"lr\": 0.001},\n",
        "    {\"params\": rho_params, \"lr\": 0.2}\n",
        "])\n",
        "train(model_b, diffusion_b, train_data, optimizer_b, compute_adjusted_loss, num_epochs=100)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "k2GtJVmJg_En",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61c29a6d-3ae2-47ea-ea63-15ddc92cc612"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 2.8646, rho_lr: 0.200000\n",
            "rho0: 0.4468, rho1: 0.5000\n",
            "Epoch 2, Loss: 0.7710, rho_lr: 0.200000\n",
            "rho0: 0.5866, rho1: 0.5000\n",
            "Epoch 3, Loss: 0.5947, rho_lr: 0.200000\n",
            "rho0: 0.6965, rho1: 0.5000\n",
            "Epoch 4, Loss: 0.5491, rho_lr: 0.200000\n",
            "rho0: 0.7494, rho1: 0.5000\n",
            "Epoch 5, Loss: 0.5228, rho_lr: 0.200000\n",
            "rho0: 0.7944, rho1: 0.5000\n",
            "Epoch 6, Loss: 0.5179, rho_lr: 0.200000\n",
            "rho0: 0.8152, rho1: 0.5000\n",
            "Epoch 7, Loss: 0.5049, rho_lr: 0.200000\n",
            "rho0: 0.8391, rho1: 0.5000\n",
            "Epoch 8, Loss: 0.5080, rho_lr: 0.200000\n",
            "rho0: 0.8506, rho1: 0.5000\n",
            "Epoch 9, Loss: 0.5085, rho_lr: 0.200000\n",
            "rho0: 0.8585, rho1: 0.5000\n",
            "Epoch 10, Loss: 0.4985, rho_lr: 0.200000\n",
            "rho0: 0.8708, rho1: 0.5000\n",
            "Epoch 11, Loss: 0.4945, rho_lr: 0.200000\n",
            "rho0: 0.9131, rho1: 0.2992\n",
            "Epoch 12, Loss: 0.4938, rho_lr: 0.200000\n",
            "rho0: 0.9165, rho1: 0.2620\n",
            "Epoch 13, Loss: 0.4933, rho_lr: 0.200000\n",
            "rho0: 0.9138, rho1: 0.2666\n",
            "Epoch 14, Loss: 0.4944, rho_lr: 0.200000\n",
            "rho0: 0.9259, rho1: 0.2362\n",
            "Epoch 15, Loss: 0.4937, rho_lr: 0.200000\n",
            "rho0: 0.9249, rho1: 0.2092\n",
            "Epoch 16, Loss: 0.4943, rho_lr: 0.200000\n",
            "rho0: 0.9371, rho1: 0.2405\n",
            "Epoch 17, Loss: 0.4939, rho_lr: 0.200000\n",
            "rho0: 0.9348, rho1: 0.2251\n",
            "Epoch 18, Loss: 0.4954, rho_lr: 0.200000\n",
            "rho0: 0.9362, rho1: 0.2139\n",
            "Epoch 19, Loss: 0.4936, rho_lr: 0.200000\n",
            "rho0: 0.9371, rho1: 0.2048\n",
            "Epoch 20, Loss: 0.4922, rho_lr: 0.020000\n",
            "rho0: 0.9397, rho1: 0.2090\n",
            "Epoch 21, Loss: 0.4913, rho_lr: 0.020000\n",
            "rho0: 0.9394, rho1: 0.2113\n",
            "Epoch 22, Loss: 0.4908, rho_lr: 0.020000\n",
            "rho0: 0.9404, rho1: 0.2131\n",
            "Epoch 23, Loss: 0.4904, rho_lr: 0.020000\n",
            "rho0: 0.9436, rho1: 0.2156\n",
            "Epoch 24, Loss: 0.4906, rho_lr: 0.020000\n",
            "rho0: 0.9450, rho1: 0.2000\n",
            "Epoch 25, Loss: 0.4897, rho_lr: 0.020000\n",
            "rho0: 0.9509, rho1: 0.2189\n",
            "Epoch 26, Loss: 0.4893, rho_lr: 0.020000\n",
            "rho0: 0.9499, rho1: 0.2230\n",
            "Epoch 27, Loss: 0.4893, rho_lr: 0.020000\n",
            "rho0: 0.9517, rho1: 0.2166\n",
            "Epoch 28, Loss: 0.4885, rho_lr: 0.020000\n",
            "rho0: 0.9546, rho1: 0.2336\n",
            "Epoch 29, Loss: 0.4892, rho_lr: 0.020000\n",
            "rho0: 0.9597, rho1: 0.2088\n",
            "Epoch 30, Loss: 0.4884, rho_lr: 0.020000\n",
            "rho0: 0.9599, rho1: 0.2338\n",
            "Epoch 31, Loss: 0.4880, rho_lr: 0.020000\n",
            "rho0: 0.9623, rho1: 0.2193\n",
            "Epoch 32, Loss: 0.4897, rho_lr: 0.020000\n",
            "rho0: 0.9762, rho1: 0.2054\n",
            "Epoch 33, Loss: 0.4897, rho_lr: 0.020000\n",
            "rho0: 0.9713, rho1: 0.2928\n",
            "Epoch 34, Loss: 0.4885, rho_lr: 0.020000\n",
            "rho0: 0.9697, rho1: 0.2307\n",
            "Epoch 35, Loss: 0.4873, rho_lr: 0.020000\n",
            "rho0: 0.9700, rho1: 0.2815\n",
            "Epoch 36, Loss: 0.4879, rho_lr: 0.020000\n",
            "rho0: 0.9718, rho1: 0.2813\n",
            "Epoch 37, Loss: 0.4872, rho_lr: 0.020000\n",
            "rho0: 0.9716, rho1: 0.2623\n",
            "Epoch 38, Loss: 0.4868, rho_lr: 0.020000\n",
            "rho0: 0.9738, rho1: 0.2415\n",
            "Epoch 39, Loss: 0.4867, rho_lr: 0.020000\n",
            "rho0: 0.9736, rho1: 0.2941\n",
            "Epoch 40, Loss: 0.4866, rho_lr: 0.002000\n",
            "rho0: 0.9741, rho1: 0.2726\n",
            "Epoch 41, Loss: 0.4865, rho_lr: 0.002000\n",
            "rho0: 0.9736, rho1: 0.2755\n",
            "Epoch 42, Loss: 0.4865, rho_lr: 0.002000\n",
            "rho0: 0.9743, rho1: 0.3116\n",
            "Epoch 43, Loss: 0.4865, rho_lr: 0.002000\n",
            "rho0: 0.9764, rho1: 0.2366\n",
            "Epoch 44, Loss: 0.4863, rho_lr: 0.002000\n",
            "rho0: 0.9752, rho1: 0.3051\n",
            "Epoch 45, Loss: 0.4857, rho_lr: 0.002000\n",
            "rho0: 0.9750, rho1: 0.2954\n",
            "Epoch 46, Loss: 0.4865, rho_lr: 0.002000\n",
            "rho0: 0.9743, rho1: 0.2658\n",
            "Epoch 47, Loss: 0.4862, rho_lr: 0.002000\n",
            "rho0: 0.9765, rho1: 0.3252\n",
            "Epoch 48, Loss: 0.4860, rho_lr: 0.002000\n",
            "rho0: 0.9770, rho1: 0.3084\n",
            "Epoch 49, Loss: 0.4857, rho_lr: 0.002000\n",
            "rho0: 0.9750, rho1: 0.3043\n",
            "Epoch 50, Loss: 0.4859, rho_lr: 0.002000\n",
            "rho0: 0.9761, rho1: 0.2957\n",
            "Epoch 51, Loss: 0.4856, rho_lr: 0.002000\n",
            "rho0: 0.9764, rho1: 0.3395\n",
            "Epoch 52, Loss: 0.4857, rho_lr: 0.002000\n",
            "rho0: 0.9765, rho1: 0.3440\n",
            "Epoch 53, Loss: 0.4858, rho_lr: 0.002000\n",
            "rho0: 0.9759, rho1: 0.3486\n",
            "Epoch 54, Loss: 0.4852, rho_lr: 0.002000\n",
            "rho0: 0.9762, rho1: 0.3166\n",
            "Epoch 55, Loss: 0.4856, rho_lr: 0.002000\n",
            "rho0: 0.9760, rho1: 0.3310\n",
            "Epoch 56, Loss: 0.4855, rho_lr: 0.002000\n",
            "rho0: 0.9758, rho1: 0.3543\n",
            "Epoch 57, Loss: 0.4854, rho_lr: 0.002000\n",
            "rho0: 0.9764, rho1: 0.3417\n",
            "Epoch 58, Loss: 0.4854, rho_lr: 0.002000\n",
            "rho0: 0.9768, rho1: 0.3636\n",
            "Epoch 59, Loss: 0.4848, rho_lr: 0.002000\n",
            "rho0: 0.9766, rho1: 0.3704\n",
            "Epoch 60, Loss: 0.4855, rho_lr: 0.000200\n",
            "rho0: 0.9756, rho1: 0.3583\n",
            "Epoch 61, Loss: 0.4852, rho_lr: 0.000200\n",
            "rho0: 0.9764, rho1: 0.3583\n",
            "Epoch 62, Loss: 0.4849, rho_lr: 0.000200\n",
            "rho0: 0.9764, rho1: 0.3667\n",
            "Epoch 63, Loss: 0.4853, rho_lr: 0.000200\n",
            "rho0: 0.9764, rho1: 0.3556\n",
            "Epoch 64, Loss: 0.4850, rho_lr: 0.000200\n",
            "rho0: 0.9765, rho1: 0.3645\n",
            "Epoch 65, Loss: 0.4847, rho_lr: 0.000200\n",
            "rho0: 0.9766, rho1: 0.3779\n",
            "Epoch 66, Loss: 0.4851, rho_lr: 0.000200\n",
            "rho0: 0.9768, rho1: 0.3765\n",
            "Epoch 67, Loss: 0.4852, rho_lr: 0.000200\n",
            "rho0: 0.9767, rho1: 0.3788\n",
            "Epoch 68, Loss: 0.4856, rho_lr: 0.000200\n",
            "rho0: 0.9753, rho1: 0.3311\n",
            "Epoch 69, Loss: 0.4848, rho_lr: 0.000200\n",
            "rho0: 0.9762, rho1: 0.3567\n",
            "Epoch 70, Loss: 0.4847, rho_lr: 0.000200\n",
            "rho0: 0.9762, rho1: 0.3674\n",
            "Epoch 71, Loss: 0.4847, rho_lr: 0.000200\n",
            "rho0: 0.9767, rho1: 0.3752\n",
            "Epoch 72, Loss: 0.4847, rho_lr: 0.000200\n",
            "rho0: 0.9768, rho1: 0.3770\n",
            "Epoch 73, Loss: 0.4851, rho_lr: 0.000200\n",
            "rho0: 0.9763, rho1: 0.3594\n",
            "Epoch 74, Loss: 0.4846, rho_lr: 0.000200\n",
            "rho0: 0.9764, rho1: 0.3793\n",
            "Epoch 75, Loss: 0.4846, rho_lr: 0.000200\n",
            "rho0: 0.9766, rho1: 0.3857\n",
            "Epoch 76, Loss: 0.4850, rho_lr: 0.000200\n",
            "rho0: 0.9765, rho1: 0.3878\n",
            "Epoch 77, Loss: 0.4848, rho_lr: 0.000200\n",
            "rho0: 0.9768, rho1: 0.3872\n",
            "Epoch 78, Loss: 0.4846, rho_lr: 0.000200\n",
            "rho0: 0.9769, rho1: 0.4001\n",
            "Epoch 79, Loss: 0.4848, rho_lr: 0.000200\n",
            "rho0: 0.9769, rho1: 0.3873\n",
            "Epoch 80, Loss: 0.4847, rho_lr: 0.000020\n",
            "rho0: 0.9768, rho1: 0.3954\n",
            "Epoch 81, Loss: 0.4842, rho_lr: 0.000020\n",
            "rho0: 0.9768, rho1: 0.3969\n",
            "Epoch 82, Loss: 0.4846, rho_lr: 0.000020\n",
            "rho0: 0.9768, rho1: 0.3975\n",
            "Epoch 83, Loss: 0.4848, rho_lr: 0.000020\n",
            "rho0: 0.9768, rho1: 0.3959\n",
            "Epoch 84, Loss: 0.4849, rho_lr: 0.000020\n",
            "rho0: 0.9769, rho1: 0.3955\n",
            "Epoch 85, Loss: 0.4844, rho_lr: 0.000020\n",
            "rho0: 0.9769, rho1: 0.3973\n",
            "Epoch 86, Loss: 0.4844, rho_lr: 0.000020\n",
            "rho0: 0.9769, rho1: 0.3990\n",
            "Epoch 87, Loss: 0.4848, rho_lr: 0.000020\n",
            "rho0: 0.9769, rho1: 0.3972\n",
            "Epoch 88, Loss: 0.4845, rho_lr: 0.000020\n",
            "rho0: 0.9770, rho1: 0.3990\n",
            "Epoch 89, Loss: 0.4845, rho_lr: 0.000020\n",
            "rho0: 0.9769, rho1: 0.4002\n",
            "Epoch 90, Loss: 0.4843, rho_lr: 0.000020\n",
            "rho0: 0.9770, rho1: 0.4009\n",
            "Epoch 91, Loss: 0.4845, rho_lr: 0.000020\n",
            "rho0: 0.9770, rho1: 0.4004\n",
            "Epoch 92, Loss: 0.4847, rho_lr: 0.000020\n",
            "rho0: 0.9770, rho1: 0.3996\n",
            "Epoch 93, Loss: 0.4845, rho_lr: 0.000020\n",
            "rho0: 0.9771, rho1: 0.4020\n",
            "Epoch 94, Loss: 0.4844, rho_lr: 0.000020\n",
            "rho0: 0.9771, rho1: 0.4021\n",
            "Epoch 95, Loss: 0.4842, rho_lr: 0.000020\n",
            "rho0: 0.9771, rho1: 0.4050\n",
            "Epoch 96, Loss: 0.4843, rho_lr: 0.000020\n",
            "rho0: 0.9771, rho1: 0.4057\n",
            "Epoch 97, Loss: 0.4843, rho_lr: 0.000020\n",
            "rho0: 0.9771, rho1: 0.4052\n",
            "Epoch 98, Loss: 0.4843, rho_lr: 0.000020\n",
            "rho0: 0.9771, rho1: 0.4070\n",
            "Epoch 99, Loss: 0.4843, rho_lr: 0.000020\n",
            "rho0: 0.9771, rho1: 0.4080\n",
            "Epoch 100, Loss: 0.4844, rho_lr: 0.000002\n",
            "rho0: 0.9772, rho1: 0.4089\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4844406757354736"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b_loss = evaluate_model(model_b, diffusion_b, val_data, compute_adjusted_loss)\n",
        "print(f\"Shallow architecture loss on validation set: {b_loss}\")"
      ],
      "metadata": {
        "id": "7pgDvwHiPEnl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d13d2094-6700-4d60-9773-f154462f6770"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shallow architecture loss on validation set: 0.4844267252087593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_c = ScoreNet(channels=[64, 128, 256, 512])\n",
        "model_c.to(device)\n",
        "diffusion_c = Diffusion(model_c, n_steps=200, device=device, min_beta=1e-4, max_beta=0.01)\n",
        "rho_params = [model_c.rho0, model_c.rho1]\n",
        "rho_ids = set(id(p) for p in rho_params)\n",
        "base_params = [p for p in model_c.parameters() if id(p) not in rho_ids]\n",
        "optimizer_c = torch.optim.Adam([\n",
        "    {\"params\": base_params, \"lr\": 0.001},\n",
        "    {\"params\": rho_params, \"lr\": 0.2}\n",
        "])\n",
        "train(model_c, diffusion_c, train_data, optimizer_c, compute_adjusted_loss, num_epochs=100)"
      ],
      "metadata": {
        "id": "QU3Xu8B1hB04",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7848a157-7dc7-4ff5-9cc6-7d74480b6382"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 7.1302, rho_lr: 0.200000\n",
            "rho0: 0.2415, rho1: 0.5000\n",
            "Epoch 2, Loss: 1.2157, rho_lr: 0.200000\n",
            "rho0: 0.3438, rho1: 0.5000\n",
            "Epoch 3, Loss: 0.8457, rho_lr: 0.200000\n",
            "rho0: 0.3850, rho1: 0.5000\n",
            "Epoch 4, Loss: 0.6524, rho_lr: 0.200000\n",
            "rho0: 0.4351, rho1: 0.5000\n",
            "Epoch 5, Loss: 0.6081, rho_lr: 0.200000\n",
            "rho0: 0.4715, rho1: 0.5000\n",
            "Epoch 6, Loss: 0.5825, rho_lr: 0.200000\n",
            "rho0: 0.4985, rho1: 0.5000\n",
            "Epoch 7, Loss: 0.5492, rho_lr: 0.200000\n",
            "rho0: 0.5350, rho1: 0.5000\n",
            "Epoch 8, Loss: 0.5513, rho_lr: 0.200000\n",
            "rho0: 0.5634, rho1: 0.5000\n",
            "Epoch 9, Loss: 0.5351, rho_lr: 0.200000\n",
            "rho0: 0.5810, rho1: 0.5000\n",
            "Epoch 10, Loss: 0.5174, rho_lr: 0.200000\n",
            "rho0: 0.6236, rho1: 0.5000\n",
            "Epoch 11, Loss: 0.5031, rho_lr: 0.200000\n",
            "rho0: 0.8013, rho1: 0.2000\n",
            "Epoch 12, Loss: 0.4953, rho_lr: 0.200000\n",
            "rho0: 0.8088, rho1: 0.2000\n",
            "Epoch 13, Loss: 0.4944, rho_lr: 0.200000\n",
            "rho0: 0.8171, rho1: 0.2000\n",
            "Epoch 14, Loss: 0.4940, rho_lr: 0.200000\n",
            "rho0: 0.8252, rho1: 0.2000\n",
            "Epoch 15, Loss: 0.4940, rho_lr: 0.200000\n",
            "rho0: 0.8337, rho1: 0.2000\n",
            "Epoch 16, Loss: 0.4962, rho_lr: 0.200000\n",
            "rho0: 0.8383, rho1: 0.2000\n",
            "Epoch 17, Loss: 0.4986, rho_lr: 0.200000\n",
            "rho0: 0.8467, rho1: 0.2000\n",
            "Epoch 18, Loss: 0.4925, rho_lr: 0.200000\n",
            "rho0: 0.8547, rho1: 0.2000\n",
            "Epoch 19, Loss: 0.4974, rho_lr: 0.200000\n",
            "rho0: 0.8661, rho1: 0.2000\n",
            "Epoch 20, Loss: 0.4918, rho_lr: 0.020000\n",
            "rho0: 0.8693, rho1: 0.2000\n",
            "Epoch 21, Loss: 0.4924, rho_lr: 0.020000\n",
            "rho0: 0.8722, rho1: 0.2000\n",
            "Epoch 22, Loss: 0.4913, rho_lr: 0.020000\n",
            "rho0: 0.8788, rho1: 0.2000\n",
            "Epoch 23, Loss: 0.4932, rho_lr: 0.020000\n",
            "rho0: 0.8831, rho1: 0.2000\n",
            "Epoch 24, Loss: 0.4910, rho_lr: 0.020000\n",
            "rho0: 0.8897, rho1: 0.2000\n",
            "Epoch 25, Loss: 0.4910, rho_lr: 0.020000\n",
            "rho0: 0.8932, rho1: 0.2000\n",
            "Epoch 26, Loss: 0.4906, rho_lr: 0.020000\n",
            "rho0: 0.8963, rho1: 0.2000\n",
            "Epoch 27, Loss: 0.4896, rho_lr: 0.020000\n",
            "rho0: 0.9061, rho1: 0.2000\n",
            "Epoch 28, Loss: 0.4923, rho_lr: 0.020000\n",
            "rho0: 0.9088, rho1: 0.2018\n",
            "Epoch 29, Loss: 0.4888, rho_lr: 0.020000\n",
            "rho0: 0.9143, rho1: 0.2000\n",
            "Epoch 30, Loss: 0.4883, rho_lr: 0.020000\n",
            "rho0: 0.9159, rho1: 0.2000\n",
            "Epoch 31, Loss: 0.4895, rho_lr: 0.020000\n",
            "rho0: 0.9193, rho1: 0.2000\n",
            "Epoch 32, Loss: 0.4882, rho_lr: 0.020000\n",
            "rho0: 0.9275, rho1: 0.2000\n",
            "Epoch 33, Loss: 0.4883, rho_lr: 0.020000\n",
            "rho0: 0.9309, rho1: 0.2000\n",
            "Epoch 34, Loss: 0.4876, rho_lr: 0.020000\n",
            "rho0: 0.9351, rho1: 0.2000\n",
            "Epoch 35, Loss: 0.4885, rho_lr: 0.020000\n",
            "rho0: 0.9419, rho1: 0.2000\n",
            "Epoch 36, Loss: 0.4882, rho_lr: 0.020000\n",
            "rho0: 0.9362, rho1: 0.2422\n",
            "Epoch 37, Loss: 0.4874, rho_lr: 0.020000\n",
            "rho0: 0.9439, rho1: 0.2000\n",
            "Epoch 38, Loss: 0.4872, rho_lr: 0.020000\n",
            "rho0: 0.9471, rho1: 0.2103\n",
            "Epoch 39, Loss: 0.4875, rho_lr: 0.020000\n",
            "rho0: 0.9499, rho1: 0.2000\n",
            "Epoch 40, Loss: 0.4871, rho_lr: 0.002000\n",
            "rho0: 0.9544, rho1: 0.2000\n",
            "Epoch 41, Loss: 0.4864, rho_lr: 0.002000\n",
            "rho0: 0.9543, rho1: 0.2074\n",
            "Epoch 42, Loss: 0.4866, rho_lr: 0.002000\n",
            "rho0: 0.9555, rho1: 0.2179\n",
            "Epoch 43, Loss: 0.4867, rho_lr: 0.002000\n",
            "rho0: 0.9580, rho1: 0.2000\n",
            "Epoch 44, Loss: 0.4862, rho_lr: 0.002000\n",
            "rho0: 0.9606, rho1: 0.2093\n",
            "Epoch 45, Loss: 0.4864, rho_lr: 0.002000\n",
            "rho0: 0.9617, rho1: 0.2000\n",
            "Epoch 46, Loss: 0.4864, rho_lr: 0.002000\n",
            "rho0: 0.9623, rho1: 0.2138\n",
            "Epoch 47, Loss: 0.4876, rho_lr: 0.002000\n",
            "rho0: 0.9640, rho1: 0.2019\n",
            "Epoch 48, Loss: 0.4858, rho_lr: 0.002000\n",
            "rho0: 0.9640, rho1: 0.2475\n",
            "Epoch 49, Loss: 0.4859, rho_lr: 0.002000\n",
            "rho0: 0.9656, rho1: 0.2393\n",
            "Epoch 50, Loss: 0.4857, rho_lr: 0.002000\n",
            "rho0: 0.9662, rho1: 0.2199\n",
            "Epoch 51, Loss: 0.4855, rho_lr: 0.002000\n",
            "rho0: 0.9664, rho1: 0.2640\n",
            "Epoch 52, Loss: 0.4854, rho_lr: 0.002000\n",
            "rho0: 0.9661, rho1: 0.2773\n",
            "Epoch 53, Loss: 0.4855, rho_lr: 0.002000\n",
            "rho0: 0.9672, rho1: 0.2690\n",
            "Epoch 54, Loss: 0.4855, rho_lr: 0.002000\n",
            "rho0: 0.9676, rho1: 0.2778\n",
            "Epoch 55, Loss: 0.4856, rho_lr: 0.002000\n",
            "rho0: 0.9682, rho1: 0.2567\n",
            "Epoch 56, Loss: 0.4856, rho_lr: 0.002000\n",
            "rho0: 0.9686, rho1: 0.2442\n",
            "Epoch 57, Loss: 0.4853, rho_lr: 0.002000\n",
            "rho0: 0.9685, rho1: 0.2732\n",
            "Epoch 58, Loss: 0.4850, rho_lr: 0.002000\n",
            "rho0: 0.9694, rho1: 0.2458\n",
            "Epoch 59, Loss: 0.4848, rho_lr: 0.002000\n",
            "rho0: 0.9693, rho1: 0.3170\n",
            "Epoch 60, Loss: 0.4852, rho_lr: 0.000200\n",
            "rho0: 0.9694, rho1: 0.2824\n",
            "Epoch 61, Loss: 0.4849, rho_lr: 0.000200\n",
            "rho0: 0.9702, rho1: 0.3039\n",
            "Epoch 62, Loss: 0.4851, rho_lr: 0.000200\n",
            "rho0: 0.9703, rho1: 0.3040\n",
            "Epoch 63, Loss: 0.4858, rho_lr: 0.000200\n",
            "rho0: 0.9699, rho1: 0.2857\n",
            "Epoch 64, Loss: 0.4850, rho_lr: 0.000200\n",
            "rho0: 0.9706, rho1: 0.2974\n",
            "Epoch 65, Loss: 0.4848, rho_lr: 0.000200\n",
            "rho0: 0.9710, rho1: 0.3131\n",
            "Epoch 66, Loss: 0.4845, rho_lr: 0.000200\n",
            "rho0: 0.9713, rho1: 0.3214\n",
            "Epoch 67, Loss: 0.4846, rho_lr: 0.000200\n",
            "rho0: 0.9716, rho1: 0.3240\n",
            "Epoch 68, Loss: 0.4848, rho_lr: 0.000200\n",
            "rho0: 0.9718, rho1: 0.3350\n",
            "Epoch 69, Loss: 0.4846, rho_lr: 0.000200\n",
            "rho0: 0.9721, rho1: 0.3441\n",
            "Epoch 70, Loss: 0.4844, rho_lr: 0.000200\n",
            "rho0: 0.9720, rho1: 0.3604\n",
            "Epoch 71, Loss: 0.4846, rho_lr: 0.000200\n",
            "rho0: 0.9720, rho1: 0.3637\n",
            "Epoch 72, Loss: 0.4845, rho_lr: 0.000200\n",
            "rho0: 0.9722, rho1: 0.3644\n",
            "Epoch 73, Loss: 0.4847, rho_lr: 0.000200\n",
            "rho0: 0.9722, rho1: 0.3553\n",
            "Epoch 74, Loss: 0.4841, rho_lr: 0.000200\n",
            "rho0: 0.9722, rho1: 0.3698\n",
            "Epoch 75, Loss: 0.4844, rho_lr: 0.000200\n",
            "rho0: 0.9723, rho1: 0.3764\n",
            "Epoch 76, Loss: 0.4845, rho_lr: 0.000200\n",
            "rho0: 0.9723, rho1: 0.3846\n",
            "Epoch 77, Loss: 0.4844, rho_lr: 0.000200\n",
            "rho0: 0.9726, rho1: 0.3857\n",
            "Epoch 78, Loss: 0.4844, rho_lr: 0.000200\n",
            "rho0: 0.9726, rho1: 0.3917\n",
            "Epoch 79, Loss: 0.4841, rho_lr: 0.000200\n",
            "rho0: 0.9729, rho1: 0.3916\n",
            "Epoch 80, Loss: 0.4842, rho_lr: 0.000020\n",
            "rho0: 0.9730, rho1: 0.3984\n",
            "Epoch 81, Loss: 0.4844, rho_lr: 0.000020\n",
            "rho0: 0.9730, rho1: 0.3960\n",
            "Epoch 82, Loss: 0.4837, rho_lr: 0.000020\n",
            "rho0: 0.9731, rho1: 0.3976\n",
            "Epoch 83, Loss: 0.4840, rho_lr: 0.000020\n",
            "rho0: 0.9731, rho1: 0.3981\n",
            "Epoch 84, Loss: 0.4841, rho_lr: 0.000020\n",
            "rho0: 0.9731, rho1: 0.3995\n",
            "Epoch 85, Loss: 0.4842, rho_lr: 0.000020\n",
            "rho0: 0.9732, rho1: 0.4001\n",
            "Epoch 86, Loss: 0.4841, rho_lr: 0.000020\n",
            "rho0: 0.9733, rho1: 0.4013\n",
            "Epoch 87, Loss: 0.4838, rho_lr: 0.000020\n",
            "rho0: 0.9733, rho1: 0.4023\n",
            "Epoch 88, Loss: 0.4841, rho_lr: 0.000020\n",
            "rho0: 0.9735, rho1: 0.4018\n",
            "Epoch 89, Loss: 0.4841, rho_lr: 0.000020\n",
            "rho0: 0.9736, rho1: 0.4038\n",
            "Epoch 90, Loss: 0.4839, rho_lr: 0.000020\n",
            "rho0: 0.9737, rho1: 0.4049\n",
            "Epoch 91, Loss: 0.4839, rho_lr: 0.000020\n",
            "rho0: 0.9737, rho1: 0.4057\n",
            "Epoch 92, Loss: 0.4839, rho_lr: 0.000020\n",
            "rho0: 0.9737, rho1: 0.4071\n",
            "Epoch 93, Loss: 0.4837, rho_lr: 0.000020\n",
            "rho0: 0.9738, rho1: 0.4085\n",
            "Epoch 94, Loss: 0.4838, rho_lr: 0.000020\n",
            "rho0: 0.9738, rho1: 0.4099\n",
            "Epoch 95, Loss: 0.4841, rho_lr: 0.000020\n",
            "rho0: 0.9739, rho1: 0.4114\n",
            "Epoch 96, Loss: 0.4836, rho_lr: 0.000020\n",
            "rho0: 0.9739, rho1: 0.4128\n",
            "Epoch 97, Loss: 0.4838, rho_lr: 0.000020\n",
            "rho0: 0.9741, rho1: 0.4137\n",
            "Epoch 98, Loss: 0.4835, rho_lr: 0.000020\n",
            "rho0: 0.9742, rho1: 0.4163\n",
            "Epoch 99, Loss: 0.4840, rho_lr: 0.000020\n",
            "rho0: 0.9742, rho1: 0.4178\n",
            "Epoch 100, Loss: 0.4840, rho_lr: 0.000002\n",
            "rho0: 0.9743, rho1: 0.4193\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.48402223402261735"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c_loss = evaluate_model(model_c, diffusion_c, val_dat, compute_adjusted_loss)\n",
        "print(f\"Wide architecture loss loss on validation set: {c_loss}\")"
      ],
      "metadata": {
        "id": "R-d8ZkvqPKpK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55e1b881-e6ef-4257-89a1-f770cd688bd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wide architecture loss loss on validation set: 0.4840617722272873\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose to compare the original network with a shallower network and a deeper network to see if changing the depth or width of the network would affect the loss."
      ],
      "metadata": {
        "id": "e3UhUcvNjEur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8"
      ],
      "metadata": {
        "id": "VS1WRqVOjN5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def sample_reverse_process(diffusion, model, batch_size):\n",
        "    model.eval()\n",
        "    device = diffusion.device\n",
        "    T = diffusion.n_steps\n",
        "\n",
        "    x_t = torch.randn((batch_size, 1, 28, 28), device=device)\n",
        "\n",
        "    for t in reversed(range(T)):\n",
        "        t_tensor = torch.full((batch_size,), t, device=device, dtype=torch.long)\n",
        "        mu = diffusion.predict_next(x_t, t_tensor)\n",
        "\n",
        "        if t > 0:\n",
        "            z = torch.randn_like(x_t)\n",
        "            alpha_t = diffusion.alpha[t_tensor].view(batch_size, 1, 1, 1)\n",
        "            x_t = mu + torch.sqrt(1.0 - alpha_t) * z\n",
        "        else:\n",
        "            x_t = mu.clamp(0, 1)\n",
        "\n",
        "    return x_t"
      ],
      "metadata": {
        "id": "zXVHlNrjnsL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9"
      ],
      "metadata": {
        "id": "AYTBrgpnjRb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def matrix_sqrt(A):\n",
        "    eigenvalues, eigenvectors = np.linalg.eigh(A)\n",
        "    eigenvalues = np.maximum(eigenvalues, 0)\n",
        "    eigenvalues = np.sqrt(eigenvalues)\n",
        "    return eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n",
        "\n",
        "def fid(mu1, sigma1, mu2, sigma2):\n",
        "    return np.linalg.norm(mu1 - mu2) ** 2 + np.trace(sigma1 + sigma2 - 2*matrix_sqrt(matrix_sqrt(sigma1) @ sigma2 @ matrix_sqrt(sigma1)))"
      ],
      "metadata": {
        "id": "r5gRGv9MjSXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "samples = sample_reverse_process(diffusion_c, model_c, 20)\n",
        "samples = samples.cpu().squeeze()\n",
        "\n",
        "fig, axs = plt.subplots(5, 4, figsize=(8, 10))\n",
        "for i, ax in enumerate(axs.flat):\n",
        "    ax.imshow(samples[i], cmap='gray')\n",
        "    ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6XVpsNYm1uRE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c6da9789-6818-46cf-c3fc-b71c71e895bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x1000 with 20 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mu_pop = np.mean(test_data.cpu().numpy().reshape(len(test_data), -1), axis=0)\n",
        "sigma_pop = np.cov(test_data.cpu().numpy().reshape(len(test_data), -1), rowvar=False)\n",
        "\n",
        "samples = sample_reverse_process(diffusion_c, model_c, batch_size=1000)\n",
        "samples = samples.cpu().numpy().reshape(1000, -1)\n",
        "samples.astype(np.float32)\n",
        "\n",
        "mu_gen = np.mean(samples, axis=0)\n",
        "sigma_gen = np.cov(samples, rowvar=False)\n",
        "\n",
        "fid_score = fid(mu_pop, sigma_pop, mu_gen, sigma_gen)\n",
        "print(f\"FID score: {fid_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkrUowX43H3a",
        "outputId": "67ae3d23-7f48-4f00-bcde-7fbf89b1f239"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FID score: 5.7456810222031205\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code II"
      ],
      "metadata": {
        "id": "prOI9-FPqMPE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1"
      ],
      "metadata": {
        "id": "sqkzk074D5iV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from matplotlib import pyplot as plt\n",
        "from torch.nn import functional as F\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class ScoreNet2(nn.Module):\n",
        "    def __init__(self, channels=[32, 64, 128, 256], embed_dim=256, group_num=4):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Sequential(GaussianFourierProjection(embed_dim=embed_dim),\n",
        "         nn.Linear(embed_dim, embed_dim))\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, channels[0], 3, stride=1, bias=False)\n",
        "        self.dense1 = Dense(embed_dim, channels[0])\n",
        "        self.gnorm1 = nn.GroupNorm(group_num, num_channels=channels[0])\n",
        "\n",
        "        self.conv2 = nn.Conv2d(channels[0], channels[1], 3, stride=2, bias=False)\n",
        "        self.dense2 = Dense(embed_dim, channels[1])\n",
        "        self.gnorm2 = nn.GroupNorm(group_num, num_channels=channels[1])\n",
        "\n",
        "        self.conv3 = nn.Conv2d(channels[1], channels[2], 3, stride=2, bias=False)\n",
        "        self.dense3 = Dense(embed_dim, channels[2])\n",
        "        self.gnorm3 = nn.GroupNorm(group_num, num_channels=channels[2])\n",
        "\n",
        "        self.conv4 = nn.Conv2d(channels[2], channels[3], 3, stride=2, bias=False)\n",
        "        self.dense4 = Dense(embed_dim, channels[3])\n",
        "        self.gnorm4 = nn.GroupNorm(group_num, num_channels=channels[3])\n",
        "\n",
        "        self.tconv4 = nn.ConvTranspose2d(channels[3], channels[2], 3, stride=2, bias=False)\n",
        "        self.dense5 = Dense(embed_dim, channels[2])\n",
        "        self.tgnorm4 = nn.GroupNorm(group_num, num_channels=channels[2])\n",
        "\n",
        "        self.tconv3 = nn.ConvTranspose2d(channels[2] + channels[2], channels[1], 3, stride=2, bias=False, output_padding=1)\n",
        "        self.dense6 = Dense(embed_dim, channels[1])\n",
        "        self.tgnorm3 = nn.GroupNorm(group_num, num_channels=channels[1])\n",
        "\n",
        "        self.tconv2 = nn.ConvTranspose2d(channels[1] + channels[1], channels[0], 3, stride=2, bias=False, output_padding=1)\n",
        "        self.dense7 = Dense(embed_dim, channels[0])\n",
        "        self.tgnorm2 = nn.GroupNorm(group_num, num_channels=channels[0])\n",
        "\n",
        "        self.tconv1 = nn.ConvTranspose2d(channels[0] + channels[0], 1, 3, stride=1)\n",
        "\n",
        "        # The swish activation function\n",
        "        self.act = lambda x: x * torch.sigmoid(x)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        embed = self.act(self.embed(t))\n",
        "\n",
        "        eps1 = self.conv1(x)\n",
        "        eps1 += self.dense1(embed)\n",
        "        eps1 = self.gnorm1(eps1)\n",
        "        eps1 = self.act(eps1)\n",
        "\n",
        "        eps2 = self.conv2(eps1)\n",
        "        eps2 += self.dense2(embed)\n",
        "        eps2 = self.gnorm2(eps2)\n",
        "        eps2 = self.act(eps2)\n",
        "\n",
        "        eps3 = self.conv3(eps2)\n",
        "        eps3 += self.dense3(embed)\n",
        "        eps3 = self.gnorm3(eps3)\n",
        "        eps3 = self.act(eps3)\n",
        "\n",
        "        eps4 = self.conv4(eps3)\n",
        "        eps4 += self.dense4(embed)\n",
        "        eps4 = self.gnorm4(eps4)\n",
        "        eps4 = self.act(eps4)\n",
        "\n",
        "        eps = self.tconv4(eps4)\n",
        "        eps += self.dense5(embed)\n",
        "        eps = self.tgnorm4(eps)\n",
        "        eps = self.act(eps)\n",
        "\n",
        "        eps = self.tconv3(torch.cat([eps, eps3], dim=1))\n",
        "        eps += self.dense6(embed)\n",
        "        eps = self.tgnorm3(eps)\n",
        "        eps = self.act(eps)\n",
        "\n",
        "        eps = self.tconv2(torch.cat([eps, eps2], dim=1))\n",
        "        eps += self.dense7(embed)\n",
        "        eps = self.tgnorm2(eps)\n",
        "        eps = self.act(eps)\n",
        "\n",
        "        eps = self.tconv1(torch.cat([eps, eps1], dim=1))\n",
        "\n",
        "        return eps\n",
        "\n",
        "class Diffusion2(nn.Module):\n",
        "    def __init__(self, model, n_steps, device, min_beta=0.0001, max_beta=0.02):\n",
        "        # Store beta, alpha and \\bar alpha.\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.n_steps = n_steps\n",
        "        self.device = device\n",
        "\n",
        "        self.beta = torch.linspace(min_beta, max_beta, n_steps).to(device)\n",
        "        self.alpha = 1.0 - self.beta\n",
        "        self.alpha_bar = torch.cumprod(self.alpha, dim=0).to(device)\n",
        "\n",
        "    def forward_process(self, x0, t):\n",
        "        # Sample x_{t-1},x_t given x_0\n",
        "        alpha_bar_tm1 = self.alpha_bar[t - 1].view(-1, 1, 1, 1)\n",
        "        alpha_t = self.alpha[t].view(-1, 1, 1, 1)\n",
        "        alpha_bar_t = self.alpha_bar[t].view(-1, 1, 1, 1)\n",
        "\n",
        "        noise1 = torch.randn_like(x0)\n",
        "        x_tm1 = torch.sqrt(alpha_bar_tm1) * x0 + torch.sqrt(1 - alpha_bar_tm1) * noise1\n",
        "\n",
        "        noise2 = torch.randn_like(x0)\n",
        "        x_t = torch.sqrt(alpha_t) * x_tm1 + torch.sqrt(1 - alpha_t) * noise2\n",
        "        return x_tm1, x_t\n",
        "\n",
        "    def predict_next(self, x_t, t):\n",
        "        # Compute mu(xt,t)\n",
        "        return self.model(x_t, t)\n",
        "\n",
        "    def sample_xt_given_x0(self, x0, t):\n",
        "        alpha_bar_t = self.alpha_bar[t].view(-1, 1, 1, 1)\n",
        "        noise = torch.randn_like(x0)\n",
        "        x_t = torch.sqrt(alpha_bar_t) * x0 + torch.sqrt(1 - alpha_bar_t) * noise\n",
        "        return x_t\n",
        "\n",
        "def compute_loss2(diffusion, model, x0):\n",
        "    B = x0.shape[0]\n",
        "    device = x0.device\n",
        "    T = diffusion.n_steps\n",
        "    t = torch.randint(1, T, (B,), device=device)\n",
        "    epsilon = torch.randn_like(x0)\n",
        "\n",
        "    alpha_t = diffusion.alpha[t].view(B, 1, 1, 1)\n",
        "    alpha_bar_t = diffusion.alpha_bar[t].view(B, 1, 1, 1)\n",
        "\n",
        "    x_t = torch.sqrt(alpha_bar_t) * x0 + torch.sqrt(1 - alpha_bar_t) * epsilon\n",
        "\n",
        "    eps_pred = model(x_t, t)\n",
        "\n",
        "    weight = (1 - alpha_t) / (2 * alpha_t * (1 - alpha_bar_t))\n",
        "    loss = weight.view(B) * ((epsilon - eps_pred) ** 2).view(B, -1).mean(dim=1)\n",
        "    return loss.mean()\n",
        "\n",
        "def train_code2(model, diffusion, train_data, val_data, optimizer, loss_fn, num_epochs=60, batch_size=100):\n",
        "    device = diffusion.device\n",
        "    train_dataset = torch.utils.data.TensorDataset(train_data)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    val_dataset = torch.utils.data.TensorDataset(val_data)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "        for (x0_batch,) in train_loader:\n",
        "            x0 = x0_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss = loss_fn(diffusion, model, x0)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = epoch_loss / len(train_loader)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for (x0_batch,) in val_loader:\n",
        "                x0 = x0_batch.to(device)\n",
        "                loss = loss_fn(diffusion, model, x0)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss}, Val Loss: {avg_val_loss}\")"
      ],
      "metadata": {
        "id": "Y2TgyMO-rFPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, val_data, test_data = get_mnist()\n",
        "\n",
        "model = ScoreNet2().to(device)\n",
        "diffusion = Diffusion2(model, n_steps=200, device=device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "train_code2(model, diffusion, train_data, val_data, optimizer, compute_loss2, num_epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8U0kNmOzdSB",
        "outputId": "1978d854-b0e5-47e1-97bc-286453eaac07",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Loss: 0.01738809134811163, Val Loss: 0.01081402603769675\n",
            "Epoch 2, Train Loss: 0.010226392120588571, Val Loss: 0.0100587550830096\n",
            "Epoch 3, Train Loss: 0.008896564358379691, Val Loss: 0.007421897421590984\n",
            "Epoch 4, Train Loss: 0.006071869387757033, Val Loss: 0.004922879147343338\n",
            "Epoch 5, Train Loss: 0.004975866341032088, Val Loss: 0.004616801892407239\n",
            "Epoch 6, Train Loss: 0.004177857598988339, Val Loss: 0.0040808379964437334\n",
            "Epoch 7, Train Loss: 0.0033995047614444047, Val Loss: 0.0031415442563593387\n",
            "Epoch 8, Train Loss: 0.0031029752988833933, Val Loss: 0.0029722211568150667\n",
            "Epoch 9, Train Loss: 0.0028556813031900673, Val Loss: 0.004636967942351475\n",
            "Epoch 10, Train Loss: 0.0031626534138340504, Val Loss: 0.00280512121738866\n",
            "Epoch 11, Train Loss: 0.002587103643454611, Val Loss: 0.0027243248035665603\n",
            "Epoch 12, Train Loss: 0.002623216218315065, Val Loss: 0.002291754926554859\n",
            "Epoch 13, Train Loss: 0.0089852963257581, Val Loss: 0.006209969168994575\n",
            "Epoch 14, Train Loss: 0.00445988158590626, Val Loss: 0.0044806888524908575\n",
            "Epoch 15, Train Loss: 0.003950075590284542, Val Loss: 0.0036498348147142676\n",
            "Epoch 16, Train Loss: 0.004033293661661446, Val Loss: 0.003786865210859105\n",
            "Epoch 17, Train Loss: 0.003952157676685601, Val Loss: 0.0034575399314053356\n",
            "Epoch 18, Train Loss: 0.0035048460193211214, Val Loss: 0.003555098567157984\n",
            "Epoch 19, Train Loss: 0.0056825758683262395, Val Loss: 0.006037600425770506\n",
            "Epoch 20, Train Loss: 0.005284756433451548, Val Loss: 0.004709141386556439\n",
            "Epoch 21, Train Loss: 0.004208578125573695, Val Loss: 0.004035618515918032\n",
            "Epoch 22, Train Loss: 0.006258953182725236, Val Loss: 0.008965317893307656\n",
            "Epoch 23, Train Loss: 0.005936554670799524, Val Loss: 0.005634060658048838\n",
            "Epoch 24, Train Loss: 0.004809279970824719, Val Loss: 0.0038270014664158226\n",
            "Epoch 25, Train Loss: 0.004393361213151366, Val Loss: 0.004544725284795277\n",
            "Epoch 26, Train Loss: 0.005023545015021227, Val Loss: 0.004896957240998745\n",
            "Epoch 27, Train Loss: 0.004343200895702466, Val Loss: 0.0041973083047196266\n",
            "Epoch 28, Train Loss: 0.0042548351873410864, Val Loss: 0.004784653487149626\n",
            "Epoch 29, Train Loss: 0.004493520117830485, Val Loss: 0.004293710659840144\n",
            "Epoch 30, Train Loss: 0.004332279232214205, Val Loss: 0.004019843017449603\n",
            "Epoch 31, Train Loss: 0.004391645478783175, Val Loss: 0.0033820255048340186\n",
            "Epoch 32, Train Loss: 0.003427580243558623, Val Loss: 0.0033609458757564424\n",
            "Epoch 33, Train Loss: 0.0034389915495412426, Val Loss: 0.003579366910853423\n",
            "Epoch 34, Train Loss: 0.0036170677022309976, Val Loss: 0.0035288668918656185\n",
            "Epoch 35, Train Loss: 0.0029657892608083783, Val Loss: 0.002379905817215331\n",
            "Epoch 36, Train Loss: 0.003402186684543267, Val Loss: 0.0027776626194827257\n",
            "Epoch 37, Train Loss: 0.002319753388990648, Val Loss: 0.0022333660232834516\n",
            "Epoch 38, Train Loss: 0.002217314475448802, Val Loss: 0.0021170334232738243\n",
            "Epoch 39, Train Loss: 0.0021287195099284873, Val Loss: 0.002188837854191661\n",
            "Epoch 40, Train Loss: 0.0020691994675435125, Val Loss: 0.0019213017995934932\n",
            "Epoch 41, Train Loss: 0.0020898502302588895, Val Loss: 0.0019947971287183463\n",
            "Epoch 42, Train Loss: 0.002099540621275082, Val Loss: 0.0020733145106351003\n",
            "Epoch 43, Train Loss: 0.002039141955669038, Val Loss: 0.0019405784131959081\n",
            "Epoch 44, Train Loss: 0.0029417254582513124, Val Loss: 0.0025393172551412134\n",
            "Epoch 45, Train Loss: 0.0027518474301323295, Val Loss: 0.0030014078901149334\n",
            "Epoch 46, Train Loss: 0.002579038807656616, Val Loss: 0.0023557728074956683\n",
            "Epoch 47, Train Loss: 0.002373751796898432, Val Loss: 0.002305491673760116\n",
            "Epoch 48, Train Loss: 0.0021521949687739834, Val Loss: 0.002127427288796753\n",
            "Epoch 49, Train Loss: 0.0020807096385397016, Val Loss: 0.002068791262572631\n",
            "Epoch 50, Train Loss: 0.002050314049934968, Val Loss: 0.001900767055922188\n",
            "Epoch 51, Train Loss: 0.002124780169222504, Val Loss: 0.0028287642810028045\n",
            "Epoch 52, Train Loss: 0.002475522890686989, Val Loss: 0.002028874066891149\n",
            "Epoch 53, Train Loss: 0.0020507889370201156, Val Loss: 0.002074753760243766\n",
            "Epoch 54, Train Loss: 0.0021162102641537786, Val Loss: 0.0021508874755818397\n",
            "Epoch 55, Train Loss: 0.002060783829190768, Val Loss: 0.0026844006980536505\n",
            "Epoch 56, Train Loss: 0.0020844155641971157, Val Loss: 0.0019056977663422002\n",
            "Epoch 57, Train Loss: 0.002126673577586189, Val Loss: 0.001991746622370556\n",
            "Epoch 58, Train Loss: 0.0020236091604456307, Val Loss: 0.0021038848342141135\n",
            "Epoch 59, Train Loss: 0.002075427149422467, Val Loss: 0.0019023232336621732\n",
            "Epoch 60, Train Loss: 0.0020113867345498875, Val Loss: 0.0019534830475458876\n",
            "Epoch 61, Train Loss: 0.0025047854895237833, Val Loss: 0.0025057413277681916\n",
            "Epoch 62, Train Loss: 0.0021387503936421124, Val Loss: 0.0020518505142536015\n",
            "Epoch 63, Train Loss: 0.0020607723097782584, Val Loss: 0.0021257108461577444\n",
            "Epoch 64, Train Loss: 0.002032877242076211, Val Loss: 0.0021285265503684057\n",
            "Epoch 65, Train Loss: 0.0020356696699745953, Val Loss: 0.002277832924155518\n",
            "Epoch 66, Train Loss: 0.0020615489435149357, Val Loss: 0.002044706692104228\n",
            "Epoch 67, Train Loss: 0.0021328518432565035, Val Loss: 0.001934568997239694\n",
            "Epoch 68, Train Loss: 0.002066822411841713, Val Loss: 0.0021478446276159955\n",
            "Epoch 69, Train Loss: 0.002057649361435324, Val Loss: 0.002253100115340203\n",
            "Epoch 70, Train Loss: 0.0020375114030903203, Val Loss: 0.002048229950014502\n",
            "Epoch 71, Train Loss: 0.002054732412332669, Val Loss: 0.0020799604413332417\n",
            "Epoch 72, Train Loss: 0.0021077410819707438, Val Loss: 0.0023723014944698664\n",
            "Epoch 73, Train Loss: 0.0020736397475702687, Val Loss: 0.0019691513304132967\n",
            "Epoch 74, Train Loss: 0.0020527755797374995, Val Loss: 0.0020653063344070687\n",
            "Epoch 75, Train Loss: 0.0020406644280301406, Val Loss: 0.002078089746646583\n",
            "Epoch 76, Train Loss: 0.0021307995880488306, Val Loss: 0.0021960861241677775\n",
            "Epoch 77, Train Loss: 0.0020715353339910507, Val Loss: 0.0020623450505081564\n",
            "Epoch 78, Train Loss: 0.00199896154133603, Val Loss: 0.0021221577102551238\n",
            "Epoch 79, Train Loss: 0.0020793640521587805, Val Loss: 0.002003916568355635\n",
            "Epoch 80, Train Loss: 0.0021009599932003764, Val Loss: 0.002144492510706186\n",
            "Epoch 81, Train Loss: 0.002002102842205204, Val Loss: 0.002069961629458703\n",
            "Epoch 82, Train Loss: 0.002021093198796734, Val Loss: 0.0020351376326289026\n",
            "Epoch 83, Train Loss: 0.001991256603738293, Val Loss: 0.001956505667185411\n",
            "Epoch 84, Train Loss: 0.0020055629465496167, Val Loss: 0.002024723675567657\n",
            "Epoch 85, Train Loss: 0.001953229942941107, Val Loss: 0.0019112268782919273\n",
            "Epoch 86, Train Loss: 0.0020363604468293488, Val Loss: 0.0019536867953138426\n",
            "Epoch 87, Train Loss: 0.002017219009110704, Val Loss: 0.0023888099519535897\n",
            "Epoch 88, Train Loss: 0.0020196818235563114, Val Loss: 0.0019920086895581337\n",
            "Epoch 89, Train Loss: 0.00198660807905253, Val Loss: 0.0019186638860264793\n",
            "Epoch 90, Train Loss: 0.0019904753698501737, Val Loss: 0.0018945601023733615\n",
            "Epoch 91, Train Loss: 0.002089341450249776, Val Loss: 0.0021080134983640164\n",
            "Epoch 92, Train Loss: 0.0019700875013368203, Val Loss: 0.0020745718549005686\n",
            "Epoch 93, Train Loss: 0.002061943657696247, Val Loss: 0.0019967057928442955\n",
            "Epoch 94, Train Loss: 0.002038485739962198, Val Loss: 0.0020049056771676986\n",
            "Epoch 95, Train Loss: 0.0020182903337990863, Val Loss: 0.0021310588682536035\n",
            "Epoch 96, Train Loss: 0.002112936151912436, Val Loss: 0.0020667267544195057\n",
            "Epoch 97, Train Loss: 0.0019691698391688986, Val Loss: 0.002051125835860148\n",
            "Epoch 98, Train Loss: 0.0020090976922074334, Val Loss: 0.0020903240359621124\n",
            "Epoch 99, Train Loss: 0.001946526540326886, Val Loss: 0.0018924611323745922\n",
            "Epoch 100, Train Loss: 0.002004244298557751, Val Loss: 0.00193763020390179\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2"
      ],
      "metadata": {
        "id": "Mz1PuW6BD7Ui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def sample2(diffusion, model, batch_size):\n",
        "    model.eval()\n",
        "    device = diffusion.device\n",
        "    T = diffusion.n_steps\n",
        "    x_t = torch.randn((batch_size, 1, 28, 28), device=device)\n",
        "\n",
        "    for t in reversed(range(0, T)):\n",
        "        t_tensor = torch.full((batch_size,), t, dtype=torch.long, device=device)\n",
        "\n",
        "        alpha_t = diffusion.alpha[t].view(1, 1, 1, 1)\n",
        "        alpha_bar_t = diffusion.alpha_bar[t].view(1, 1, 1, 1)\n",
        "\n",
        "        eps_pred = model(x_t, t_tensor)\n",
        "\n",
        "        coef1 = 1 / torch.sqrt(alpha_t)\n",
        "        coef2 = (1 - alpha_t) / torch.sqrt(1 - alpha_bar_t)\n",
        "        mu = coef1 * (x_t - coef2 * eps_pred)\n",
        "\n",
        "        if t > 0:\n",
        "            noise = torch.randn_like(x_t)\n",
        "            x_t = mu + torch.sqrt(1-alpha_t) * noise\n",
        "        else:\n",
        "            x_t = mu.clamp(0, 1)\n",
        "\n",
        "    return x_t\n"
      ],
      "metadata": {
        "id": "Q29RL9idD8S4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "samples = sample2(diffusion, model, 20)\n",
        "samples = samples.cpu().squeeze()\n",
        "\n",
        "fig, axs = plt.subplots(5, 4, figsize=(8, 10))\n",
        "for i, ax in enumerate(axs.flat):\n",
        "    ax.imshow(samples[i], cmap='gray')\n",
        "    ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "W3Lw89NxFDfW",
        "outputId": "f4885fe7-1a47-4c5f-eb6b-e702e173f9bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x1000 with 20 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mu_pop = np.mean(test_data.cpu().numpy().reshape(len(test_data), -1), axis=0)\n",
        "sigma_pop = np.cov(test_data.cpu().numpy().reshape(len(test_data), -1), rowvar=False)\n",
        "\n",
        "samples = sample2(diffusion, model, batch_size=1000)\n",
        "samples = samples.cpu().numpy().reshape(1000, -1)\n",
        "\n",
        "mu_gen = np.mean(samples, axis=0)\n",
        "sigma_gen = np.cov(samples, rowvar=False)\n",
        "\n",
        "fid_score = fid(mu_pop, sigma_pop, mu_gen, sigma_gen)\n",
        "print(f\"FID score: {fid_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aUMqB8GFIS0",
        "outputId": "4f065807-acd7-421c-db1a-5cef9b67fc25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FID score: 4.729483984811327\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3\n",
        "The FID score for the second version of the model has a slightly lower FID, but the samples still look pretty similar. Some of the samples look like digits while others have loops or disconected strokes."
      ],
      "metadata": {
        "id": "PzRY1KSGS4_e"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}